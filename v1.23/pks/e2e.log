I0507 10:16:48.341387      21 e2e.go:132] Starting e2e run "018beb03-baef-4b7c-b464-5fb8f5ad97e1" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1651918608 - Will randomize all specs
Will run 346 of 7042 specs

May  7 10:16:51.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:16:51.356: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  7 10:16:51.401: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  7 10:16:51.429: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  7 10:16:51.429: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May  7 10:16:51.429: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  7 10:16:51.434: INFO: e2e test version: v1.23.4
May  7 10:16:51.437: INFO: kube-apiserver version: v1.23.4+vmware.1
May  7 10:16:51.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:16:51.443: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:16:51.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubelet-test
W0507 10:16:51.541088      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
May  7 10:16:51.541: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May  7 10:16:51.551: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:16:51.570: INFO: The status of Pod busybox-scheduling-c790f2fa-87e6-4b94-bc80-140c26131e1d is Pending, waiting for it to be Running (with Ready = true)
May  7 10:16:53.584: INFO: The status of Pod busybox-scheduling-c790f2fa-87e6-4b94-bc80-140c26131e1d is Pending, waiting for it to be Running (with Ready = true)
May  7 10:16:55.575: INFO: The status of Pod busybox-scheduling-c790f2fa-87e6-4b94-bc80-140c26131e1d is Pending, waiting for it to be Running (with Ready = true)
May  7 10:16:57.584: INFO: The status of Pod busybox-scheduling-c790f2fa-87e6-4b94-bc80-140c26131e1d is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:16:57.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9823" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:16:57.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-e29dad10-cc63-4b70-9809-5fa0620083c8 in namespace container-probe-1525
May  7 10:17:01.746: INFO: Started pod busybox-e29dad10-cc63-4b70-9809-5fa0620083c8 in namespace container-probe-1525
STEP: checking the pod's current state and verifying that restartCount is present
May  7 10:17:01.751: INFO: Initial restart count of pod busybox-e29dad10-cc63-4b70-9809-5fa0620083c8 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1525" for this suite.

• [SLOW TEST:245.563 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":47,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:03.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:21:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: creating the pod
STEP: submitting the pod to kubernetes
May  7 10:21:03.336: INFO: The status of Pod pod-exec-websocket-7f9c908e-e96d-482c-a4e0-13a7c7e686df is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:05.354: INFO: The status of Pod pod-exec-websocket-7f9c908e-e96d-482c-a4e0-13a7c7e686df is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:07.349: INFO: The status of Pod pod-exec-websocket-7f9c908e-e96d-482c-a4e0-13a7c7e686df is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:07.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-385" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":48,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
May  7 10:21:07.554: INFO: Waiting up to 5m0s for pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c" in namespace "var-expansion-9318" to be "Succeeded or Failed"
May  7 10:21:07.560: INFO: Pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.470267ms
May  7 10:21:09.568: INFO: Pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014183239s
May  7 10:21:11.578: INFO: Pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023854015s
May  7 10:21:13.589: INFO: Pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035144259s
STEP: Saw pod success
May  7 10:21:13.589: INFO: Pod "var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c" satisfied condition "Succeeded or Failed"
May  7 10:21:13.593: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c container dapi-container: <nil>
STEP: delete the pod
May  7 10:21:13.635: INFO: Waiting for pod var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c to disappear
May  7 10:21:13.642: INFO: Pod var-expansion-e03794d5-f7d9-4c24-82a9-bab9e97a948c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:13.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9318" for this suite.

• [SLOW TEST:6.188 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":50,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:13.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jppnz in namespace proxy-2967
I0507 10:21:13.761535      21 runners.go:193] Created replication controller with name: proxy-service-jppnz, namespace: proxy-2967, replica count: 1
I0507 10:21:14.814652      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:15.815278      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:16.815636      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:17.816891      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:18.818560      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:19.819002      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:20.819684      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:21.820110      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:21:22.821148      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0507 10:21:23.821449      21 runners.go:193] proxy-service-jppnz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 10:21:23.827: INFO: setup took 10.122967842s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May  7 10:21:23.844: INFO: (0) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 16.515965ms)
May  7 10:21:23.845: INFO: (0) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 16.606201ms)
May  7 10:21:23.845: INFO: (0) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 16.910497ms)
May  7 10:21:23.846: INFO: (0) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.388592ms)
May  7 10:21:23.846: INFO: (0) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.194961ms)
May  7 10:21:23.850: INFO: (0) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 22.638686ms)
May  7 10:21:23.851: INFO: (0) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 23.011526ms)
May  7 10:21:23.851: INFO: (0) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 23.416455ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 24.107697ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 24.434546ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 24.058186ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 23.813584ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 23.964263ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 24.77194ms)
May  7 10:21:23.852: INFO: (0) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 24.618338ms)
May  7 10:21:23.853: INFO: (0) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 24.726374ms)
May  7 10:21:23.865: INFO: (1) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 10.827578ms)
May  7 10:21:23.867: INFO: (1) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 13.139443ms)
May  7 10:21:23.869: INFO: (1) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 13.846052ms)
May  7 10:21:23.870: INFO: (1) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 16.464863ms)
May  7 10:21:23.871: INFO: (1) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 16.676796ms)
May  7 10:21:23.871: INFO: (1) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 17.737668ms)
May  7 10:21:23.871: INFO: (1) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 16.332284ms)
May  7 10:21:23.872: INFO: (1) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 16.568454ms)
May  7 10:21:23.873: INFO: (1) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.65416ms)
May  7 10:21:23.877: INFO: (1) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 21.510385ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 24.546373ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 24.523817ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 26.181269ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 25.837326ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 26.422831ms)
May  7 10:21:23.880: INFO: (1) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 25.041104ms)
May  7 10:21:23.886: INFO: (2) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 6.11935ms)
May  7 10:21:23.887: INFO: (2) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 6.88024ms)
May  7 10:21:23.887: INFO: (2) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 6.668132ms)
May  7 10:21:23.892: INFO: (2) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 11.738197ms)
May  7 10:21:23.899: INFO: (2) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.41335ms)
May  7 10:21:23.899: INFO: (2) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 18.581364ms)
May  7 10:21:23.902: INFO: (2) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 21.683741ms)
May  7 10:21:23.902: INFO: (2) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 21.161502ms)
May  7 10:21:23.902: INFO: (2) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 21.317923ms)
May  7 10:21:23.902: INFO: (2) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 21.099188ms)
May  7 10:21:23.903: INFO: (2) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 22.453979ms)
May  7 10:21:23.904: INFO: (2) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 21.986846ms)
May  7 10:21:23.904: INFO: (2) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 22.036953ms)
May  7 10:21:23.905: INFO: (2) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 24.00211ms)
May  7 10:21:23.905: INFO: (2) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 23.090374ms)
May  7 10:21:23.906: INFO: (2) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 25.474399ms)
May  7 10:21:23.934: INFO: (3) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 26.925337ms)
May  7 10:21:23.934: INFO: (3) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 26.743063ms)
May  7 10:21:23.934: INFO: (3) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 26.564976ms)
May  7 10:21:23.936: INFO: (3) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 28.003259ms)
May  7 10:21:23.936: INFO: (3) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 28.371632ms)
May  7 10:21:23.936: INFO: (3) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 28.375593ms)
May  7 10:21:23.937: INFO: (3) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 30.245298ms)
May  7 10:21:23.937: INFO: (3) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 30.021756ms)
May  7 10:21:23.939: INFO: (3) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 32.012805ms)
May  7 10:21:23.943: INFO: (3) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 35.681422ms)
May  7 10:21:23.943: INFO: (3) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 35.820318ms)
May  7 10:21:23.944: INFO: (3) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 37.30571ms)
May  7 10:21:23.943: INFO: (3) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 35.888144ms)
May  7 10:21:23.943: INFO: (3) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 35.682451ms)
May  7 10:21:23.943: INFO: (3) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 36.040953ms)
May  7 10:21:23.944: INFO: (3) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 36.9835ms)
May  7 10:21:23.954: INFO: (4) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 9.593424ms)
May  7 10:21:23.956: INFO: (4) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 10.828066ms)
May  7 10:21:23.962: INFO: (4) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.691147ms)
May  7 10:21:23.964: INFO: (4) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 19.699652ms)
May  7 10:21:23.964: INFO: (4) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 19.267845ms)
May  7 10:21:23.965: INFO: (4) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 19.492911ms)
May  7 10:21:23.965: INFO: (4) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 19.809765ms)
May  7 10:21:23.965: INFO: (4) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 20.186183ms)
May  7 10:21:23.965: INFO: (4) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 20.09506ms)
May  7 10:21:23.966: INFO: (4) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 20.444107ms)
May  7 10:21:23.966: INFO: (4) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 20.795601ms)
May  7 10:21:23.967: INFO: (4) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 21.925184ms)
May  7 10:21:23.967: INFO: (4) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 21.959053ms)
May  7 10:21:23.968: INFO: (4) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 22.792741ms)
May  7 10:21:23.968: INFO: (4) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 22.532336ms)
May  7 10:21:23.968: INFO: (4) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 23.009214ms)
May  7 10:21:23.982: INFO: (5) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 13.515359ms)
May  7 10:21:23.982: INFO: (5) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 13.229379ms)
May  7 10:21:23.986: INFO: (5) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 17.816682ms)
May  7 10:21:23.986: INFO: (5) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 17.405154ms)
May  7 10:21:23.986: INFO: (5) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.459321ms)
May  7 10:21:23.986: INFO: (5) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 17.655862ms)
May  7 10:21:23.988: INFO: (5) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.479334ms)
May  7 10:21:23.988: INFO: (5) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.703345ms)
May  7 10:21:23.988: INFO: (5) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 19.502536ms)
May  7 10:21:23.988: INFO: (5) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 19.464892ms)
May  7 10:21:23.993: INFO: (5) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 24.466592ms)
May  7 10:21:23.994: INFO: (5) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 24.451977ms)
May  7 10:21:23.994: INFO: (5) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 24.822175ms)
May  7 10:21:23.994: INFO: (5) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 24.584082ms)
May  7 10:21:23.995: INFO: (5) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 26.063ms)
May  7 10:21:23.995: INFO: (5) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 26.286991ms)
May  7 10:21:24.010: INFO: (6) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 13.473326ms)
May  7 10:21:24.013: INFO: (6) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 15.743684ms)
May  7 10:21:24.013: INFO: (6) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.505913ms)
May  7 10:21:24.013: INFO: (6) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 17.681444ms)
May  7 10:21:24.014: INFO: (6) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 18.853025ms)
May  7 10:21:24.015: INFO: (6) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 19.078296ms)
May  7 10:21:24.016: INFO: (6) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 20.041432ms)
May  7 10:21:24.016: INFO: (6) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 19.499252ms)
May  7 10:21:24.016: INFO: (6) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 20.445926ms)
May  7 10:21:24.016: INFO: (6) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 19.99853ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 19.810142ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 20.130467ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 20.888437ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 20.142161ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 20.582076ms)
May  7 10:21:24.017: INFO: (6) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 20.440867ms)
May  7 10:21:24.034: INFO: (7) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 16.200236ms)
May  7 10:21:24.034: INFO: (7) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 16.372331ms)
May  7 10:21:24.034: INFO: (7) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 15.951706ms)
May  7 10:21:24.036: INFO: (7) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 18.33645ms)
May  7 10:21:24.037: INFO: (7) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.437303ms)
May  7 10:21:24.038: INFO: (7) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 19.463885ms)
May  7 10:21:24.038: INFO: (7) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 20.14401ms)
May  7 10:21:24.041: INFO: (7) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 22.36424ms)
May  7 10:21:24.044: INFO: (7) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 25.513294ms)
May  7 10:21:24.044: INFO: (7) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 25.346722ms)
May  7 10:21:24.044: INFO: (7) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 25.652513ms)
May  7 10:21:24.044: INFO: (7) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 25.778422ms)
May  7 10:21:24.045: INFO: (7) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 26.03442ms)
May  7 10:21:24.045: INFO: (7) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 26.365648ms)
May  7 10:21:24.045: INFO: (7) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 26.787982ms)
May  7 10:21:24.045: INFO: (7) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 27.317197ms)
May  7 10:21:24.065: INFO: (8) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 19.102864ms)
May  7 10:21:24.067: INFO: (8) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 15.781431ms)
May  7 10:21:24.067: INFO: (8) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 14.860903ms)
May  7 10:21:24.067: INFO: (8) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 16.334659ms)
May  7 10:21:24.068: INFO: (8) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 15.553039ms)
May  7 10:21:24.069: INFO: (8) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 18.474281ms)
May  7 10:21:24.069: INFO: (8) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 17.464444ms)
May  7 10:21:24.072: INFO: (8) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 19.251601ms)
May  7 10:21:24.072: INFO: (8) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.332827ms)
May  7 10:21:24.072: INFO: (8) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 18.50406ms)
May  7 10:21:24.072: INFO: (8) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 20.692199ms)
May  7 10:21:24.072: INFO: (8) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 18.723594ms)
May  7 10:21:24.073: INFO: (8) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 19.35376ms)
May  7 10:21:24.073: INFO: (8) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 20.944894ms)
May  7 10:21:24.073: INFO: (8) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 20.298878ms)
May  7 10:21:24.073: INFO: (8) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 19.843556ms)
May  7 10:21:24.081: INFO: (9) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 7.284341ms)
May  7 10:21:24.082: INFO: (9) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 8.537769ms)
May  7 10:21:24.084: INFO: (9) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 10.682339ms)
May  7 10:21:24.084: INFO: (9) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 10.565323ms)
May  7 10:21:24.086: INFO: (9) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 11.35952ms)
May  7 10:21:24.087: INFO: (9) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 13.138704ms)
May  7 10:21:24.087: INFO: (9) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 12.939719ms)
May  7 10:21:24.090: INFO: (9) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 14.862808ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 17.380122ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 17.092328ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 17.211015ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.527697ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 18.277621ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 18.385263ms)
May  7 10:21:24.092: INFO: (9) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 18.699917ms)
May  7 10:21:24.093: INFO: (9) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 18.348792ms)
May  7 10:21:24.105: INFO: (10) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 11.975877ms)
May  7 10:21:24.106: INFO: (10) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 13.30557ms)
May  7 10:21:24.106: INFO: (10) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 13.354648ms)
May  7 10:21:24.106: INFO: (10) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 13.311928ms)
May  7 10:21:24.109: INFO: (10) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.07979ms)
May  7 10:21:24.109: INFO: (10) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.122102ms)
May  7 10:21:24.109: INFO: (10) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 15.928125ms)
May  7 10:21:24.109: INFO: (10) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 16.214614ms)
May  7 10:21:24.110: INFO: (10) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 16.711638ms)
May  7 10:21:24.110: INFO: (10) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 17.330987ms)
May  7 10:21:24.111: INFO: (10) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 17.366096ms)
May  7 10:21:24.111: INFO: (10) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 17.836648ms)
May  7 10:21:24.113: INFO: (10) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 19.744168ms)
May  7 10:21:24.113: INFO: (10) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 20.013413ms)
May  7 10:21:24.113: INFO: (10) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 19.885339ms)
May  7 10:21:24.114: INFO: (10) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 20.863335ms)
May  7 10:21:24.125: INFO: (11) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 10.798879ms)
May  7 10:21:24.130: INFO: (11) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 15.294493ms)
May  7 10:21:24.132: INFO: (11) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.228866ms)
May  7 10:21:24.132: INFO: (11) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 17.489734ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 18.07862ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.154475ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 18.527863ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 18.887995ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 18.566212ms)
May  7 10:21:24.133: INFO: (11) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 18.813949ms)
May  7 10:21:24.134: INFO: (11) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 19.490432ms)
May  7 10:21:24.134: INFO: (11) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 19.366366ms)
May  7 10:21:24.134: INFO: (11) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 19.774361ms)
May  7 10:21:24.134: INFO: (11) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 20.035892ms)
May  7 10:21:24.134: INFO: (11) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 19.708928ms)
May  7 10:21:24.135: INFO: (11) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 20.041343ms)
May  7 10:21:24.150: INFO: (12) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 14.924878ms)
May  7 10:21:24.151: INFO: (12) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 15.366695ms)
May  7 10:21:24.151: INFO: (12) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 14.795346ms)
May  7 10:21:24.151: INFO: (12) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 16.008582ms)
May  7 10:21:24.151: INFO: (12) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 15.459562ms)
May  7 10:21:24.152: INFO: (12) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 15.585646ms)
May  7 10:21:24.152: INFO: (12) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 16.618362ms)
May  7 10:21:24.152: INFO: (12) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 16.501938ms)
May  7 10:21:24.154: INFO: (12) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.914222ms)
May  7 10:21:24.154: INFO: (12) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 17.696531ms)
May  7 10:21:24.154: INFO: (12) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.927548ms)
May  7 10:21:24.154: INFO: (12) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 18.316702ms)
May  7 10:21:24.155: INFO: (12) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 18.921277ms)
May  7 10:21:24.155: INFO: (12) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 19.102292ms)
May  7 10:21:24.158: INFO: (12) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 21.633831ms)
May  7 10:21:24.158: INFO: (12) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 21.877715ms)
May  7 10:21:24.175: INFO: (13) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.540582ms)
May  7 10:21:24.175: INFO: (13) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 16.790108ms)
May  7 10:21:24.181: INFO: (13) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 22.797331ms)
May  7 10:21:24.181: INFO: (13) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 22.850513ms)
May  7 10:21:24.182: INFO: (13) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 23.137152ms)
May  7 10:21:24.182: INFO: (13) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 23.464107ms)
May  7 10:21:24.183: INFO: (13) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 24.508452ms)
May  7 10:21:24.183: INFO: (13) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 24.612726ms)
May  7 10:21:24.183: INFO: (13) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 24.549611ms)
May  7 10:21:24.183: INFO: (13) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 24.767979ms)
May  7 10:21:24.184: INFO: (13) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 25.634537ms)
May  7 10:21:24.184: INFO: (13) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 25.42693ms)
May  7 10:21:24.184: INFO: (13) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 25.26283ms)
May  7 10:21:24.184: INFO: (13) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 25.84938ms)
May  7 10:21:24.185: INFO: (13) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 26.334371ms)
May  7 10:21:24.186: INFO: (13) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 27.173152ms)
May  7 10:21:24.223: INFO: (14) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 37.246986ms)
May  7 10:21:24.224: INFO: (14) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 37.472265ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 39.197326ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 39.615951ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 39.267134ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 39.544591ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 39.630475ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 40.239418ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 39.921704ms)
May  7 10:21:24.226: INFO: (14) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 39.916538ms)
May  7 10:21:24.227: INFO: (14) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 40.960077ms)
May  7 10:21:24.228: INFO: (14) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 41.840317ms)
May  7 10:21:24.228: INFO: (14) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 41.815029ms)
May  7 10:21:24.228: INFO: (14) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 41.620869ms)
May  7 10:21:24.228: INFO: (14) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 41.589628ms)
May  7 10:21:24.229: INFO: (14) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 42.746028ms)
May  7 10:21:24.238: INFO: (15) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 9.302338ms)
May  7 10:21:24.243: INFO: (15) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 12.858905ms)
May  7 10:21:24.244: INFO: (15) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 13.065521ms)
May  7 10:21:24.244: INFO: (15) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 14.002014ms)
May  7 10:21:24.244: INFO: (15) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 14.318644ms)
May  7 10:21:24.245: INFO: (15) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 14.116016ms)
May  7 10:21:24.245: INFO: (15) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 14.582571ms)
May  7 10:21:24.246: INFO: (15) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 15.402155ms)
May  7 10:21:24.247: INFO: (15) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 16.507932ms)
May  7 10:21:24.247: INFO: (15) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 16.620824ms)
May  7 10:21:24.247: INFO: (15) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 17.007826ms)
May  7 10:21:24.247: INFO: (15) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 16.992862ms)
May  7 10:21:24.248: INFO: (15) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 17.116094ms)
May  7 10:21:24.248: INFO: (15) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 17.723671ms)
May  7 10:21:24.249: INFO: (15) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 18.412469ms)
May  7 10:21:24.249: INFO: (15) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 18.375669ms)
May  7 10:21:24.257: INFO: (16) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 7.267197ms)
May  7 10:21:24.261: INFO: (16) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 11.372394ms)
May  7 10:21:24.264: INFO: (16) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 14.294491ms)
May  7 10:21:24.266: INFO: (16) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 15.952621ms)
May  7 10:21:24.266: INFO: (16) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 16.108114ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.958653ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 16.888384ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 16.740487ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 17.181386ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.701754ms)
May  7 10:21:24.267: INFO: (16) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 17.682414ms)
May  7 10:21:24.268: INFO: (16) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 18.054972ms)
May  7 10:21:24.268: INFO: (16) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 18.822667ms)
May  7 10:21:24.269: INFO: (16) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 18.881612ms)
May  7 10:21:24.269: INFO: (16) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 18.823231ms)
May  7 10:21:24.269: INFO: (16) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 19.011343ms)
May  7 10:21:24.284: INFO: (17) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 14.087019ms)
May  7 10:21:24.284: INFO: (17) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 13.840786ms)
May  7 10:21:24.284: INFO: (17) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 13.771027ms)
May  7 10:21:24.284: INFO: (17) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 14.132808ms)
May  7 10:21:24.292: INFO: (17) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 21.966265ms)
May  7 10:21:24.293: INFO: (17) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 22.874343ms)
May  7 10:21:24.293: INFO: (17) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 22.440148ms)
May  7 10:21:24.292: INFO: (17) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 21.724165ms)
May  7 10:21:24.293: INFO: (17) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 22.682344ms)
May  7 10:21:24.294: INFO: (17) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 22.84941ms)
May  7 10:21:24.295: INFO: (17) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 24.548544ms)
May  7 10:21:24.299: INFO: (17) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 28.622783ms)
May  7 10:21:24.299: INFO: (17) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 27.856177ms)
May  7 10:21:24.300: INFO: (17) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 29.258596ms)
May  7 10:21:24.301: INFO: (17) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 30.284577ms)
May  7 10:21:24.302: INFO: (17) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 30.361434ms)
May  7 10:21:24.315: INFO: (18) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 12.497374ms)
May  7 10:21:24.317: INFO: (18) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 14.770919ms)
May  7 10:21:24.318: INFO: (18) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 15.556181ms)
May  7 10:21:24.318: INFO: (18) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 15.332688ms)
May  7 10:21:24.318: INFO: (18) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.183091ms)
May  7 10:21:24.318: INFO: (18) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 16.410234ms)
May  7 10:21:24.319: INFO: (18) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 16.019153ms)
May  7 10:21:24.319: INFO: (18) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 16.142143ms)
May  7 10:21:24.319: INFO: (18) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 16.594157ms)
May  7 10:21:24.319: INFO: (18) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 16.596345ms)
May  7 10:21:24.319: INFO: (18) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 17.488945ms)
May  7 10:21:24.320: INFO: (18) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 17.719291ms)
May  7 10:21:24.320: INFO: (18) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 17.734442ms)
May  7 10:21:24.320: INFO: (18) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 17.619316ms)
May  7 10:21:24.320: INFO: (18) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 17.744695ms)
May  7 10:21:24.321: INFO: (18) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 19.181363ms)
May  7 10:21:24.328: INFO: (19) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8/proxy/rewriteme">test</a> (200; 6.105031ms)
May  7 10:21:24.329: INFO: (19) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:460/proxy/: tls baz (200; 7.666413ms)
May  7 10:21:24.331: INFO: (19) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 8.229523ms)
May  7 10:21:24.331: INFO: (19) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:443/proxy/tlsrewritem... (200; 8.863644ms)
May  7 10:21:24.333: INFO: (19) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname2/proxy/: bar (200; 10.410083ms)
May  7 10:21:24.333: INFO: (19) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">test<... (200; 10.84497ms)
May  7 10:21:24.334: INFO: (19) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname2/proxy/: bar (200; 12.243154ms)
May  7 10:21:24.334: INFO: (19) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 11.45688ms)
May  7 10:21:24.338: INFO: (19) /api/v1/namespaces/proxy-2967/services/http:proxy-service-jppnz:portname1/proxy/: foo (200; 14.889846ms)
May  7 10:21:24.338: INFO: (19) /api/v1/namespaces/proxy-2967/services/proxy-service-jppnz:portname1/proxy/: foo (200; 15.109019ms)
May  7 10:21:24.339: INFO: (19) /api/v1/namespaces/proxy-2967/pods/https:proxy-service-jppnz-7dqx8:462/proxy/: tls qux (200; 15.845598ms)
May  7 10:21:24.339: INFO: (19) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname2/proxy/: tls qux (200; 16.001765ms)
May  7 10:21:24.340: INFO: (19) /api/v1/namespaces/proxy-2967/pods/proxy-service-jppnz-7dqx8:162/proxy/: bar (200; 16.294053ms)
May  7 10:21:24.340: INFO: (19) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:1080/proxy/rewriteme">... (200; 16.95103ms)
May  7 10:21:24.340: INFO: (19) /api/v1/namespaces/proxy-2967/services/https:proxy-service-jppnz:tlsportname1/proxy/: tls baz (200; 17.06193ms)
May  7 10:21:24.341: INFO: (19) /api/v1/namespaces/proxy-2967/pods/http:proxy-service-jppnz-7dqx8:160/proxy/: foo (200; 17.470682ms)
STEP: deleting ReplicationController proxy-service-jppnz in namespace proxy-2967, will wait for the garbage collector to delete the pods
May  7 10:21:24.412: INFO: Deleting ReplicationController proxy-service-jppnz took: 12.42949ms
May  7 10:21:24.512: INFO: Terminating ReplicationController proxy-service-jppnz pods took: 100.744901ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2967" for this suite.

• [SLOW TEST:14.487 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":5,"skipped":53,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:28.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May  7 10:21:28.232: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:30.247: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:32.253: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:34.246: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  7 10:21:36.245: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May  7 10:21:37.280: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:37.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4452" for this suite.

• [SLOW TEST:9.264 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":6,"skipped":65,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:37.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-1c00c96c-5512-4a68-a7c8-190e27ede29b
STEP: Creating a pod to test consume configMaps
May  7 10:21:37.507: INFO: Waiting up to 5m0s for pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721" in namespace "configmap-7641" to be "Succeeded or Failed"
May  7 10:21:37.526: INFO: Pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.011325ms
May  7 10:21:39.539: INFO: Pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03186966s
May  7 10:21:41.550: INFO: Pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721": Phase="Running", Reason="", readiness=true. Elapsed: 4.042788301s
May  7 10:21:43.561: INFO: Pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054655775s
STEP: Saw pod success
May  7 10:21:43.562: INFO: Pod "pod-configmaps-e78b02bf-be20-4413-befe-211a94970721" satisfied condition "Succeeded or Failed"
May  7 10:21:43.572: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-e78b02bf-be20-4413-befe-211a94970721 container agnhost-container: <nil>
STEP: delete the pod
May  7 10:21:43.634: INFO: Waiting for pod pod-configmaps-e78b02bf-be20-4413-befe-211a94970721 to disappear
May  7 10:21:43.638: INFO: Pod pod-configmaps-e78b02bf-be20-4413-befe-211a94970721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:43.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7641" for this suite.

• [SLOW TEST:6.237 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":7,"skipped":74,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:43.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:21:43.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-640" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":8,"skipped":87,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:21:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May  7 10:21:43.797: INFO: PodSpec: initContainers in spec.initContainers
May  7 10:22:29.863: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7dc835d1-9f21-4c61-b71d-3b8811fcd1c9", GenerateName:"", Namespace:"init-container-7130", SelfLink:"", UID:"b7222c79-bb26-44e4-8b72-17f800aa0554", ResourceVersion:"4805", Generation:0, CreationTimestamp:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"797095583"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001bf2a68), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 7, 10, 21, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001bf2a98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-9kwp4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0023b54c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9kwp4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9kwp4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9kwp4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003915280), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034ae380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003915300)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003915320)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003915328), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00391532c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00363a910), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"30.1.0.4", PodIP:"40.0.11.2", PodIPs:[]v1.PodIP{v1.PodIP{IP:"40.0.11.2"}}, StartTime:time.Date(2022, time.May, 7, 10, 21, 43, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034ae460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034ae4d0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e4bd2644d503a5abad3863a4eb405b0fdf1761b99d3cd16a667ff4656a7abfce", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023b5680), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023b5660), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00391538c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:22:29.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7130" for this suite.

• [SLOW TEST:46.162 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":9,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:22:29.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May  7 10:22:30.025: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9992  5bbac6c6-63c4-4c34-8b1b-f2dd59a9292c 4815 0 2022-05-07 10:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-07 10:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 10:22:30.027: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9992  5bbac6c6-63c4-4c34-8b1b-f2dd59a9292c 4816 0 2022-05-07 10:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-07 10:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:22:30.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9992" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":10,"skipped":114,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:22:30.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 10:22:30.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62" in namespace "projected-6011" to be "Succeeded or Failed"
May  7 10:22:30.155: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Pending", Reason="", readiness=false. Elapsed: 15.035288ms
May  7 10:22:32.165: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02492296s
May  7 10:22:34.178: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037321216s
May  7 10:22:36.190: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049143071s
May  7 10:22:38.199: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058459204s
May  7 10:22:40.215: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.0747662s
STEP: Saw pod success
May  7 10:22:40.215: INFO: Pod "downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62" satisfied condition "Succeeded or Failed"
May  7 10:22:40.220: INFO: Trying to get logs from node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 pod downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62 container client-container: <nil>
STEP: delete the pod
May  7 10:22:40.281: INFO: Waiting for pod downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62 to disappear
May  7 10:22:40.289: INFO: Pod downwardapi-volume-2fe693e7-a314-4de1-955f-98afc59f5a62 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:22:40.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6011" for this suite.

• [SLOW TEST:10.255 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:22:40.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e9034c5a-1781-4e51-9fec-7c8962c059f4
STEP: Creating a pod to test consume secrets
May  7 10:22:40.424: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178" in namespace "projected-8209" to be "Succeeded or Failed"
May  7 10:22:40.446: INFO: Pod "pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178": Phase="Pending", Reason="", readiness=false. Elapsed: 21.871258ms
May  7 10:22:42.453: INFO: Pod "pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028687353s
May  7 10:22:44.463: INFO: Pod "pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038319146s
STEP: Saw pod success
May  7 10:22:44.463: INFO: Pod "pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178" satisfied condition "Succeeded or Failed"
May  7 10:22:44.468: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  7 10:22:44.505: INFO: Waiting for pod pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178 to disappear
May  7 10:22:44.509: INFO: Pod pod-projected-secrets-cfcd2ef2-9940-404c-9a66-b6cff2981178 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:22:44.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8209" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":158,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:22:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1825
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  7 10:22:44.597: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  7 10:22:44.700: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:22:46.715: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:22:48.708: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:22:50.714: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:22:52.711: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:22:54.707: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:22:56.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:22:58.707: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:23:00.715: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:23:02.713: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:23:04.710: INFO: The status of Pod netserver-0 is Running (Ready = true)
May  7 10:23:04.720: INFO: The status of Pod netserver-1 is Running (Ready = true)
May  7 10:23:04.789: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May  7 10:23:08.830: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  7 10:23:08.831: INFO: Breadth first check of 40.0.9.4 on host 30.1.0.3...
May  7 10:23:08.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.9.5:9080/dial?request=hostname&protocol=http&host=40.0.9.4&port=8083&tries=1'] Namespace:pod-network-test-1825 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:23:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:23:08.845: INFO: ExecWithOptions: Clientset creation
May  7 10:23:08.845: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1825/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.9.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D40.0.9.4%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:23:09.006: INFO: Waiting for responses: map[]
May  7 10:23:09.006: INFO: reached 40.0.9.4 after 0/1 tries
May  7 10:23:09.006: INFO: Breadth first check of 40.0.9.3 on host 30.1.0.5...
May  7 10:23:09.016: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.9.5:9080/dial?request=hostname&protocol=http&host=40.0.9.3&port=8083&tries=1'] Namespace:pod-network-test-1825 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:23:09.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:23:09.021: INFO: ExecWithOptions: Clientset creation
May  7 10:23:09.021: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1825/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.9.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D40.0.9.3%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:23:09.146: INFO: Waiting for responses: map[]
May  7 10:23:09.146: INFO: reached 40.0.9.3 after 0/1 tries
May  7 10:23:09.146: INFO: Breadth first check of 40.0.9.2 on host 30.1.0.4...
May  7 10:23:09.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.9.5:9080/dial?request=hostname&protocol=http&host=40.0.9.2&port=8083&tries=1'] Namespace:pod-network-test-1825 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:23:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:23:09.154: INFO: ExecWithOptions: Clientset creation
May  7 10:23:09.154: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1825/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.9.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D40.0.9.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:23:09.281: INFO: Waiting for responses: map[]
May  7 10:23:09.281: INFO: reached 40.0.9.2 after 0/1 tries
May  7 10:23:09.281: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:23:09.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1825" for this suite.

• [SLOW TEST:24.776 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":162,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:23:09.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 10:23:09.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332" in namespace "downward-api-6858" to be "Succeeded or Failed"
May  7 10:23:09.420: INFO: Pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205087ms
May  7 10:23:11.436: INFO: Pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023790993s
May  7 10:23:13.462: INFO: Pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04959684s
May  7 10:23:15.473: INFO: Pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060677306s
STEP: Saw pod success
May  7 10:23:15.473: INFO: Pod "downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332" satisfied condition "Succeeded or Failed"
May  7 10:23:15.482: INFO: Trying to get logs from node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 pod downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332 container client-container: <nil>
STEP: delete the pod
May  7 10:23:15.537: INFO: Waiting for pod downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332 to disappear
May  7 10:23:15.541: INFO: Pod downwardapi-volume-0b1505a9-c96e-4fa6-b73f-27282a33b332 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:23:15.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6858" for this suite.

• [SLOW TEST:6.249 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":163,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:23:15.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May  7 10:23:15.615: INFO: namespace kubectl-5342
May  7 10:23:15.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5342 create -f -'
May  7 10:23:16.987: INFO: stderr: ""
May  7 10:23:16.987: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May  7 10:23:17.995: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 10:23:17.996: INFO: Found 0 / 1
May  7 10:23:18.999: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 10:23:18.999: INFO: Found 0 / 1
May  7 10:23:20.004: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 10:23:20.004: INFO: Found 1 / 1
May  7 10:23:20.004: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  7 10:23:20.010: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 10:23:20.011: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  7 10:23:20.011: INFO: wait on agnhost-primary startup in kubectl-5342 
May  7 10:23:20.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5342 logs agnhost-primary-x67s9 agnhost-primary'
May  7 10:23:20.113: INFO: stderr: ""
May  7 10:23:20.113: INFO: stdout: "Paused\n"
STEP: exposing RC
May  7 10:23:20.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5342 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  7 10:23:20.213: INFO: stderr: ""
May  7 10:23:20.213: INFO: stdout: "service/rm2 exposed\n"
May  7 10:23:20.250: INFO: Service rm2 in namespace kubectl-5342 found.
STEP: exposing service
May  7 10:23:22.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5342 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  7 10:23:22.399: INFO: stderr: ""
May  7 10:23:22.399: INFO: stdout: "service/rm3 exposed\n"
May  7 10:23:22.403: INFO: Service rm3 in namespace kubectl-5342 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:23:24.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5342" for this suite.

• [SLOW TEST:8.861 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":15,"skipped":172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:23:24.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4578
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-4578
May  7 10:23:24.550: INFO: Found 0 stateful pods, waiting for 1
May  7 10:23:34.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
May  7 10:23:34.599: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
May  7 10:23:34.612: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
May  7 10:23:34.616: INFO: Observed &StatefulSet event: ADDED
May  7 10:23:34.616: INFO: Found Statefulset ss in namespace statefulset-4578 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  7 10:23:34.616: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
May  7 10:23:34.616: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  7 10:23:34.636: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
May  7 10:23:34.639: INFO: Observed &StatefulSet event: ADDED
May  7 10:23:34.639: INFO: Observed Statefulset ss in namespace statefulset-4578 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  7 10:23:34.640: INFO: Observed &StatefulSet event: MODIFIED
May  7 10:23:34.640: INFO: Found Statefulset ss in namespace statefulset-4578 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 10:23:34.640: INFO: Deleting all statefulset in ns statefulset-4578
May  7 10:23:34.644: INFO: Scaling statefulset ss to 0
May  7 10:23:44.670: INFO: Waiting for statefulset status.replicas updated to 0
May  7 10:23:44.675: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:23:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4578" for this suite.

• [SLOW TEST:20.291 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":16,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:23:44.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:23:44.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  7 10:23:48.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-3398 --namespace=crd-publish-openapi-3398 create -f -'
May  7 10:23:50.699: INFO: stderr: ""
May  7 10:23:50.700: INFO: stdout: "e2e-test-crd-publish-openapi-5292-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  7 10:23:50.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-3398 --namespace=crd-publish-openapi-3398 delete e2e-test-crd-publish-openapi-5292-crds test-cr'
May  7 10:23:50.784: INFO: stderr: ""
May  7 10:23:50.784: INFO: stdout: "e2e-test-crd-publish-openapi-5292-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  7 10:23:50.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-3398 --namespace=crd-publish-openapi-3398 apply -f -'
May  7 10:23:51.008: INFO: stderr: ""
May  7 10:23:51.008: INFO: stdout: "e2e-test-crd-publish-openapi-5292-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  7 10:23:51.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-3398 --namespace=crd-publish-openapi-3398 delete e2e-test-crd-publish-openapi-5292-crds test-cr'
May  7 10:23:51.080: INFO: stderr: ""
May  7 10:23:51.080: INFO: stdout: "e2e-test-crd-publish-openapi-5292-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  7 10:23:51.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-3398 explain e2e-test-crd-publish-openapi-5292-crds'
May  7 10:23:51.262: INFO: stderr: ""
May  7 10:23:51.262: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5292-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:23:54.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3398" for this suite.

• [SLOW TEST:9.957 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":17,"skipped":254,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:23:54.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May  7 10:23:58.781: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5793 PodName:var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:23:58.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:23:58.783: INFO: ExecWithOptions: Clientset creation
May  7 10:23:58.783: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/var-expansion-5793/pods/var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
May  7 10:23:58.884: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5793 PodName:var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:23:58.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:23:58.885: INFO: ExecWithOptions: Clientset creation
May  7 10:23:58.885: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/var-expansion-5793/pods/var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
May  7 10:23:59.495: INFO: Successfully updated pod "var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May  7 10:23:59.501: INFO: Deleting pod "var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d" in namespace "var-expansion-5793"
May  7 10:23:59.518: INFO: Wait up to 5m0s for pod "var-expansion-3315af80-9171-4d79-81ec-1e45a0266a7d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:24:33.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5793" for this suite.

• [SLOW TEST:38.865 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":18,"skipped":264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:24:33.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May  7 10:24:33.669: INFO: Waiting up to 5m0s for pod "pod-52da97b4-eb0a-4076-9860-3219d65f8571" in namespace "emptydir-5955" to be "Succeeded or Failed"
May  7 10:24:33.688: INFO: Pod "pod-52da97b4-eb0a-4076-9860-3219d65f8571": Phase="Pending", Reason="", readiness=false. Elapsed: 19.025173ms
May  7 10:24:35.700: INFO: Pod "pod-52da97b4-eb0a-4076-9860-3219d65f8571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031123108s
May  7 10:24:37.711: INFO: Pod "pod-52da97b4-eb0a-4076-9860-3219d65f8571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041961805s
STEP: Saw pod success
May  7 10:24:37.711: INFO: Pod "pod-52da97b4-eb0a-4076-9860-3219d65f8571" satisfied condition "Succeeded or Failed"
May  7 10:24:37.723: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-52da97b4-eb0a-4076-9860-3219d65f8571 container test-container: <nil>
STEP: delete the pod
May  7 10:24:37.757: INFO: Waiting for pod pod-52da97b4-eb0a-4076-9860-3219d65f8571 to disappear
May  7 10:24:37.761: INFO: Pod pod-52da97b4-eb0a-4076-9860-3219d65f8571 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:24:37.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5955" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:24:37.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
May  7 10:24:37.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5054 api-versions'
May  7 10:24:37.934: INFO: stderr: ""
May  7 10:24:37.934: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnsx.vmware.com/v1\npksapi.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvmware.com/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:24:37.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5054" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":20,"skipped":371,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:24:37.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:24:38.006: INFO: Creating simple deployment test-new-deployment
May  7 10:24:38.028: INFO: deployment "test-new-deployment" doesn't have the required revision set
May  7 10:24:40.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 10, 24, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 24, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 10, 24, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 24, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 10:24:42.207: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7690  a690f720-eeab-4085-b1a8-094f48ffb94e 5469 3 2022-05-07 10:24:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-05-07 10:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 10:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054fab88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-05-07 10:24:41 +0000 UTC,LastTransitionTime:2022-05-07 10:24:38 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-07 10:24:42 +0000 UTC,LastTransitionTime:2022-05-07 10:24:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  7 10:24:42.237: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-7690  40d240d0-baff-43e6-bd80-ed7b6bf632bf 5475 2 2022-05-07 10:24:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a690f720-eeab-4085-b1a8-094f48ffb94e 0xc0055d0b07 0xc0055d0b08}] []  [{kube-controller-manager Update apps/v1 2022-05-07 10:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a690f720-eeab-4085-b1a8-094f48ffb94e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 10:24:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055d0bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  7 10:24:42.283: INFO: Pod "test-new-deployment-5d9fdcc779-6pmpm" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-6pmpm test-new-deployment-5d9fdcc779- deployment-7690  4e1c708b-51a2-431c-8ca4-ecb713409b56 5479 0 2022-05-07 10:24:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 40d240d0-baff-43e6-bd80-ed7b6bf632bf 0xc0055d1030 0xc0055d1031}] []  [{kube-controller-manager Update v1 2022-05-07 10:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40d240d0-baff-43e6-bd80-ed7b6bf632bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d55kq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d55kq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 10:24:42.284: INFO: Pod "test-new-deployment-5d9fdcc779-k6dkc" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-k6dkc test-new-deployment-5d9fdcc779- deployment-7690  edded87c-8e84-47cb-86cd-3234acd3e2f4 5476 0 2022-05-07 10:24:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 40d240d0-baff-43e6-bd80-ed7b6bf632bf 0xc0055d11a7 0xc0055d11a8}] []  [{Go-http-client Update v1 2022-05-07 10:24:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 10:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40d240d0-baff-43e6-bd80-ed7b6bf632bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8k2vc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8k2vc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 10:24:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 10:24:42.284: INFO: Pod "test-new-deployment-5d9fdcc779-sv9qb" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-sv9qb test-new-deployment-5d9fdcc779- deployment-7690  b7559afe-49e4-4e79-9e8d-c5983fa973cd 5461 0 2022-05-07 10:24:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 40d240d0-baff-43e6-bd80-ed7b6bf632bf 0xc0055d1390 0xc0055d1391}] []  [{kube-controller-manager Update v1 2022-05-07 10:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40d240d0-baff-43e6-bd80-ed7b6bf632bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 10:24:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.10.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52bsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52bsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 10:24:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.10.2,StartTime:2022-05-07 10:24:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 10:24:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9d330273078d50296997e964ddc017a2bad208a107eab778c760240c0ee46f9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.10.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:24:42.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7690" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":21,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:24:42.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2e6984a0-b4f1-4291-97b7-25fbf4efcd31
STEP: Creating a pod to test consume secrets
May  7 10:24:42.491: INFO: Waiting up to 5m0s for pod "pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648" in namespace "secrets-8382" to be "Succeeded or Failed"
May  7 10:24:42.499: INFO: Pod "pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648": Phase="Pending", Reason="", readiness=false. Elapsed: 8.690641ms
May  7 10:24:44.509: INFO: Pod "pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018787029s
May  7 10:24:46.524: INFO: Pod "pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033439047s
STEP: Saw pod success
May  7 10:24:46.524: INFO: Pod "pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648" satisfied condition "Succeeded or Failed"
May  7 10:24:46.532: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648 container secret-volume-test: <nil>
STEP: delete the pod
May  7 10:24:46.568: INFO: Waiting for pod pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648 to disappear
May  7 10:24:46.585: INFO: Pod pod-secrets-91e456b0-0aa7-4637-a638-5f8d365a6648 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:24:46.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8382" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":396,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:24:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May  7 10:24:46.702: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 10:25:46.761: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:25:46.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:25:46.846: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May  7 10:25:46.854: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:25:46.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8474" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:25:46.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5689" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.431 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":23,"skipped":410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:25:47.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May  7 10:25:47.086: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 10:26:47.158: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:26:47.167: INFO: Starting informer...
STEP: Starting pod...
May  7 10:26:47.405: INFO: Pod is running on 5615889a-d356-43b5-a818-02fb040c6965. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May  7 10:26:47.495: INFO: Pod wasn't evicted. Proceeding
May  7 10:26:47.495: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May  7 10:28:02.684: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:28:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5885" for this suite.

• [SLOW TEST:135.681 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":24,"skipped":439,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:28:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May  7 10:28:02.848: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 10:29:02.915: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:29:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May  7 10:29:09.063: INFO: found a healthy node: 5615889a-d356-43b5-a818-02fb040c6965
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:29:27.161: INFO: pods created so far: [1 1 1]
May  7 10:29:27.162: INFO: length of pods created so far: 3
May  7 10:29:29.179: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:29:36.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4186" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:29:36.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3615" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.607 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":25,"skipped":439,"failed":0}
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:29:36.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:29:36.413: INFO: Create a RollingUpdate DaemonSet
May  7 10:29:36.430: INFO: Check that daemon pods launch on every node of the cluster
May  7 10:29:36.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:29:36.442: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:37.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:29:37.459: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:38.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:29:38.457: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:39.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:29:39.462: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:40.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:29:40.460: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:41.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:29:41.461: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:42.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:29:42.457: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:43.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:29:43.466: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:44.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:29:44.464: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:45.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:29:45.459: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:29:46.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 10:29:46.455: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May  7 10:29:46.455: INFO: Update the DaemonSet to trigger a rollout
May  7 10:29:46.476: INFO: Updating DaemonSet daemon-set
May  7 10:29:50.509: INFO: Roll back the DaemonSet before rollout is complete
May  7 10:29:50.524: INFO: Updating DaemonSet daemon-set
May  7 10:29:50.524: INFO: Make sure DaemonSet rollback is complete
May  7 10:29:50.528: INFO: Wrong image for pod: daemon-set-jhdrx. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May  7 10:29:50.528: INFO: Pod daemon-set-jhdrx is not available
May  7 10:29:56.543: INFO: Pod daemon-set-tqtv7 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6332, will wait for the garbage collector to delete the pods
May  7 10:29:56.621: INFO: Deleting DaemonSet.extensions daemon-set took: 9.27912ms
May  7 10:29:56.721: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.685075ms
May  7 10:30:00.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:30:00.131: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 10:30:00.135: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6967"},"items":null}

May  7 10:30:00.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6967"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:30:00.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6332" for this suite.

• [SLOW TEST:23.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":26,"skipped":439,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:30:00.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
May  7 10:30:02.275: INFO: pods: 0 < 3
May  7 10:30:04.282: INFO: running pods: 0 < 3
May  7 10:30:06.286: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May  7 10:30:12.453: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:30:14.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6221" for this suite.

• [SLOW TEST:14.357 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":27,"skipped":454,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:30:14.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:32:00.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6339" for this suite.

• [SLOW TEST:106.137 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":28,"skipped":469,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:32:00.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-456
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-456
I0507 10:32:00.827428      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-456, replica count: 2
I0507 10:32:03.878289      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:32:06.880407      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 10:32:06.880: INFO: Creating new exec pod
May  7 10:32:11.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  7 10:32:12.192: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  7 10:32:12.193: INFO: stdout: ""
May  7 10:32:13.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  7 10:32:13.335: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  7 10:32:13.335: INFO: stdout: "externalname-service-r96j4"
May  7 10:32:13.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.92.34 80'
May  7 10:32:13.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.92.34 80\nConnection to 10.150.92.34 80 port [tcp/http] succeeded!\n"
May  7 10:32:13.491: INFO: stdout: ""
May  7 10:32:14.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.92.34 80'
May  7 10:32:14.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.92.34 80\nConnection to 10.150.92.34 80 port [tcp/http] succeeded!\n"
May  7 10:32:14.652: INFO: stdout: ""
May  7 10:32:15.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.92.34 80'
May  7 10:32:15.643: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.92.34 80\nConnection to 10.150.92.34 80 port [tcp/http] succeeded!\n"
May  7 10:32:15.643: INFO: stdout: ""
May  7 10:32:16.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.92.34 80'
May  7 10:32:16.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.92.34 80\nConnection to 10.150.92.34 80 port [tcp/http] succeeded!\n"
May  7 10:32:16.642: INFO: stdout: "externalname-service-r96j4"
May  7 10:32:16.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30475'
May  7 10:32:16.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30475\nConnection to 30.1.0.3 30475 port [tcp/*] succeeded!\n"
May  7 10:32:16.800: INFO: stdout: ""
May  7 10:32:17.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30475'
May  7 10:32:17.970: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30475\nConnection to 30.1.0.3 30475 port [tcp/*] succeeded!\n"
May  7 10:32:17.970: INFO: stdout: ""
May  7 10:32:18.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30475'
May  7 10:32:18.969: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30475\nConnection to 30.1.0.3 30475 port [tcp/*] succeeded!\n"
May  7 10:32:18.969: INFO: stdout: ""
May  7 10:32:19.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30475'
May  7 10:32:19.955: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30475\nConnection to 30.1.0.3 30475 port [tcp/*] succeeded!\n"
May  7 10:32:19.955: INFO: stdout: ""
May  7 10:32:20.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30475'
May  7 10:32:20.946: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30475\nConnection to 30.1.0.3 30475 port [tcp/*] succeeded!\n"
May  7 10:32:20.946: INFO: stdout: "externalname-service-r96j4"
May  7 10:32:20.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-456 exec execpodspnw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.5 30475'
May  7 10:32:21.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.5 30475\nConnection to 30.1.0.5 30475 port [tcp/*] succeeded!\n"
May  7 10:32:21.105: INFO: stdout: "externalname-service-r96j4"
May  7 10:32:21.105: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:32:21.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-456" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:20.504 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":29,"skipped":471,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:32:21.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  7 10:32:21.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:32:21.273: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:32:22.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:32:22.289: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:32:23.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:32:23.287: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:32:24.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:32:24.290: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:32:25.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:32:25.286: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:32:26.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 10:32:26.300: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May  7 10:32:26.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 10:32:26.368: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 10:32:27.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 10:32:27.385: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 10:32:28.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 10:32:28.383: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 10:32:29.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 10:32:29.387: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 10:32:30.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 10:32:30.386: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7094, will wait for the garbage collector to delete the pods
May  7 10:32:30.463: INFO: Deleting DaemonSet.extensions daemon-set took: 8.746437ms
May  7 10:32:30.564: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.712972ms
May  7 10:32:33.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:32:33.569: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 10:32:33.572: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7612"},"items":null}

May  7 10:32:33.575: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7612"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:32:33.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7094" for this suite.

• [SLOW TEST:12.429 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":30,"skipped":479,"failed":0}
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:32:33.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May  7 10:32:33.667: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7121  944197eb-4778-488a-85e4-6115178c7141 7619 0 2022-05-07 10:32:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-07 10:32:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 10:32:33.668: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7121  944197eb-4778-488a-85e4-6115178c7141 7620 0 2022-05-07 10:32:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-07 10:32:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May  7 10:32:33.684: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7121  944197eb-4778-488a-85e4-6115178c7141 7621 0 2022-05-07 10:32:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-07 10:32:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 10:32:33.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7121  944197eb-4778-488a-85e4-6115178c7141 7622 0 2022-05-07 10:32:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-07 10:32:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:32:33.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7121" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":31,"skipped":479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:32:33.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:32:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  7 10:32:37.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-2981 --namespace=crd-publish-openapi-2981 create -f -'
May  7 10:32:39.576: INFO: stderr: ""
May  7 10:32:39.576: INFO: stdout: "e2e-test-crd-publish-openapi-8091-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  7 10:32:39.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-2981 --namespace=crd-publish-openapi-2981 delete e2e-test-crd-publish-openapi-8091-crds test-cr'
May  7 10:32:39.658: INFO: stderr: ""
May  7 10:32:39.658: INFO: stdout: "e2e-test-crd-publish-openapi-8091-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  7 10:32:39.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-2981 --namespace=crd-publish-openapi-2981 apply -f -'
May  7 10:32:39.922: INFO: stderr: ""
May  7 10:32:39.922: INFO: stdout: "e2e-test-crd-publish-openapi-8091-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  7 10:32:39.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-2981 --namespace=crd-publish-openapi-2981 delete e2e-test-crd-publish-openapi-8091-crds test-cr'
May  7 10:32:40.003: INFO: stderr: ""
May  7 10:32:40.003: INFO: stdout: "e2e-test-crd-publish-openapi-8091-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  7 10:32:40.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-2981 explain e2e-test-crd-publish-openapi-8091-crds'
May  7 10:32:40.244: INFO: stderr: ""
May  7 10:32:40.245: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8091-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:32:42.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2981" for this suite.

• [SLOW TEST:9.001 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":32,"skipped":535,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:32:42.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-970b0e97-79d3-453a-8705-dc9b00571e3d
STEP: Creating secret with name s-test-opt-upd-7eb862d5-788c-4e2a-a6ca-ccf31b9b5232
STEP: Creating the pod
May  7 10:32:42.763: INFO: The status of Pod pod-projected-secrets-8af02f40-9a19-425d-8a71-6993cf894dfe is Pending, waiting for it to be Running (with Ready = true)
May  7 10:32:44.777: INFO: The status of Pod pod-projected-secrets-8af02f40-9a19-425d-8a71-6993cf894dfe is Pending, waiting for it to be Running (with Ready = true)
May  7 10:32:46.770: INFO: The status of Pod pod-projected-secrets-8af02f40-9a19-425d-8a71-6993cf894dfe is Pending, waiting for it to be Running (with Ready = true)
May  7 10:32:48.768: INFO: The status of Pod pod-projected-secrets-8af02f40-9a19-425d-8a71-6993cf894dfe is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-970b0e97-79d3-453a-8705-dc9b00571e3d
STEP: Updating secret s-test-opt-upd-7eb862d5-788c-4e2a-a6ca-ccf31b9b5232
STEP: Creating secret with name s-test-opt-create-930ce16b-4074-44f2-a409-a83e5ba14d41
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:05.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-964" for this suite.

• [SLOW TEST:82.597 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":538,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:05.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0a5b0410-a6b5-4058-a17a-490dbba15423
STEP: Creating a pod to test consume configMaps
May  7 10:34:05.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e" in namespace "projected-4094" to be "Succeeded or Failed"
May  7 10:34:05.404: INFO: Pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.244155ms
May  7 10:34:07.411: INFO: Pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014605332s
May  7 10:34:09.418: INFO: Pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021612779s
May  7 10:34:11.433: INFO: Pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03654045s
STEP: Saw pod success
May  7 10:34:11.433: INFO: Pod "pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e" satisfied condition "Succeeded or Failed"
May  7 10:34:11.437: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e container agnhost-container: <nil>
STEP: delete the pod
May  7 10:34:11.469: INFO: Waiting for pod pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e to disappear
May  7 10:34:11.473: INFO: Pod pod-projected-configmaps-784e36a3-7217-4462-83c0-3dfd99f4670e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:11.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4094" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":546,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:11.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
May  7 10:34:11.633: INFO: Waiting up to 5m0s for pod "var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24" in namespace "var-expansion-2162" to be "Succeeded or Failed"
May  7 10:34:11.639: INFO: Pod "var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759216ms
May  7 10:34:13.648: INFO: Pod "var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015203623s
May  7 10:34:15.665: INFO: Pod "var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032674411s
STEP: Saw pod success
May  7 10:34:15.665: INFO: Pod "var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24" satisfied condition "Succeeded or Failed"
May  7 10:34:15.671: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24 container dapi-container: <nil>
STEP: delete the pod
May  7 10:34:15.704: INFO: Waiting for pod var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24 to disappear
May  7 10:34:15.707: INFO: Pod var-expansion-23c1ef37-2422-4695-82f9-ba32b1844e24 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:15.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2162" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":35,"skipped":564,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:15.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
May  7 10:34:15.784: INFO: created test-podtemplate-1
May  7 10:34:15.789: INFO: created test-podtemplate-2
May  7 10:34:15.799: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May  7 10:34:15.805: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May  7 10:34:15.821: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5266" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":36,"skipped":571,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:15.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May  7 10:34:15.973: INFO: Waiting up to 5m0s for pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195" in namespace "security-context-836" to be "Succeeded or Failed"
May  7 10:34:15.993: INFO: Pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195": Phase="Pending", Reason="", readiness=false. Elapsed: 20.524591ms
May  7 10:34:18.006: INFO: Pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033108369s
May  7 10:34:20.025: INFO: Pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052742331s
May  7 10:34:22.036: INFO: Pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063417897s
STEP: Saw pod success
May  7 10:34:22.036: INFO: Pod "security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195" satisfied condition "Succeeded or Failed"
May  7 10:34:22.040: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195 container test-container: <nil>
STEP: delete the pod
May  7 10:34:22.073: INFO: Waiting for pod security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195 to disappear
May  7 10:34:22.083: INFO: Pod security-context-defa1877-f5e9-4adc-a2c5-3428a62ca195 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:22.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-836" for this suite.

• [SLOW TEST:6.257 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":37,"skipped":587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:22.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:34:22.167: INFO: Creating ReplicaSet my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6
May  7 10:34:22.194: INFO: Pod name my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6: Found 0 pods out of 1
May  7 10:34:27.208: INFO: Pod name my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6: Found 1 pods out of 1
May  7 10:34:27.208: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6" is running
May  7 10:34:27.212: INFO: Pod "my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6-jmpv5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:34:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:34:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:34:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:34:22 +0000 UTC Reason: Message:}])
May  7 10:34:27.212: INFO: Trying to dial the pod
May  7 10:34:32.415: INFO: Controller my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6: Got expected result from replica 1 [my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6-jmpv5]: "my-hostname-basic-cbb80f3c-1a15-4cb1-a036-e26ed2e5c8f6-jmpv5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:32.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3341" for this suite.

• [SLOW TEST:10.336 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":38,"skipped":656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:32.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4570
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4570
I0507 10:34:32.621503      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4570, replica count: 2
I0507 10:34:35.672953      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:34:38.673448      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 10:34:38.673: INFO: Creating new exec pod
May  7 10:34:43.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4570 exec execpodn7sqn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  7 10:34:43.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  7 10:34:43.890: INFO: stdout: "externalname-service-bq86r"
May  7 10:34:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4570 exec execpodn7sqn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.110.166 80'
May  7 10:34:44.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.110.166 80\nConnection to 10.150.110.166 80 port [tcp/http] succeeded!\n"
May  7 10:34:44.042: INFO: stdout: "externalname-service-727w2"
May  7 10:34:44.043: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:34:44.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4570" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.660 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":39,"skipped":679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:34:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May  7 10:34:44.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4646" for this suite.

• [SLOW TEST:19.817 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":40,"skipped":702,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:03.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-1750
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1750 to expose endpoints map[]
May  7 10:35:04.028: INFO: successfully validated that service endpoint-test2 in namespace services-1750 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1750
May  7 10:35:04.056: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:35:06.075: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:35:08.064: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:35:10.064: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1750 to expose endpoints map[pod1:[80]]
May  7 10:35:10.079: INFO: successfully validated that service endpoint-test2 in namespace services-1750 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
May  7 10:35:10.079: INFO: Creating new exec pod
May  7 10:35:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  7 10:35:17.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  7 10:35:17.307: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:35:17.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.17.37 80'
May  7 10:35:17.462: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.17.37 80\nConnection to 10.150.17.37 80 port [tcp/http] succeeded!\n"
May  7 10:35:17.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1750
May  7 10:35:17.479: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:35:19.488: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:35:21.489: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1750 to expose endpoints map[pod1:[80] pod2:[80]]
May  7 10:35:21.507: INFO: successfully validated that service endpoint-test2 in namespace services-1750 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
May  7 10:35:22.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  7 10:35:22.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  7 10:35:22.698: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:35:22.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.17.37 80'
May  7 10:35:22.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.17.37 80\nConnection to 10.150.17.37 80 port [tcp/http] succeeded!\n"
May  7 10:35:22.869: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1750
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1750 to expose endpoints map[pod2:[80]]
May  7 10:35:22.943: INFO: successfully validated that service endpoint-test2 in namespace services-1750 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
May  7 10:35:23.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  7 10:35:24.135: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  7 10:35:24.135: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:35:24.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1750 exec execpods5sxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.17.37 80'
May  7 10:35:24.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.17.37 80\nConnection to 10.150.17.37 80 port [tcp/http] succeeded!\n"
May  7 10:35:24.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1750
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1750 to expose endpoints map[]
May  7 10:35:25.415: INFO: successfully validated that service endpoint-test2 in namespace services-1750 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:25.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1750" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.552 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":41,"skipped":717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:25.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d571786e-4477-4a27-b3eb-3da5cc7b13bf
STEP: Creating a pod to test consume secrets
May  7 10:35:25.543: INFO: Waiting up to 5m0s for pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30" in namespace "secrets-1275" to be "Succeeded or Failed"
May  7 10:35:25.554: INFO: Pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30": Phase="Pending", Reason="", readiness=false. Elapsed: 11.580525ms
May  7 10:35:27.572: INFO: Pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029268953s
May  7 10:35:29.579: INFO: Pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30": Phase="Running", Reason="", readiness=true. Elapsed: 4.036354682s
May  7 10:35:31.584: INFO: Pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041259724s
STEP: Saw pod success
May  7 10:35:31.584: INFO: Pod "pod-secrets-38814646-be2b-4812-8940-286132d3ac30" satisfied condition "Succeeded or Failed"
May  7 10:35:31.587: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-38814646-be2b-4812-8940-286132d3ac30 container secret-volume-test: <nil>
STEP: delete the pod
May  7 10:35:31.621: INFO: Waiting for pod pod-secrets-38814646-be2b-4812-8940-286132d3ac30 to disappear
May  7 10:35:31.633: INFO: Pod pod-secrets-38814646-be2b-4812-8940-286132d3ac30 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:31.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1275" for this suite.

• [SLOW TEST:6.172 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":752,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-164579b5-e45b-45eb-ac0a-e5d6f7004929
STEP: Creating a pod to test consume secrets
May  7 10:35:31.733: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900" in namespace "projected-19" to be "Succeeded or Failed"
May  7 10:35:31.758: INFO: Pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900": Phase="Pending", Reason="", readiness=false. Elapsed: 25.130408ms
May  7 10:35:33.771: INFO: Pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037926782s
May  7 10:35:35.780: INFO: Pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900": Phase="Running", Reason="", readiness=true. Elapsed: 4.046956129s
May  7 10:35:37.793: INFO: Pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059328591s
STEP: Saw pod success
May  7 10:35:37.793: INFO: Pod "pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900" satisfied condition "Succeeded or Failed"
May  7 10:35:37.796: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  7 10:35:37.825: INFO: Waiting for pod pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900 to disappear
May  7 10:35:37.828: INFO: Pod pod-projected-secrets-96f1ec79-f476-4f1a-85f3-862079042900 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:37.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-19" for this suite.

• [SLOW TEST:6.191 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:37.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-15589283-3e27-40bc-b825-88bacff2d2b7
STEP: Creating a pod to test consume configMaps
May  7 10:35:37.903: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62" in namespace "projected-1107" to be "Succeeded or Failed"
May  7 10:35:37.911: INFO: Pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62": Phase="Pending", Reason="", readiness=false. Elapsed: 7.971817ms
May  7 10:35:39.929: INFO: Pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025692519s
May  7 10:35:41.941: INFO: Pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62": Phase="Running", Reason="", readiness=true. Elapsed: 4.0372059s
May  7 10:35:43.949: INFO: Pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045575405s
STEP: Saw pod success
May  7 10:35:43.949: INFO: Pod "pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62" satisfied condition "Succeeded or Failed"
May  7 10:35:43.956: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62 container agnhost-container: <nil>
STEP: delete the pod
May  7 10:35:44.000: INFO: Waiting for pod pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62 to disappear
May  7 10:35:44.005: INFO: Pod pod-projected-configmaps-7d1ab4a2-ab84-4fab-b467-26544585cf62 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:44.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1107" for this suite.

• [SLOW TEST:6.188 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":780,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:44.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May  7 10:35:44.116: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:53.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1663" for this suite.

• [SLOW TEST:9.919 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":789,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May  7 10:35:54.033: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:54.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4980" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":46,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:54.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:35:54.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7052" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":47,"skipped":838,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:35:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May  7 10:36:00.872: INFO: Successfully updated pod "adopt-release-782cm"
STEP: Checking that the Job readopts the Pod
May  7 10:36:00.872: INFO: Waiting up to 15m0s for pod "adopt-release-782cm" in namespace "job-8213" to be "adopted"
May  7 10:36:00.904: INFO: Pod "adopt-release-782cm": Phase="Running", Reason="", readiness=true. Elapsed: 32.060496ms
May  7 10:36:02.915: INFO: Pod "adopt-release-782cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.042947989s
May  7 10:36:02.915: INFO: Pod "adopt-release-782cm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May  7 10:36:03.444: INFO: Successfully updated pod "adopt-release-782cm"
STEP: Checking that the Job releases the Pod
May  7 10:36:03.444: INFO: Waiting up to 15m0s for pod "adopt-release-782cm" in namespace "job-8213" to be "released"
May  7 10:36:03.487: INFO: Pod "adopt-release-782cm": Phase="Running", Reason="", readiness=true. Elapsed: 42.683701ms
May  7 10:36:03.487: INFO: Pod "adopt-release-782cm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:36:03.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8213" for this suite.

• [SLOW TEST:9.288 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":48,"skipped":840,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:36:03.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-664af612-8fa2-4f0f-be3d-11abb1ba015d
STEP: Creating a pod to test consume configMaps
May  7 10:36:03.583: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16" in namespace "projected-7790" to be "Succeeded or Failed"
May  7 10:36:03.587: INFO: Pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303113ms
May  7 10:36:05.606: INFO: Pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023303938s
May  7 10:36:07.616: INFO: Pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16": Phase="Running", Reason="", readiness=true. Elapsed: 4.033217789s
May  7 10:36:09.637: INFO: Pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05369592s
STEP: Saw pod success
May  7 10:36:09.637: INFO: Pod "pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16" satisfied condition "Succeeded or Failed"
May  7 10:36:09.644: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  7 10:36:09.682: INFO: Waiting for pod pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16 to disappear
May  7 10:36:09.694: INFO: Pod pod-projected-configmaps-82709e2b-7258-41dd-9e6c-15464f979b16 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:36:09.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7790" for this suite.

• [SLOW TEST:6.193 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":859,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:36:09.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-p2bz
STEP: Creating a pod to test atomic-volume-subpath
May  7 10:36:09.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p2bz" in namespace "subpath-4761" to be "Succeeded or Failed"
May  7 10:36:09.833: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Pending", Reason="", readiness=false. Elapsed: 11.040951ms
May  7 10:36:11.845: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023063805s
May  7 10:36:13.906: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08377283s
May  7 10:36:15.919: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 6.097011252s
May  7 10:36:17.929: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 8.106833394s
May  7 10:36:19.936: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 10.114011882s
May  7 10:36:21.945: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 12.123565054s
May  7 10:36:23.953: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 14.130969682s
May  7 10:36:25.960: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 16.137781676s
May  7 10:36:27.969: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 18.147485868s
May  7 10:36:29.977: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 20.154794156s
May  7 10:36:31.986: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 22.163876242s
May  7 10:36:33.993: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Running", Reason="", readiness=true. Elapsed: 24.170883892s
May  7 10:36:36.003: INFO: Pod "pod-subpath-test-downwardapi-p2bz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.181296414s
STEP: Saw pod success
May  7 10:36:36.003: INFO: Pod "pod-subpath-test-downwardapi-p2bz" satisfied condition "Succeeded or Failed"
May  7 10:36:36.007: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-subpath-test-downwardapi-p2bz container test-container-subpath-downwardapi-p2bz: <nil>
STEP: delete the pod
May  7 10:36:36.032: INFO: Waiting for pod pod-subpath-test-downwardapi-p2bz to disappear
May  7 10:36:36.043: INFO: Pod pod-subpath-test-downwardapi-p2bz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-p2bz
May  7 10:36:36.043: INFO: Deleting pod "pod-subpath-test-downwardapi-p2bz" in namespace "subpath-4761"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:36:36.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4761" for this suite.

• [SLOW TEST:26.336 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":50,"skipped":873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:36:36.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-acb6da75-5728-49d4-a75f-cd3881b69343
STEP: Creating the pod
May  7 10:36:36.126: INFO: The status of Pod pod-projected-configmaps-c777ea42-fb04-433b-9d4a-71cc71366a0e is Pending, waiting for it to be Running (with Ready = true)
May  7 10:36:38.136: INFO: The status of Pod pod-projected-configmaps-c777ea42-fb04-433b-9d4a-71cc71366a0e is Pending, waiting for it to be Running (with Ready = true)
May  7 10:36:40.131: INFO: The status of Pod pod-projected-configmaps-c777ea42-fb04-433b-9d4a-71cc71366a0e is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-acb6da75-5728-49d4-a75f-cd3881b69343
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3696" for this suite.

• [SLOW TEST:92.576 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":943,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:38:08.708: INFO: Endpoints addresses: [30.1.0.2] , ports: [8443]
May  7 10:38:08.708: INFO: EndpointSlices addresses: [30.1.0.2] , ports: [8443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:08.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9778" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":52,"skipped":950,"failed":0}

------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
May  7 10:38:08.763: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  7 10:38:08.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:09.903: INFO: stderr: ""
May  7 10:38:09.903: INFO: stdout: "service/agnhost-replica created\n"
May  7 10:38:09.903: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  7 10:38:09.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:10.152: INFO: stderr: ""
May  7 10:38:10.152: INFO: stdout: "service/agnhost-primary created\n"
May  7 10:38:10.152: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  7 10:38:10.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:10.465: INFO: stderr: ""
May  7 10:38:10.465: INFO: stdout: "service/frontend created\n"
May  7 10:38:10.466: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  7 10:38:10.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:10.685: INFO: stderr: ""
May  7 10:38:10.685: INFO: stdout: "deployment.apps/frontend created\n"
May  7 10:38:10.685: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  7 10:38:10.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:10.920: INFO: stderr: ""
May  7 10:38:10.920: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  7 10:38:10.920: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  7 10:38:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 create -f -'
May  7 10:38:11.144: INFO: stderr: ""
May  7 10:38:11.144: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May  7 10:38:11.144: INFO: Waiting for all frontend pods to be Running.
May  7 10:38:16.196: INFO: Waiting for frontend to serve content.
May  7 10:38:16.216: INFO: Trying to add a new entry to the guestbook.
May  7 10:38:16.238: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May  7 10:38:16.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.304: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May  7 10:38:17.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.410: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May  7 10:38:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.518: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  7 10:38:17.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.609: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.609: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  7 10:38:17.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.702: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.702: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May  7 10:38:17.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-4181 delete --grace-period=0 --force -f -'
May  7 10:38:17.801: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:38:17.801: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:17.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4181" for this suite.

• [SLOW TEST:9.104 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":53,"skipped":950,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:29.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4415" for this suite.

• [SLOW TEST:11.202 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":54,"skipped":951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:29.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:38:29.096: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c607deea-2941-432e-9eba-9219137fe9c6" in namespace "security-context-test-9870" to be "Succeeded or Failed"
May  7 10:38:29.101: INFO: Pod "busybox-user-65534-c607deea-2941-432e-9eba-9219137fe9c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.513146ms
May  7 10:38:31.108: INFO: Pod "busybox-user-65534-c607deea-2941-432e-9eba-9219137fe9c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011357971s
May  7 10:38:33.124: INFO: Pod "busybox-user-65534-c607deea-2941-432e-9eba-9219137fe9c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027950021s
May  7 10:38:33.125: INFO: Pod "busybox-user-65534-c607deea-2941-432e-9eba-9219137fe9c6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:33.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9870" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":1015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:33.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:43.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7802" for this suite.

• [SLOW TEST:10.098 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":56,"skipped":1078,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May  7 10:38:43.335: INFO: running pods: 0 < 1
May  7 10:38:45.349: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:38:47.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4311" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":57,"skipped":1079,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:38:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 10:38:47.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 10:38:49.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 10, 38, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 38, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 10, 38, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 38, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 10:38:52.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:39:03.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9952" for this suite.
STEP: Destroying namespace "webhook-9952-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":58,"skipped":1081,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:39:03.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:39:03.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: creating the pod
STEP: submitting the pod to kubernetes
May  7 10:39:03.422: INFO: The status of Pod pod-logs-websocket-dcf50915-3489-4b3d-b0ff-37c5ba905614 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:39:05.432: INFO: The status of Pod pod-logs-websocket-dcf50915-3489-4b3d-b0ff-37c5ba905614 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:39:07.432: INFO: The status of Pod pod-logs-websocket-dcf50915-3489-4b3d-b0ff-37c5ba905614 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:39:07.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6937" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:39:07.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3599
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-3599
May  7 10:39:07.571: INFO: Found 0 stateful pods, waiting for 1
May  7 10:39:17.586: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 10:39:17.651: INFO: Deleting all statefulset in ns statefulset-3599
May  7 10:39:17.658: INFO: Scaling statefulset ss to 0
May  7 10:39:27.762: INFO: Waiting for statefulset status.replicas updated to 0
May  7 10:39:27.767: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:39:27.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3599" for this suite.

• [SLOW TEST:20.328 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":60,"skipped":1120,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:39:27.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May  7 10:39:27.846: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 10:40:27.898: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:40:27.904: INFO: Starting informer...
STEP: Starting pods...
May  7 10:40:28.151: INFO: Pod1 is running on 5615889a-d356-43b5-a818-02fb040c6965. Tainting Node
May  7 10:40:32.194: INFO: Pod2 is running on 5615889a-d356-43b5-a818-02fb040c6965. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May  7 10:40:37.868: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  7 10:40:57.909: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:40:57.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7867" for this suite.

• [SLOW TEST:90.201 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":61,"skipped":1135,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:40:58.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-849
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-849 to expose endpoints map[]
May  7 10:40:58.174: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May  7 10:40:59.190: INFO: successfully validated that service multi-endpoint-test in namespace services-849 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-849
May  7 10:40:59.213: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:41:01.220: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:41:03.244: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-849 to expose endpoints map[pod1:[100]]
May  7 10:41:03.270: INFO: successfully validated that service multi-endpoint-test in namespace services-849 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-849
May  7 10:41:03.287: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:41:05.296: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:41:07.293: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-849 to expose endpoints map[pod1:[100] pod2:[101]]
May  7 10:41:07.312: INFO: successfully validated that service multi-endpoint-test in namespace services-849 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
May  7 10:41:07.312: INFO: Creating new exec pod
May  7 10:41:12.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-849 exec execpodth5wd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May  7 10:41:12.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  7 10:41:12.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:41:12.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-849 exec execpodth5wd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.160.26 80'
May  7 10:41:12.763: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.160.26 80\nConnection to 10.150.160.26 80 port [tcp/http] succeeded!\n"
May  7 10:41:12.763: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:41:12.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-849 exec execpodth5wd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May  7 10:41:12.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  7 10:41:12.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:41:12.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-849 exec execpodth5wd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.160.26 81'
May  7 10:41:13.088: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.160.26 81\nConnection to 10.150.160.26 81 port [tcp/*] succeeded!\n"
May  7 10:41:13.088: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-849
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-849 to expose endpoints map[pod2:[101]]
May  7 10:41:13.151: INFO: successfully validated that service multi-endpoint-test in namespace services-849 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-849
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-849 to expose endpoints map[]
May  7 10:41:13.194: INFO: successfully validated that service multi-endpoint-test in namespace services-849 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:13.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-849" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.236 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":62,"skipped":1147,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:13.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  7 10:41:13.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:13.374: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:14.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:14.385: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:15.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:15.390: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:16.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:41:16.389: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:17.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 10:41:17.388: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
May  7 10:41:17.395: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
May  7 10:41:17.406: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
May  7 10:41:17.410: INFO: Observed &DaemonSet event: ADDED
May  7 10:41:17.410: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.410: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.411: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.411: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.411: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.411: INFO: Found daemon set daemon-set in namespace daemonsets-764 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  7 10:41:17.411: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
May  7 10:41:17.423: INFO: Observed &DaemonSet event: ADDED
May  7 10:41:17.423: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.424: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.424: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.425: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.425: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.425: INFO: Observed daemon set daemon-set in namespace daemonsets-764 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  7 10:41:17.425: INFO: Observed &DaemonSet event: MODIFIED
May  7 10:41:17.425: INFO: Found daemon set daemon-set in namespace daemonsets-764 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  7 10:41:17.425: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-764, will wait for the garbage collector to delete the pods
May  7 10:41:17.492: INFO: Deleting DaemonSet.extensions daemon-set took: 8.447845ms
May  7 10:41:17.592: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.522676ms
May  7 10:41:21.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:21.200: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 10:41:21.204: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10296"},"items":null}

May  7 10:41:21.208: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10296"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:21.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-764" for this suite.

• [SLOW TEST:8.021 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":63,"skipped":1156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 10:41:21.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78948c58f6\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May  7 10:41:23.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 10, 41, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 10:41:26.913: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8604" for this suite.
STEP: Destroying namespace "webhook-8604-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":64,"skipped":1186,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:27.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-c5639a56-7b99-4d57-80ac-911fe527c3b3
STEP: Creating a pod to test consume configMaps
May  7 10:41:27.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1" in namespace "configmap-2314" to be "Succeeded or Failed"
May  7 10:41:27.150: INFO: Pod "pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110327ms
May  7 10:41:29.159: INFO: Pod "pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014652401s
May  7 10:41:31.167: INFO: Pod "pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022483647s
STEP: Saw pod success
May  7 10:41:31.167: INFO: Pod "pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1" satisfied condition "Succeeded or Failed"
May  7 10:41:31.172: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1 container agnhost-container: <nil>
STEP: delete the pod
May  7 10:41:31.203: INFO: Waiting for pod pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1 to disappear
May  7 10:41:31.212: INFO: Pod pod-configmaps-896329fb-af55-4a9f-b096-c7d0757272e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:31.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2314" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May  7 10:41:31.314: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May  7 10:41:31.321: INFO: starting watch
STEP: patching
STEP: updating
May  7 10:41:31.336: INFO: waiting for watch events with expected annotations
May  7 10:41:31.336: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:31.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6641" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":66,"skipped":1238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:31.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
May  7 10:41:32.080: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:32.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0507 10:41:32.080069      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-9357" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":67,"skipped":1292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:32.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-a4a0bd09-02f9-44b3-8c21-70f2cee17d07
STEP: Creating a pod to test consume secrets
May  7 10:41:32.173: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca" in namespace "projected-1253" to be "Succeeded or Failed"
May  7 10:41:32.183: INFO: Pod "pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.741599ms
May  7 10:41:34.189: INFO: Pod "pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016294493s
May  7 10:41:36.196: INFO: Pod "pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023496411s
STEP: Saw pod success
May  7 10:41:36.197: INFO: Pod "pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca" satisfied condition "Succeeded or Failed"
May  7 10:41:36.200: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca container projected-secret-volume-test: <nil>
STEP: delete the pod
May  7 10:41:36.217: INFO: Waiting for pod pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca to disappear
May  7 10:41:36.223: INFO: Pod pod-projected-secrets-956c9f8d-ae6a-41e2-b187-83a800abc6ca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:36.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1253" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:36.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  7 10:41:36.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:36.346: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:37.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:37.363: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:38.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 10:41:38.362: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:39.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 10:41:39.372: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 10:41:40.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 10:41:40.365: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
May  7 10:41:40.419: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10584"},"items":null}

May  7 10:41:40.424: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10584"},"items":[{"metadata":{"name":"daemon-set-ldm7z","generateName":"daemon-set-","namespace":"daemonsets-8398","uid":"65fcdcb8-0cf3-4f18-b296-de680baa108c","resourceVersion":"10580","creationTimestamp":"2022-05-07T10:41:36Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"55dbc562-4765-4d01-9e9f-517c4f053a3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55dbc562-4765-4d01-9e9f-517c4f053a3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.8.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nbvjd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nbvjd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"5615889a-d356-43b5-a818-02fb040c6965","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["5615889a-d356-43b5-a818-02fb040c6965"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"}],"hostIP":"30.1.0.5","podIP":"40.0.8.2","podIPs":[{"ip":"40.0.8.2"}],"startTime":"2022-05-07T10:41:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-07T10:41:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://47f297cfe99d1a6c33193871acf46ed83aea3517881f886b2de003dd267fe138","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nfqk7","generateName":"daemon-set-","namespace":"daemonsets-8398","uid":"7edf3de3-000a-43c8-9dae-4cda7591dc00","resourceVersion":"10582","creationTimestamp":"2022-05-07T10:41:36Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"55dbc562-4765-4d01-9e9f-517c4f053a3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55dbc562-4765-4d01-9e9f-517c4f053a3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.8.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tbbqh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tbbqh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"}],"hostIP":"30.1.0.3","podIP":"40.0.8.3","podIPs":[{"ip":"40.0.8.3"}],"startTime":"2022-05-07T10:41:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-07T10:41:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f8f49b14766714e779aca4dace205bc131216c9f31de6734a1855c16d3d4b32b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tw5jw","generateName":"daemon-set-","namespace":"daemonsets-8398","uid":"3c690d9f-102d-48a9-82a6-1a9a9b0d1c72","resourceVersion":"10574","creationTimestamp":"2022-05-07T10:41:36Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"55dbc562-4765-4d01-9e9f-517c4f053a3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55dbc562-4765-4d01-9e9f-517c4f053a3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-05-07T10:41:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.8.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zw8k2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zw8k2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-07T10:41:36Z"}],"hostIP":"30.1.0.4","podIP":"40.0.8.4","podIPs":[{"ip":"40.0.8.4"}],"startTime":"2022-05-07T10:41:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-07T10:41:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://606eedef07bbefffbcbd0f5532e33654c7b496b590975ba1aa2589c7837dde86","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:40.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8398" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":69,"skipped":1352,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:40.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:40.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2959" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":70,"skipped":1362,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:40.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:46.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9637" for this suite.
STEP: Destroying namespace "nsdeletetest-1831" for this suite.
May  7 10:41:46.824: INFO: Namespace nsdeletetest-1831 was already deleted
STEP: Destroying namespace "nsdeletetest-6153" for this suite.

• [SLOW TEST:6.284 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":71,"skipped":1363,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:46.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May  7 10:41:46.865: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  7 10:41:46.875: INFO: Waiting for terminating namespaces to be deleted...
May  7 10:41:46.879: INFO: 
Logging pods the apiserver thinks is on node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 before test
May  7 10:41:46.896: INFO: coredns-787c57488d-xh45r from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container coredns ready: true, restart count 0
May  7 10:41:46.896: INFO: event-controller-795755d67-lkvmj from pks-system started at 2022-05-07 10:26:54 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container event-controller ready: true, restart count 0
May  7 10:41:46.896: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 10:41:46.896: INFO: fluent-bit-lgdqp from pks-system started at 2022-05-07 10:27:00 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 10:41:46.896: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 10:41:46.896: INFO: node-exporter-cggqd from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 10:41:46.896: INFO: observability-manager-764bb5b6d9-5pw7g from pks-system started at 2022-05-07 10:26:47 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container observability-manager ready: true, restart count 0
May  7 10:41:46.896: INFO: sink-controller-77c8c69d54-vmhrt from pks-system started at 2022-05-07 10:26:54 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container sink-controller ready: true, restart count 0
May  7 10:41:46.896: INFO: telegraf-8f7tz from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.896: INFO: 	Container telegraf ready: true, restart count 0
May  7 10:41:46.896: INFO: telemetry-agent-6f6f75b47-qvqbh from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.897: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May  7 10:41:46.897: INFO: wavefront-collector-6b7bf647bb-zvh8w from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.897: INFO: 	Container wavefront-collector ready: true, restart count 0
May  7 10:41:46.897: INFO: sonobuoy from sonobuoy started at 2022-05-07 10:16:30 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.897: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  7 10:41:46.897: INFO: sonobuoy-e2e-job-fe2325511c6f494a from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.897: INFO: 	Container e2e ready: true, restart count 0
May  7 10:41:46.897: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 10:41:46.897: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.897: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 10:41:46.897: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 10:41:46.897: INFO: 
Logging pods the apiserver thinks is on node 5615889a-d356-43b5-a818-02fb040c6965 before test
May  7 10:41:46.913: INFO: coredns-787c57488d-2wsp6 from kube-system started at 2022-05-07 10:41:08 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.913: INFO: 	Container coredns ready: true, restart count 0
May  7 10:41:46.913: INFO: fluent-bit-7vdxl from pks-system started at 2022-05-07 10:40:58 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.913: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 10:41:46.913: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 10:41:46.913: INFO: node-exporter-2bp48 from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.913: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 10:41:46.913: INFO: telegraf-x4jms from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.913: INFO: 	Container telegraf ready: true, restart count 0
May  7 10:41:46.913: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-8dzqr from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 10:41:46.913: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 10:41:46.913: INFO: 
Logging pods the apiserver thinks is on node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 before test
May  7 10:41:46.935: INFO: coredns-787c57488d-mhgg5 from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container coredns ready: true, restart count 0
May  7 10:41:46.935: INFO: metrics-server-66c5bff789-gl2ww from kube-system started at 2022-05-07 10:26:48 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container metrics-server ready: true, restart count 0
May  7 10:41:46.935: INFO: fluent-bit-pzwr7 from pks-system started at 2022-05-07 10:27:01 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 10:41:46.935: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 10:41:46.935: INFO: metric-controller-669f9bc57b-tzdqq from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container metric-controller ready: true, restart count 0
May  7 10:41:46.935: INFO: node-exporter-bs69m from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 10:41:46.935: INFO: telegraf-dp5wl from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container telegraf ready: true, restart count 0
May  7 10:41:46.935: INFO: validator-8567d4b66-q8584 from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container validator ready: true, restart count 0
May  7 10:41:46.935: INFO: wavefront-proxy-54bfcd9d6b-4lb72 from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container wavefront-proxy ready: true, restart count 0
May  7 10:41:46.935: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-qtqsp from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 10:41:46.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 10:41:46.935: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f28f6e2b-178f-43e3-a544-b255800d47bd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f28f6e2b-178f-43e3-a544-b255800d47bd off the node 5615889a-d356-43b5-a818-02fb040c6965
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f28f6e2b-178f-43e3-a544-b255800d47bd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:41:57.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6419" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.261 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":72,"skipped":1367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:41:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-0f8bf630-9f23-4c78-8254-bdb22ba2869b in namespace container-probe-2966
May  7 10:42:03.191: INFO: Started pod liveness-0f8bf630-9f23-4c78-8254-bdb22ba2869b in namespace container-probe-2966
STEP: checking the pod's current state and verifying that restartCount is present
May  7 10:42:03.197: INFO: Initial restart count of pod liveness-0f8bf630-9f23-4c78-8254-bdb22ba2869b is 0
May  7 10:42:19.330: INFO: Restart count of pod container-probe-2966/liveness-0f8bf630-9f23-4c78-8254-bdb22ba2869b is now 1 (16.132706677s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:42:19.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2966" for this suite.

• [SLOW TEST:22.268 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:42:19.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
May  7 10:42:19.452: INFO: Waiting up to 5m0s for pod "pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3" in namespace "emptydir-3277" to be "Succeeded or Failed"
May  7 10:42:19.471: INFO: Pod "pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.862909ms
May  7 10:42:21.482: INFO: Pod "pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028931375s
May  7 10:42:23.490: INFO: Pod "pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037300772s
STEP: Saw pod success
May  7 10:42:23.490: INFO: Pod "pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3" satisfied condition "Succeeded or Failed"
May  7 10:42:23.493: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3 container test-container: <nil>
STEP: delete the pod
May  7 10:42:23.523: INFO: Waiting for pod pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3 to disappear
May  7 10:42:23.526: INFO: Pod pod-e718ed0f-5450-4a52-b2f8-07f76508e9f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:42:23.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3277" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1446,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:42:23.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6197
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6197
STEP: creating replication controller externalsvc in namespace services-6197
I0507 10:42:23.665865      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6197, replica count: 2
I0507 10:42:26.717613      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:42:29.718787      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May  7 10:42:29.745: INFO: Creating new exec pod
May  7 10:42:33.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6197 exec execpodqz56c -- /bin/sh -x -c nslookup nodeport-service.services-6197.svc.cluster.local'
May  7 10:42:34.001: INFO: stderr: "+ nslookup nodeport-service.services-6197.svc.cluster.local\n"
May  7 10:42:34.001: INFO: stdout: "Server:\t\t10.150.0.2\nAddress:\t10.150.0.2#53\n\nnodeport-service.services-6197.svc.cluster.local\tcanonical name = externalsvc.services-6197.svc.cluster.local.\nName:\texternalsvc.services-6197.svc.cluster.local\nAddress: 10.150.195.189\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6197, will wait for the garbage collector to delete the pods
May  7 10:42:34.070: INFO: Deleting ReplicationController externalsvc took: 13.038664ms
May  7 10:42:34.181: INFO: Terminating ReplicationController externalsvc pods took: 110.871655ms
May  7 10:42:36.513: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:42:36.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6197" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.023 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":75,"skipped":1449,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:42:36.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:42:36.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:42:43.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3815" for this suite.

• [SLOW TEST:6.494 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":76,"skipped":1460,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:42:43.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May  7 10:42:43.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5907 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  7 10:42:44.075: INFO: stderr: ""
May  7 10:42:44.075: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May  7 10:42:49.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5907 get pod e2e-test-httpd-pod -o json'
May  7 10:42:49.197: INFO: stderr: ""
May  7 10:42:49.197: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-05-07T10:42:44Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5907\",\n        \"resourceVersion\": \"11093\",\n        \"uid\": \"f13cef01-e6f2-460b-aaf7-523ff2be44b6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vgrgk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"5615889a-d356-43b5-a818-02fb040c6965\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vgrgk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-07T10:42:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-07T10:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-07T10:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-07T10:42:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://4cfb4a959e38801911ab4225133a3a746db024c44327877fb8b290531dd0c21f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-05-07T10:42:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"30.1.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"40.0.8.2\",\n        \"podIPs\": [\n            {\n                \"ip\": \"40.0.8.2\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-05-07T10:42:44Z\"\n    }\n}\n"
STEP: replace the image in the pod
May  7 10:42:49.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5907 replace -f -'
May  7 10:42:50.290: INFO: stderr: ""
May  7 10:42:50.290: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
May  7 10:42:50.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-5907 delete pods e2e-test-httpd-pod'
May  7 10:42:52.316: INFO: stderr: ""
May  7 10:42:52.316: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:42:52.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5907" for this suite.

• [SLOW TEST:9.283 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":77,"skipped":1469,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:42:52.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
May  7 10:42:52.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  7 10:42:52.476: INFO: stderr: ""
May  7 10:42:52.476: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
May  7 10:42:52.476: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  7 10:42:52.476: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6021" to be "running and ready, or succeeded"
May  7 10:42:52.482: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.447124ms
May  7 10:42:54.494: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017747776s
May  7 10:42:56.502: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.026420835s
May  7 10:42:56.502: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  7 10:42:56.503: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May  7 10:42:56.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator'
May  7 10:42:56.590: INFO: stderr: ""
May  7 10:42:56.590: INFO: stdout: "I0507 10:42:55.324981       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffgg 478\nI0507 10:42:55.525225       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/d2kv 389\nI0507 10:42:55.726022       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/lv2 478\nI0507 10:42:55.925538       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/7x5 524\nI0507 10:42:56.126037       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5qj 344\nI0507 10:42:56.325538       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/m67z 220\nI0507 10:42:56.526100       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/75mt 313\n"
STEP: limiting log lines
May  7 10:42:56.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator --tail=1'
May  7 10:42:56.681: INFO: stderr: ""
May  7 10:42:56.681: INFO: stdout: "I0507 10:42:56.526100       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/75mt 313\n"
May  7 10:42:56.681: INFO: got output "I0507 10:42:56.526100       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/75mt 313\n"
STEP: limiting log bytes
May  7 10:42:56.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator --limit-bytes=1'
May  7 10:42:56.777: INFO: stderr: ""
May  7 10:42:56.777: INFO: stdout: "I"
May  7 10:42:56.777: INFO: got output "I"
STEP: exposing timestamps
May  7 10:42:56.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator --tail=1 --timestamps'
May  7 10:42:56.859: INFO: stderr: ""
May  7 10:42:56.859: INFO: stdout: "2022-05-07T10:42:56.725667525Z I0507 10:42:56.725504       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/272r 411\n"
May  7 10:42:56.859: INFO: got output "2022-05-07T10:42:56.725667525Z I0507 10:42:56.725504       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/272r 411\n"
STEP: restricting to a time range
May  7 10:42:59.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator --since=1s'
May  7 10:42:59.441: INFO: stderr: ""
May  7 10:42:59.441: INFO: stdout: "I0507 10:42:58.525192       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/wdm 313\nI0507 10:42:58.725802       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/j5g 214\nI0507 10:42:58.925170       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/pnbl 470\nI0507 10:42:59.125628       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ssd 213\nI0507 10:42:59.326082       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/q99h 397\n"
May  7 10:42:59.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 logs logs-generator logs-generator --since=24h'
May  7 10:42:59.530: INFO: stderr: ""
May  7 10:42:59.530: INFO: stdout: "I0507 10:42:55.324981       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/ffgg 478\nI0507 10:42:55.525225       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/d2kv 389\nI0507 10:42:55.726022       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/lv2 478\nI0507 10:42:55.925538       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/7x5 524\nI0507 10:42:56.126037       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/5qj 344\nI0507 10:42:56.325538       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/m67z 220\nI0507 10:42:56.526100       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/75mt 313\nI0507 10:42:56.725504       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/272r 411\nI0507 10:42:56.925126       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/q7jn 587\nI0507 10:42:57.125565       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/6x4 559\nI0507 10:42:57.326039       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/2ql 262\nI0507 10:42:57.525543       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/lxlh 539\nI0507 10:42:57.726082       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/g5kn 362\nI0507 10:42:57.925633       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/5gp9 294\nI0507 10:42:58.126078       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/c2vm 201\nI0507 10:42:58.325848       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/6mm 377\nI0507 10:42:58.525192       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/wdm 313\nI0507 10:42:58.725802       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/j5g 214\nI0507 10:42:58.925170       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/pnbl 470\nI0507 10:42:59.125628       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ssd 213\nI0507 10:42:59.326082       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/q99h 397\nI0507 10:42:59.525720       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/hv8m 226\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
May  7 10:42:59.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6021 delete pod logs-generator'
May  7 10:43:01.327: INFO: stderr: ""
May  7 10:43:01.327: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:01.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6021" for this suite.

• [SLOW TEST:8.996 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":78,"skipped":1487,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 10:43:01.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3" in namespace "projected-6741" to be "Succeeded or Failed"
May  7 10:43:01.457: INFO: Pod "downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.104083ms
May  7 10:43:03.467: INFO: Pod "downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020575283s
May  7 10:43:05.478: INFO: Pod "downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031591498s
STEP: Saw pod success
May  7 10:43:05.478: INFO: Pod "downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3" satisfied condition "Succeeded or Failed"
May  7 10:43:05.482: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3 container client-container: <nil>
STEP: delete the pod
May  7 10:43:05.504: INFO: Waiting for pod downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3 to disappear
May  7 10:43:05.514: INFO: Pod downwardapi-volume-fe450bf9-523a-4e6c-bbf7-e58d50b36ee3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:05.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6741" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1497,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:05.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:05.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3355" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":80,"skipped":1502,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:05.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5176, will wait for the garbage collector to delete the pods
May  7 10:43:11.782: INFO: Deleting Job.batch foo took: 13.123006ms
May  7 10:43:11.882: INFO: Terminating Job.batch foo pods took: 100.628473ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:43.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5176" for this suite.

• [SLOW TEST:37.957 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":81,"skipped":1515,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:43.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:43.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8376" for this suite.
STEP: Destroying namespace "nspatchtest-d7fdcd4a-7b20-4764-9426-713720e4c9aa-3527" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":82,"skipped":1521,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:43.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May  7 10:43:43.812: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:43:45.826: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:43:47.821: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:43:49.821: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May  7 10:43:49.852: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:43:51.859: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:43:53.860: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May  7 10:43:53.872: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  7 10:43:53.882: INFO: Pod pod-with-prestop-exec-hook still exists
May  7 10:43:55.883: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  7 10:43:55.893: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:43:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2391" for this suite.

• [SLOW TEST:12.207 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:43:55.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May  7 10:43:55.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 create -f -'
May  7 10:43:56.245: INFO: stderr: ""
May  7 10:43:56.245: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  7 10:43:56.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 10:43:56.361: INFO: stderr: ""
May  7 10:43:56.361: INFO: stdout: "update-demo-nautilus-2bqx8 update-demo-nautilus-dkjs7 "
May  7 10:43:56.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-2bqx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 10:43:56.438: INFO: stderr: ""
May  7 10:43:56.438: INFO: stdout: ""
May  7 10:43:56.439: INFO: update-demo-nautilus-2bqx8 is created but not running
May  7 10:44:01.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 10:44:01.513: INFO: stderr: ""
May  7 10:44:01.513: INFO: stdout: "update-demo-nautilus-2bqx8 update-demo-nautilus-dkjs7 "
May  7 10:44:01.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-2bqx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 10:44:01.585: INFO: stderr: ""
May  7 10:44:01.585: INFO: stdout: ""
May  7 10:44:01.585: INFO: update-demo-nautilus-2bqx8 is created but not running
May  7 10:44:06.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 10:44:06.669: INFO: stderr: ""
May  7 10:44:06.669: INFO: stdout: "update-demo-nautilus-2bqx8 update-demo-nautilus-dkjs7 "
May  7 10:44:06.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-2bqx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 10:44:06.744: INFO: stderr: ""
May  7 10:44:06.744: INFO: stdout: "true"
May  7 10:44:06.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-2bqx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 10:44:06.811: INFO: stderr: ""
May  7 10:44:06.811: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 10:44:06.811: INFO: validating pod update-demo-nautilus-2bqx8
May  7 10:44:06.822: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 10:44:06.822: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 10:44:06.823: INFO: update-demo-nautilus-2bqx8 is verified up and running
May  7 10:44:06.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-dkjs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 10:44:06.898: INFO: stderr: ""
May  7 10:44:06.898: INFO: stdout: "true"
May  7 10:44:06.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods update-demo-nautilus-dkjs7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 10:44:06.978: INFO: stderr: ""
May  7 10:44:06.978: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 10:44:06.978: INFO: validating pod update-demo-nautilus-dkjs7
May  7 10:44:06.984: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 10:44:06.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 10:44:06.984: INFO: update-demo-nautilus-dkjs7 is verified up and running
STEP: using delete to clean up resources
May  7 10:44:06.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 delete --grace-period=0 --force -f -'
May  7 10:44:07.072: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 10:44:07.072: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  7 10:44:07.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get rc,svc -l name=update-demo --no-headers'
May  7 10:44:07.186: INFO: stderr: "No resources found in kubectl-569 namespace.\n"
May  7 10:44:07.186: INFO: stdout: ""
May  7 10:44:07.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-569 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  7 10:44:07.274: INFO: stderr: ""
May  7 10:44:07.274: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:44:07.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-569" for this suite.

• [SLOW TEST:11.366 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":84,"skipped":1590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:44:07.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1279
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  7 10:44:07.375: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  7 10:44:07.488: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:44:09.495: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:44:11.496: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:44:13.499: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:15.501: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:17.498: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:19.498: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:21.502: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:23.500: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:25.501: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:44:27.497: INFO: The status of Pod netserver-0 is Running (Ready = true)
May  7 10:44:27.511: INFO: The status of Pod netserver-1 is Running (Ready = true)
May  7 10:44:27.521: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May  7 10:44:31.577: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  7 10:44:31.577: INFO: Going to poll 40.0.8.3 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  7 10:44:31.579: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://40.0.8.3:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:44:31.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:44:31.580: INFO: ExecWithOptions: Clientset creation
May  7 10:44:31.580: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F40.0.8.3%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 10:44:31.691: INFO: Found all 1 expected endpoints: [netserver-0]
May  7 10:44:31.691: INFO: Going to poll 40.0.8.2 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  7 10:44:31.696: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://40.0.8.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:44:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:44:31.697: INFO: ExecWithOptions: Clientset creation
May  7 10:44:31.697: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F40.0.8.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 10:44:31.801: INFO: Found all 1 expected endpoints: [netserver-1]
May  7 10:44:31.801: INFO: Going to poll 40.0.8.4 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  7 10:44:31.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://40.0.8.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:44:31.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:44:31.807: INFO: ExecWithOptions: Clientset creation
May  7 10:44:31.807: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-1279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F40.0.8.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 10:44:31.899: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:44:31.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1279" for this suite.

• [SLOW TEST:24.620 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1653,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:44:31.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 10:44:32.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178" in namespace "downward-api-3218" to be "Succeeded or Failed"
May  7 10:44:32.031: INFO: Pod "downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062498ms
May  7 10:44:34.039: INFO: Pod "downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015692s
May  7 10:44:36.051: INFO: Pod "downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027565612s
STEP: Saw pod success
May  7 10:44:36.051: INFO: Pod "downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178" satisfied condition "Succeeded or Failed"
May  7 10:44:36.056: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178 container client-container: <nil>
STEP: delete the pod
May  7 10:44:36.104: INFO: Waiting for pod downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178 to disappear
May  7 10:44:36.107: INFO: Pod downwardapi-volume-28b4a95c-9360-424d-a195-0e7ba9525178 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:44:36.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3218" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1654,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:44:36.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May  7 10:44:46.259: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0507 10:44:46.259043      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:44:46.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9125" for this suite.

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":87,"skipped":1656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:44:46.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:44:53.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4289" for this suite.

• [SLOW TEST:7.129 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":88,"skipped":1685,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:44:53.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May  7 10:44:59.642: INFO: 80 pods remaining
May  7 10:44:59.642: INFO: 80 pods has nil DeletionTimestamp
May  7 10:44:59.642: INFO: 
May  7 10:45:00.681: INFO: 71 pods remaining
May  7 10:45:00.681: INFO: 69 pods has nil DeletionTimestamp
May  7 10:45:00.681: INFO: 
May  7 10:45:02.450: INFO: 60 pods remaining
May  7 10:45:02.450: INFO: 58 pods has nil DeletionTimestamp
May  7 10:45:02.450: INFO: 
May  7 10:45:02.723: INFO: 40 pods remaining
May  7 10:45:02.723: INFO: 40 pods has nil DeletionTimestamp
May  7 10:45:02.723: INFO: 
May  7 10:45:03.596: INFO: 30 pods remaining
May  7 10:45:03.596: INFO: 29 pods has nil DeletionTimestamp
May  7 10:45:03.596: INFO: 
May  7 10:45:04.620: INFO: 20 pods remaining
May  7 10:45:04.620: INFO: 20 pods has nil DeletionTimestamp
May  7 10:45:04.620: INFO: 
STEP: Gathering metrics
May  7 10:45:05.783: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0507 10:45:05.783769      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:45:05.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8136" for this suite.

• [SLOW TEST:12.401 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":89,"skipped":1688,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:45:05.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:45:23.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3517" for this suite.
STEP: Destroying namespace "nsdeletetest-6408" for this suite.
May  7 10:45:23.108: INFO: Namespace nsdeletetest-6408 was already deleted
STEP: Destroying namespace "nsdeletetest-9255" for this suite.

• [SLOW TEST:17.302 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":90,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:45:23.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4830
STEP: creating service affinity-clusterip in namespace services-4830
STEP: creating replication controller affinity-clusterip in namespace services-4830
I0507 10:45:23.193787      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4830, replica count: 3
I0507 10:45:26.246456      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 10:45:29.247660      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 10:45:29.259: INFO: Creating new exec pod
May  7 10:45:34.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4830 exec execpod-affinity2wxm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May  7 10:45:34.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  7 10:45:34.450: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:45:34.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4830 exec execpod-affinity2wxm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.100.182 80'
May  7 10:45:34.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.100.182 80\nConnection to 10.150.100.182 80 port [tcp/http] succeeded!\n"
May  7 10:45:34.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 10:45:34.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4830 exec execpod-affinity2wxm9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.100.182:80/ ; done'
May  7 10:45:34.856: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.100.182:80/\n"
May  7 10:45:34.856: INFO: stdout: "\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n\naffinity-clusterip-dgn6n"
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Received response from host: affinity-clusterip-dgn6n
May  7 10:45:34.856: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4830, will wait for the garbage collector to delete the pods
May  7 10:45:34.944: INFO: Deleting ReplicationController affinity-clusterip took: 8.221617ms
May  7 10:45:35.145: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.998318ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:45:37.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4830" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.524 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":91,"skipped":1726,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:45:37.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
May  7 10:45:37.721: INFO: Pod name sample-pod: Found 0 pods out of 1
May  7 10:45:42.738: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
May  7 10:45:42.744: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
May  7 10:45:42.759: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
May  7 10:45:42.762: INFO: Observed &ReplicaSet event: ADDED
May  7 10:45:42.762: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.763: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.763: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.763: INFO: Found replicaset test-rs in namespace replicaset-662 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  7 10:45:42.763: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
May  7 10:45:42.763: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  7 10:45:42.771: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
May  7 10:45:42.776: INFO: Observed &ReplicaSet event: ADDED
May  7 10:45:42.776: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.776: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.777: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.777: INFO: Observed replicaset test-rs in namespace replicaset-662 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  7 10:45:42.777: INFO: Observed &ReplicaSet event: MODIFIED
May  7 10:45:42.777: INFO: Found replicaset test-rs in namespace replicaset-662 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  7 10:45:42.777: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:45:42.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-662" for this suite.

• [SLOW TEST:5.152 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":92,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:45:42.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-8d86efad-991a-4acf-b61f-bfb023e25c0c in namespace container-probe-9253
May  7 10:45:46.911: INFO: Started pod busybox-8d86efad-991a-4acf-b61f-bfb023e25c0c in namespace container-probe-9253
STEP: checking the pod's current state and verifying that restartCount is present
May  7 10:45:46.915: INFO: Initial restart count of pod busybox-8d86efad-991a-4acf-b61f-bfb023e25c0c is 0
May  7 10:46:35.181: INFO: Restart count of pod container-probe-9253/busybox-8d86efad-991a-4acf-b61f-bfb023e25c0c is now 1 (48.265911147s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9253" for this suite.

• [SLOW TEST:52.410 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1804,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:35.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May  7 10:46:35.277: INFO: Waiting up to 5m0s for pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe" in namespace "emptydir-2733" to be "Succeeded or Failed"
May  7 10:46:35.282: INFO: Pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532161ms
May  7 10:46:37.293: INFO: Pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015249091s
May  7 10:46:39.298: INFO: Pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020265918s
May  7 10:46:41.306: INFO: Pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028272696s
STEP: Saw pod success
May  7 10:46:41.306: INFO: Pod "pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe" satisfied condition "Succeeded or Failed"
May  7 10:46:41.309: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe container test-container: <nil>
STEP: delete the pod
May  7 10:46:41.347: INFO: Waiting for pod pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe to disappear
May  7 10:46:41.355: INFO: Pod pod-fc055bb5-2862-4f1f-ac8c-e220ecd2fdbe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:41.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2733" for this suite.

• [SLOW TEST:6.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1819,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:41.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-86f6b538-e1d0-42d2-98ce-80bb33ab7491
STEP: Creating a pod to test consume configMaps
May  7 10:46:41.460: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724" in namespace "projected-8680" to be "Succeeded or Failed"
May  7 10:46:41.469: INFO: Pod "pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723721ms
May  7 10:46:43.476: INFO: Pod "pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015614014s
May  7 10:46:45.492: INFO: Pod "pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031831234s
STEP: Saw pod success
May  7 10:46:45.492: INFO: Pod "pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724" satisfied condition "Succeeded or Failed"
May  7 10:46:45.495: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724 container agnhost-container: <nil>
STEP: delete the pod
May  7 10:46:45.530: INFO: Waiting for pod pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724 to disappear
May  7 10:46:45.534: INFO: Pod pod-projected-configmaps-e906f3b1-395e-4070-abe1-770dd9672724 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:45.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8680" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1821,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:45.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May  7 10:46:46.674: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0507 10:46:46.674791      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:46.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2820" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":96,"skipped":1821,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May  7 10:46:46.752: INFO: The status of Pod annotationupdated7dee0af-16c8-4640-9be8-e8b0ae907b08 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:46:48.758: INFO: The status of Pod annotationupdated7dee0af-16c8-4640-9be8-e8b0ae907b08 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:46:50.757: INFO: The status of Pod annotationupdated7dee0af-16c8-4640-9be8-e8b0ae907b08 is Running (Ready = true)
May  7 10:46:51.292: INFO: Successfully updated pod "annotationupdated7dee0af-16c8-4640-9be8-e8b0ae907b08"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:53.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8009" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5c5476be-d94e-41ce-8283-8ced2f4e41f5
STEP: Creating a pod to test consume secrets
May  7 10:46:53.416: INFO: Waiting up to 5m0s for pod "pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51" in namespace "secrets-2779" to be "Succeeded or Failed"
May  7 10:46:53.433: INFO: Pod "pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51": Phase="Pending", Reason="", readiness=false. Elapsed: 16.461423ms
May  7 10:46:55.443: INFO: Pod "pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026211995s
May  7 10:46:57.460: INFO: Pod "pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043476941s
STEP: Saw pod success
May  7 10:46:57.461: INFO: Pod "pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51" satisfied condition "Succeeded or Failed"
May  7 10:46:57.468: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51 container secret-volume-test: <nil>
STEP: delete the pod
May  7 10:46:57.494: INFO: Waiting for pod pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51 to disappear
May  7 10:46:57.500: INFO: Pod pod-secrets-fc2eb284-cf2f-4ef8-80d1-6b53efc50a51 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:46:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2779" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:46:57.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:46:57.597: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5817a0cf-8a34-463b-9f77-117ecad25484" in namespace "security-context-test-1452" to be "Succeeded or Failed"
May  7 10:46:57.613: INFO: Pod "busybox-readonly-false-5817a0cf-8a34-463b-9f77-117ecad25484": Phase="Pending", Reason="", readiness=false. Elapsed: 15.317579ms
May  7 10:46:59.621: INFO: Pod "busybox-readonly-false-5817a0cf-8a34-463b-9f77-117ecad25484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023843437s
May  7 10:47:01.633: INFO: Pod "busybox-readonly-false-5817a0cf-8a34-463b-9f77-117ecad25484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035083757s
May  7 10:47:01.633: INFO: Pod "busybox-readonly-false-5817a0cf-8a34-463b-9f77-117ecad25484" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:47:01.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1452" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:47:01.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:47:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Creating first CR 
May  7 10:47:04.323: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:04Z]] name:name1 resourceVersion:13476 uid:8d8430d9-6bb3-4050-9480-e3107a50df81] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May  7 10:47:14.332: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:14Z]] name:name2 resourceVersion:13506 uid:fc0000fa-2a0b-491a-991e-bca3ec39a092] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May  7 10:47:24.341: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:24Z]] name:name1 resourceVersion:13524 uid:8d8430d9-6bb3-4050-9480-e3107a50df81] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May  7 10:47:34.352: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:34Z]] name:name2 resourceVersion:13541 uid:fc0000fa-2a0b-491a-991e-bca3ec39a092] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May  7 10:47:44.362: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:24Z]] name:name1 resourceVersion:13557 uid:8d8430d9-6bb3-4050-9480-e3107a50df81] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May  7 10:47:54.371: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-07T10:47:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-07T10:47:34Z]] name:name2 resourceVersion:13574 uid:fc0000fa-2a0b-491a-991e-bca3ec39a092] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:48:04.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-180" for this suite.

• [SLOW TEST:63.285 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":100,"skipped":1996,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:48:04.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  7 10:48:05.020: INFO: Waiting up to 5m0s for pod "pod-32492de5-615a-4ea3-88bf-ce0d46137dce" in namespace "emptydir-6291" to be "Succeeded or Failed"
May  7 10:48:05.030: INFO: Pod "pod-32492de5-615a-4ea3-88bf-ce0d46137dce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.282094ms
May  7 10:48:07.042: INFO: Pod "pod-32492de5-615a-4ea3-88bf-ce0d46137dce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022237046s
May  7 10:48:09.051: INFO: Pod "pod-32492de5-615a-4ea3-88bf-ce0d46137dce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031563301s
STEP: Saw pod success
May  7 10:48:09.052: INFO: Pod "pod-32492de5-615a-4ea3-88bf-ce0d46137dce" satisfied condition "Succeeded or Failed"
May  7 10:48:09.055: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-32492de5-615a-4ea3-88bf-ce0d46137dce container test-container: <nil>
STEP: delete the pod
May  7 10:48:09.083: INFO: Waiting for pod pod-32492de5-615a-4ea3-88bf-ce0d46137dce to disappear
May  7 10:48:09.087: INFO: Pod pod-32492de5-615a-4ea3-88bf-ce0d46137dce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:48:09.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6291" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":2004,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:48:09.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May  7 10:48:09.188: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:11.193: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:13.194: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May  7 10:48:13.208: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:15.216: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:17.217: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  7 10:48:17.235: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  7 10:48:17.240: INFO: Pod pod-with-poststart-exec-hook still exists
May  7 10:48:19.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  7 10:48:19.248: INFO: Pod pod-with-poststart-exec-hook still exists
May  7 10:48:21.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  7 10:48:21.247: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:48:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4263" for this suite.

• [SLOW TEST:12.164 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":2005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:48:21.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f
May  7 10:48:21.363: INFO: Pod name my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f: Found 0 pods out of 1
May  7 10:48:26.374: INFO: Pod name my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f: Found 1 pods out of 1
May  7 10:48:26.375: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f" are running
May  7 10:48:26.386: INFO: Pod "my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f-q574v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:48:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:48:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:48:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-07 10:48:21 +0000 UTC Reason: Message:}])
May  7 10:48:26.386: INFO: Trying to dial the pod
May  7 10:48:31.406: INFO: Controller my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f: Got expected result from replica 1 [my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f-q574v]: "my-hostname-basic-0dfd29e8-19d6-4aa3-be5e-7b3909fb7b9f-q574v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:48:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-995" for this suite.

• [SLOW TEST:10.154 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":103,"skipped":2051,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:48:31.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May  7 10:48:31.497: INFO: The status of Pod pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:33.510: INFO: The status of Pod pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:35.502: INFO: The status of Pod pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:37.507: INFO: The status of Pod pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  7 10:48:38.055: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed"
May  7 10:48:38.055: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed" in namespace "pods-8975" to be "terminated due to deadline exceeded"
May  7 10:48:38.058: INFO: Pod "pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed": Phase="Running", Reason="", readiness=true. Elapsed: 2.929256ms
May  7 10:48:40.065: INFO: Pod "pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009690661s
May  7 10:48:40.065: INFO: Pod "pod-update-activedeadlineseconds-d1ae9fe3-5e0c-40f1-b134-2fe3057b6bed" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:48:40.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8975" for this suite.

• [SLOW TEST:8.652 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":2054,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:48:40.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4360
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  7 10:48:40.114: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  7 10:48:40.183: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:42.202: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:48:44.194: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:46.195: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:48.193: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:50.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:52.192: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:54.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:56.195: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:48:58.196: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 10:49:00.197: INFO: The status of Pod netserver-0 is Running (Ready = true)
May  7 10:49:00.216: INFO: The status of Pod netserver-1 is Running (Ready = true)
May  7 10:49:00.228: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May  7 10:49:04.258: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  7 10:49:04.258: INFO: Breadth first check of 40.0.7.3 on host 30.1.0.3...
May  7 10:49:04.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.7.5:9080/dial?request=hostname&protocol=udp&host=40.0.7.3&port=8081&tries=1'] Namespace:pod-network-test-4360 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:49:04.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:49:04.268: INFO: ExecWithOptions: Clientset creation
May  7 10:49:04.268: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-4360/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.7.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D40.0.7.3%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:49:04.381: INFO: Waiting for responses: map[]
May  7 10:49:04.382: INFO: reached 40.0.7.3 after 0/1 tries
May  7 10:49:04.382: INFO: Breadth first check of 40.0.7.2 on host 30.1.0.5...
May  7 10:49:04.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.7.5:9080/dial?request=hostname&protocol=udp&host=40.0.7.2&port=8081&tries=1'] Namespace:pod-network-test-4360 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:49:04.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:49:04.388: INFO: ExecWithOptions: Clientset creation
May  7 10:49:04.388: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-4360/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.7.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D40.0.7.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:49:04.474: INFO: Waiting for responses: map[]
May  7 10:49:04.475: INFO: reached 40.0.7.2 after 0/1 tries
May  7 10:49:04.475: INFO: Breadth first check of 40.0.7.4 on host 30.1.0.4...
May  7 10:49:04.480: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://40.0.7.5:9080/dial?request=hostname&protocol=udp&host=40.0.7.4&port=8081&tries=1'] Namespace:pod-network-test-4360 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 10:49:04.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 10:49:04.481: INFO: ExecWithOptions: Clientset creation
May  7 10:49:04.481: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-4360/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F40.0.7.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D40.0.7.4%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May  7 10:49:04.589: INFO: Waiting for responses: map[]
May  7 10:49:04.589: INFO: reached 40.0.7.4 after 0/1 tries
May  7 10:49:04.589: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:49:04.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4360" for this suite.

• [SLOW TEST:24.527 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2064,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:49:04.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
May  7 10:49:04.675: INFO: The status of Pod pod-hostip-794cd48f-fe96-4890-ab35-2403f141e6c6 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:49:06.685: INFO: The status of Pod pod-hostip-794cd48f-fe96-4890-ab35-2403f141e6c6 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:49:08.682: INFO: The status of Pod pod-hostip-794cd48f-fe96-4890-ab35-2403f141e6c6 is Running (Ready = true)
May  7 10:49:08.688: INFO: Pod pod-hostip-794cd48f-fe96-4890-ab35-2403f141e6c6 has hostIP: 30.1.0.5
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:49:08.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-313" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":2081,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:49:08.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-f7394a08-c626-4042-9b87-d57ca68b4005 in namespace container-probe-4722
May  7 10:49:12.787: INFO: Started pod liveness-f7394a08-c626-4042-9b87-d57ca68b4005 in namespace container-probe-4722
STEP: checking the pod's current state and verifying that restartCount is present
May  7 10:49:12.792: INFO: Initial restart count of pod liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is 0
May  7 10:49:31.498: INFO: Restart count of pod container-probe-4722/liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is now 1 (18.706589634s elapsed)
May  7 10:49:51.589: INFO: Restart count of pod container-probe-4722/liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is now 2 (38.797028264s elapsed)
May  7 10:50:11.696: INFO: Restart count of pod container-probe-4722/liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is now 3 (58.904047923s elapsed)
May  7 10:50:31.786: INFO: Restart count of pod container-probe-4722/liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is now 4 (1m18.994661644s elapsed)
May  7 10:51:42.192: INFO: Restart count of pod container-probe-4722/liveness-f7394a08-c626-4042-9b87-d57ca68b4005 is now 5 (2m29.400713901s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:51:42.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4722" for this suite.

• [SLOW TEST:153.522 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2087,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:51:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3192
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3192
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3192
May  7 10:51:42.293: INFO: Found 0 stateful pods, waiting for 1
May  7 10:51:52.304: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May  7 10:51:52.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 10:51:52.516: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 10:51:52.516: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 10:51:52.516: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 10:51:52.521: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  7 10:52:02.539: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  7 10:52:02.539: INFO: Waiting for statefulset status.replicas updated to 0
May  7 10:52:02.568: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May  7 10:52:02.568: INFO: ss-0  5615889a-d356-43b5-a818-02fb040c6965  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  }]
May  7 10:52:02.568: INFO: 
May  7 10:52:02.568: INFO: StatefulSet ss has not reached scale 3, at 1
May  7 10:52:03.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990134346s
May  7 10:52:04.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983000606s
May  7 10:52:05.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973247849s
May  7 10:52:06.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964397917s
May  7 10:52:07.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955706769s
May  7 10:52:08.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943172039s
May  7 10:52:09.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.937115566s
May  7 10:52:10.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927209975s
May  7 10:52:11.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.765872ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3192
May  7 10:52:12.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 10:52:12.815: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 10:52:12.815: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 10:52:12.815: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 10:52:12.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 10:52:12.986: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  7 10:52:12.986: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 10:52:12.986: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 10:52:12.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 10:52:13.159: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  7 10:52:13.159: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 10:52:13.159: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 10:52:13.165: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May  7 10:52:23.179: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 10:52:23.179: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 10:52:23.179: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May  7 10:52:23.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 10:52:23.333: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 10:52:23.333: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 10:52:23.333: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 10:52:23.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 10:52:23.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 10:52:23.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 10:52:23.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 10:52:23.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-3192 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 10:52:23.714: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 10:52:23.714: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 10:52:23.714: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 10:52:23.714: INFO: Waiting for statefulset status.replicas updated to 0
May  7 10:52:23.718: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May  7 10:52:33.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  7 10:52:33.741: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  7 10:52:33.741: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  7 10:52:33.759: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May  7 10:52:33.759: INFO: ss-0  5615889a-d356-43b5-a818-02fb040c6965  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  }]
May  7 10:52:33.759: INFO: ss-1  99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  }]
May  7 10:52:33.759: INFO: ss-2  17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  }]
May  7 10:52:33.759: INFO: 
May  7 10:52:33.759: INFO: StatefulSet ss has not reached scale 0, at 3
May  7 10:52:34.770: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
May  7 10:52:34.770: INFO: ss-0  5615889a-d356-43b5-a818-02fb040c6965  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:51:42 +0000 UTC  }]
May  7 10:52:34.770: INFO: ss-1  99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 10:52:02 +0000 UTC  }]
May  7 10:52:34.770: INFO: 
May  7 10:52:34.770: INFO: StatefulSet ss has not reached scale 0, at 2
May  7 10:52:35.775: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98441385s
May  7 10:52:36.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979515667s
May  7 10:52:37.790: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972947657s
May  7 10:52:38.796: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963466796s
May  7 10:52:39.802: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.958981221s
May  7 10:52:40.807: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.952424363s
May  7 10:52:41.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947514553s
May  7 10:52:42.823: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.022655ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3192
May  7 10:52:43.828: INFO: Scaling statefulset ss to 0
May  7 10:52:43.840: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 10:52:43.843: INFO: Deleting all statefulset in ns statefulset-3192
May  7 10:52:43.845: INFO: Scaling statefulset ss to 0
May  7 10:52:43.854: INFO: Waiting for statefulset status.replicas updated to 0
May  7 10:52:43.856: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:52:43.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3192" for this suite.

• [SLOW TEST:61.685 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":108,"skipped":2094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:52:43.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3645.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 10:52:58.022: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.028: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.044: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.050: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.064: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.073: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.078: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.096: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:52:58.096: INFO: Lookups using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local]

May  7 10:53:03.108: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.116: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.122: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.130: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.137: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.153: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.160: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:03.160: INFO: Lookups using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local]

May  7 10:53:08.115: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.120: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.125: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.131: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.142: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.145: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.150: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:08.150: INFO: Lookups using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local]

May  7 10:53:13.105: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.111: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.115: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.119: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.123: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.127: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.131: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.136: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:13.136: INFO: Lookups using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local]

May  7 10:53:18.103: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:18.108: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:18.119: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:18.123: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:18.131: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local from pod dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d: the server could not find the requested resource (get pods dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d)
May  7 10:53:18.131: INFO: Lookups using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3645.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3645.svc.cluster.local]

May  7 10:53:23.137: INFO: DNS probes using dns-3645/dns-test-29a98cc3-ea71-4270-bb14-f102836ccf4d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:53:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3645" for this suite.

• [SLOW TEST:39.358 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":109,"skipped":2186,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:53:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May  7 10:53:23.698: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:53:25.707: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 10:53:27.707: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May  7 10:53:27.729: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:53:29.741: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 10:53:31.737: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May  7 10:53:31.751: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  7 10:53:31.756: INFO: Pod pod-with-prestop-http-hook still exists
May  7 10:53:33.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  7 10:53:33.763: INFO: Pod pod-with-prestop-http-hook still exists
May  7 10:53:35.758: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  7 10:53:35.763: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:53:35.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5021" for this suite.

• [SLOW TEST:12.527 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2202,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:53:35.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-zrjx
STEP: Creating a pod to test atomic-volume-subpath
May  7 10:53:35.865: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zrjx" in namespace "subpath-9619" to be "Succeeded or Failed"
May  7 10:53:35.875: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.570938ms
May  7 10:53:37.887: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021560797s
May  7 10:53:39.896: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 4.030320269s
May  7 10:53:41.902: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 6.037008021s
May  7 10:53:43.908: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 8.042909245s
May  7 10:53:45.922: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 10.056308356s
May  7 10:53:48.024: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 12.15832134s
May  7 10:53:50.032: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 14.166376401s
May  7 10:53:52.041: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 16.175290807s
May  7 10:53:54.046: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 18.180879588s
May  7 10:53:56.057: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 20.191982196s
May  7 10:53:58.066: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Running", Reason="", readiness=true. Elapsed: 22.200850661s
May  7 10:54:00.076: INFO: Pod "pod-subpath-test-secret-zrjx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.210843871s
STEP: Saw pod success
May  7 10:54:00.076: INFO: Pod "pod-subpath-test-secret-zrjx" satisfied condition "Succeeded or Failed"
May  7 10:54:00.080: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-subpath-test-secret-zrjx container test-container-subpath-secret-zrjx: <nil>
STEP: delete the pod
May  7 10:54:00.114: INFO: Waiting for pod pod-subpath-test-secret-zrjx to disappear
May  7 10:54:00.118: INFO: Pod pod-subpath-test-secret-zrjx no longer exists
STEP: Deleting pod pod-subpath-test-secret-zrjx
May  7 10:54:00.118: INFO: Deleting pod "pod-subpath-test-secret-zrjx" in namespace "subpath-9619"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:00.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9619" for this suite.

• [SLOW TEST:24.332 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":111,"skipped":2208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:00.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May  7 10:54:00.254: INFO: Waiting up to 5m0s for pod "downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5" in namespace "downward-api-215" to be "Succeeded or Failed"
May  7 10:54:00.268: INFO: Pod "downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.576285ms
May  7 10:54:02.279: INFO: Pod "downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024165327s
May  7 10:54:04.290: INFO: Pod "downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035790044s
STEP: Saw pod success
May  7 10:54:04.290: INFO: Pod "downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5" satisfied condition "Succeeded or Failed"
May  7 10:54:04.294: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5 container dapi-container: <nil>
STEP: delete the pod
May  7 10:54:04.324: INFO: Waiting for pod downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5 to disappear
May  7 10:54:04.328: INFO: Pod downward-api-6de36c44-cfb7-4177-9c09-0072d4ad37a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:04.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-215" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2290,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:04.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  7 10:54:04.410: INFO: Waiting up to 5m0s for pod "pod-4d5540b2-6273-46d2-b380-3ccc190a01c4" in namespace "emptydir-34" to be "Succeeded or Failed"
May  7 10:54:04.422: INFO: Pod "pod-4d5540b2-6273-46d2-b380-3ccc190a01c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.247369ms
May  7 10:54:06.428: INFO: Pod "pod-4d5540b2-6273-46d2-b380-3ccc190a01c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017388111s
May  7 10:54:08.438: INFO: Pod "pod-4d5540b2-6273-46d2-b380-3ccc190a01c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027234138s
STEP: Saw pod success
May  7 10:54:08.438: INFO: Pod "pod-4d5540b2-6273-46d2-b380-3ccc190a01c4" satisfied condition "Succeeded or Failed"
May  7 10:54:08.442: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-4d5540b2-6273-46d2-b380-3ccc190a01c4 container test-container: <nil>
STEP: delete the pod
May  7 10:54:08.467: INFO: Waiting for pod pod-4d5540b2-6273-46d2-b380-3ccc190a01c4 to disappear
May  7 10:54:08.470: INFO: Pod pod-4d5540b2-6273-46d2-b380-3ccc190a01c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:08.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-34" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2300,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:08.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:08.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3480" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":114,"skipped":2306,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  7 10:54:08.637: INFO: Waiting up to 5m0s for pod "pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec" in namespace "emptydir-9132" to be "Succeeded or Failed"
May  7 10:54:08.640: INFO: Pod "pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177971ms
May  7 10:54:10.652: INFO: Pod "pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015104687s
May  7 10:54:12.660: INFO: Pod "pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022443872s
STEP: Saw pod success
May  7 10:54:12.660: INFO: Pod "pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec" satisfied condition "Succeeded or Failed"
May  7 10:54:12.663: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec container test-container: <nil>
STEP: delete the pod
May  7 10:54:12.696: INFO: Waiting for pod pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec to disappear
May  7 10:54:12.703: INFO: Pod pod-63a256f2-8d5b-4c36-af98-f5d9cca923ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:12.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9132" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2312,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:23.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7362" for this suite.

• [SLOW TEST:11.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":116,"skipped":2325,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:23.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 10:54:23.928: INFO: The status of Pod pod-secrets-32193c0a-6f24-4f59-b58d-8b8b337f7bf5 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:54:25.939: INFO: The status of Pod pod-secrets-32193c0a-6f24-4f59-b58d-8b8b337f7bf5 is Pending, waiting for it to be Running (with Ready = true)
May  7 10:54:27.936: INFO: The status of Pod pod-secrets-32193c0a-6f24-4f59-b58d-8b8b337f7bf5 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:27.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-923" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":117,"skipped":2341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:27.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-1f3fb7cd-8cb4-4923-8aad-1c11db47a919
STEP: Creating a pod to test consume configMaps
May  7 10:54:28.040: INFO: Waiting up to 5m0s for pod "pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521" in namespace "configmap-1242" to be "Succeeded or Failed"
May  7 10:54:28.055: INFO: Pod "pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521": Phase="Pending", Reason="", readiness=false. Elapsed: 14.727399ms
May  7 10:54:30.070: INFO: Pod "pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0303131s
May  7 10:54:32.083: INFO: Pod "pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042807115s
STEP: Saw pod success
May  7 10:54:32.083: INFO: Pod "pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521" satisfied condition "Succeeded or Failed"
May  7 10:54:32.087: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521 container agnhost-container: <nil>
STEP: delete the pod
May  7 10:54:32.156: INFO: Waiting for pod pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521 to disappear
May  7 10:54:32.164: INFO: Pod pod-configmaps-279650cc-3003-4301-8aaf-7b71d8470521 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:32.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1242" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:32.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-5341/secret-test-554a8d69-d841-4a01-b6ad-7015a049aa6e
STEP: Creating a pod to test consume secrets
May  7 10:54:32.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc" in namespace "secrets-5341" to be "Succeeded or Failed"
May  7 10:54:32.565: INFO: Pod "pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 65.980849ms
May  7 10:54:34.574: INFO: Pod "pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074357371s
May  7 10:54:36.582: INFO: Pod "pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082718449s
STEP: Saw pod success
May  7 10:54:36.582: INFO: Pod "pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc" satisfied condition "Succeeded or Failed"
May  7 10:54:36.587: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc container env-test: <nil>
STEP: delete the pod
May  7 10:54:36.617: INFO: Waiting for pod pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc to disappear
May  7 10:54:36.620: INFO: Pod pod-configmaps-033e678d-3094-4067-8d6b-fb7e11e9f5dc no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:36.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5341" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2400,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:36.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
May  7 10:54:36.699: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  7 10:54:38.706: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  7 10:54:40.705: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 10:54:41.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1479" for this suite.

• [SLOW TEST:5.111 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":120,"skipped":2405,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 10:54:41.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:01.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4003" for this suite.

• [SLOW TEST:320.172 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":121,"skipped":2414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:02.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9297" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":122,"skipped":2445,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:02.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:00:02.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f" in namespace "downward-api-7162" to be "Succeeded or Failed"
May  7 11:00:02.551: INFO: Pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.099407ms
May  7 11:00:04.560: INFO: Pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014648843s
May  7 11:00:06.573: INFO: Pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027772133s
May  7 11:00:08.578: INFO: Pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032647664s
STEP: Saw pod success
May  7 11:00:08.578: INFO: Pod "downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f" satisfied condition "Succeeded or Failed"
May  7 11:00:08.581: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f container client-container: <nil>
STEP: delete the pod
May  7 11:00:08.618: INFO: Waiting for pod downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f to disappear
May  7 11:00:08.625: INFO: Pod downwardapi-volume-d589ef28-9bcf-486d-8842-92a6c951fe3f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:08.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7162" for this suite.

• [SLOW TEST:6.159 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2459,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6800/configmap-test-07ace3ea-3f8c-4026-969b-f9e2f704febb
STEP: Creating a pod to test consume configMaps
May  7 11:00:08.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03" in namespace "configmap-6800" to be "Succeeded or Failed"
May  7 11:00:08.706: INFO: Pod "pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562374ms
May  7 11:00:10.711: INFO: Pod "pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008791766s
May  7 11:00:12.720: INFO: Pod "pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017748654s
STEP: Saw pod success
May  7 11:00:12.720: INFO: Pod "pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03" satisfied condition "Succeeded or Failed"
May  7 11:00:12.725: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03 container env-test: <nil>
STEP: delete the pod
May  7 11:00:12.748: INFO: Waiting for pod pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03 to disappear
May  7 11:00:12.752: INFO: Pod pod-configmaps-0ad79d35-6ded-4fa7-b366-dd0ad6647d03 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:12.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6800" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":124,"skipped":2460,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:12.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-1f541169-8226-4ad0-bcb7-0fe48fbb507a
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:12.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-286" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":125,"skipped":2468,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:12.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:00:13.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:00:15.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 0, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 0, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 0, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 0, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:00:18.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:18.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3212" for this suite.
STEP: Destroying namespace "webhook-3212-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":126,"skipped":2486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:18.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:21.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7841" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":127,"skipped":2515,"failed":0}

------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:21.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May  7 11:00:21.655: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:27.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-734" for this suite.

• [SLOW TEST:6.103 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":128,"skipped":2515,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:27.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:27.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6156" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":129,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:27.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:00:27.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:31.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1662" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":130,"skipped":2568,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5796
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5796
STEP: creating replication controller externalsvc in namespace services-5796
I0507 11:00:31.125936      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5796, replica count: 2
I0507 11:00:34.179351      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:00:37.181616      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May  7 11:00:37.221: INFO: Creating new exec pod
May  7 11:00:41.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5796 exec execpodh9trd -- /bin/sh -x -c nslookup clusterip-service.services-5796.svc.cluster.local'
May  7 11:00:42.395: INFO: stderr: "+ nslookup clusterip-service.services-5796.svc.cluster.local\n"
May  7 11:00:42.395: INFO: stdout: "Server:\t\t10.150.0.2\nAddress:\t10.150.0.2#53\n\nclusterip-service.services-5796.svc.cluster.local\tcanonical name = externalsvc.services-5796.svc.cluster.local.\nName:\texternalsvc.services-5796.svc.cluster.local\nAddress: 10.150.191.255\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5796, will wait for the garbage collector to delete the pods
May  7 11:00:42.462: INFO: Deleting ReplicationController externalsvc took: 11.040919ms
May  7 11:00:42.562: INFO: Terminating ReplicationController externalsvc pods took: 100.620122ms
May  7 11:00:44.802: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:44.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5796" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.798 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":131,"skipped":2576,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:44.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-19128b2f-4a6c-46ad-8a38-17783e4455f6
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:44.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9161" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":132,"skipped":2583,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:44.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May  7 11:00:44.933: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May  7 11:00:44.943: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  7 11:00:44.943: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May  7 11:00:44.954: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  7 11:00:44.954: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May  7 11:00:44.986: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  7 11:00:44.986: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May  7 11:00:52.051: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-132" for this suite.

• [SLOW TEST:7.191 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":133,"skipped":2584,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:52.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:00:52.449: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May  7 11:00:54.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 0, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 0, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 0, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 0, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:00:57.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:00:57.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6970" for this suite.
STEP: Destroying namespace "webhook-6970-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.525 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":134,"skipped":2584,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:00:57.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May  7 11:00:57.723: INFO: Waiting up to 5m0s for pod "downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63" in namespace "downward-api-5959" to be "Succeeded or Failed"
May  7 11:00:57.728: INFO: Pod "downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616698ms
May  7 11:00:59.732: INFO: Pod "downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009338636s
May  7 11:01:01.745: INFO: Pod "downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022106885s
STEP: Saw pod success
May  7 11:01:01.745: INFO: Pod "downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63" satisfied condition "Succeeded or Failed"
May  7 11:01:01.753: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63 container dapi-container: <nil>
STEP: delete the pod
May  7 11:01:01.801: INFO: Waiting for pod downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63 to disappear
May  7 11:01:01.804: INFO: Pod downward-api-90202b77-8fc6-45fa-9117-d61ddff96d63 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:01.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5959" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2587,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:01.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4395.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4395.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4395.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4395.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:01:08.196: INFO: DNS probes using dns-4395/dns-test-6bc555be-7f7a-4285-8951-34d4ef19f849 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4395" for this suite.

• [SLOW TEST:6.493 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":136,"skipped":2606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:08.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May  7 11:01:08.439: INFO: running pods: 0 < 3
May  7 11:01:10.447: INFO: running pods: 0 < 3
May  7 11:01:12.447: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:14.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4096" for this suite.

• [SLOW TEST:6.151 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":137,"skipped":2631,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:14.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May  7 11:01:14.533: INFO: Pod name pod-release: Found 0 pods out of 1
May  7 11:01:19.539: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:20.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6273" for this suite.

• [SLOW TEST:6.126 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":138,"skipped":2642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:20.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May  7 11:01:20.721: INFO: Waiting up to 5m0s for pod "downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd" in namespace "downward-api-5396" to be "Succeeded or Failed"
May  7 11:01:20.731: INFO: Pod "downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.528903ms
May  7 11:01:22.754: INFO: Pod "downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0327839s
May  7 11:01:24.758: INFO: Pod "downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036878117s
STEP: Saw pod success
May  7 11:01:24.758: INFO: Pod "downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd" satisfied condition "Succeeded or Failed"
May  7 11:01:24.760: INFO: Trying to get logs from node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 pod downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd container dapi-container: <nil>
STEP: delete the pod
May  7 11:01:24.809: INFO: Waiting for pod downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd to disappear
May  7 11:01:24.814: INFO: Pod downward-api-4eb67f35-f5de-4950-bb00-4630e7c252bd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5396" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2692,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:24.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:01:24.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-357 version'
May  7 11:01:24.954: INFO: stderr: ""
May  7 11:01:24.955: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4\", GitCommit:\"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T12:38:05Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.4+vmware.1\", GitCommit:\"63737040614f122286381a6aa8d4902b6fdcdfd0\", GitTreeState:\"clean\", BuildDate:\"2022-03-05T04:20:39Z\", GoVersion:\"go1.17.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:01:24.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-357" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":140,"skipped":2699,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:01:24.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6184
May  7 11:01:25.031: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  7 11:01:27.043: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May  7 11:01:27.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  7 11:01:27.221: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  7 11:01:27.221: INFO: stdout: "iptables"
May  7 11:01:27.221: INFO: proxyMode: iptables
May  7 11:01:27.241: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  7 11:01:27.244: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6184
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6184
I0507 11:01:27.282489      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6184, replica count: 3
I0507 11:01:30.334916      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:01:33.335352      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:01:33.356: INFO: Creating new exec pod
May  7 11:01:38.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May  7 11:01:38.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May  7 11:01:38.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:01:38.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.221.3 80'
May  7 11:01:38.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.221.3 80\nConnection to 10.150.221.3 80 port [tcp/http] succeeded!\n"
May  7 11:01:38.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:01:38.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.4 30453'
May  7 11:01:38.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.4 30453\nConnection to 30.1.0.4 30453 port [tcp/*] succeeded!\n"
May  7 11:01:38.881: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:01:38.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 30453'
May  7 11:01:39.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 30453\nConnection to 30.1.0.3 30453 port [tcp/*] succeeded!\n"
May  7 11:01:39.043: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:01:39.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.1.0.3:30453/ ; done'
May  7 11:01:39.308: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n"
May  7 11:01:39.309: INFO: stdout: "\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9\naffinity-nodeport-timeout-67gw9"
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Received response from host: affinity-nodeport-timeout-67gw9
May  7 11:01:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.1.0.3:30453/'
May  7 11:01:39.478: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n"
May  7 11:01:39.478: INFO: stdout: "affinity-nodeport-timeout-67gw9"
May  7 11:01:59.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6184 exec execpod-affinityknfbf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.1.0.3:30453/'
May  7 11:01:59.664: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.1.0.3:30453/\n"
May  7 11:01:59.665: INFO: stdout: "affinity-nodeport-timeout-cs57q"
May  7 11:01:59.665: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6184, will wait for the garbage collector to delete the pods
May  7 11:01:59.766: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.535307ms
May  7 11:01:59.867: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.847092ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:03.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6184" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:38.156 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":141,"skipped":2713,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:03.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  7 11:02:03.539: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  7 11:02:05.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:02:08.598: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:02:08.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:12.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5958" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.000 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":142,"skipped":2714,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:12.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-fad8e711-f058-4fdd-ba0d-6e052106f099
STEP: Creating a pod to test consume secrets
May  7 11:02:12.205: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114" in namespace "projected-6433" to be "Succeeded or Failed"
May  7 11:02:12.215: INFO: Pod "pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114": Phase="Pending", Reason="", readiness=false. Elapsed: 10.144653ms
May  7 11:02:14.227: INFO: Pod "pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022442858s
May  7 11:02:16.238: INFO: Pod "pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032853653s
STEP: Saw pod success
May  7 11:02:16.238: INFO: Pod "pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114" satisfied condition "Succeeded or Failed"
May  7 11:02:16.242: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  7 11:02:16.266: INFO: Waiting for pod pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114 to disappear
May  7 11:02:16.274: INFO: Pod pod-projected-secrets-ebb4b39a-0a64-4313-96a2-f435dd461114 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:16.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6433" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:16.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:16.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7957" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":144,"skipped":2757,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:16.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:02:16.469: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  7 11:02:21.482: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  7 11:02:21.482: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  7 11:02:23.491: INFO: Creating deployment "test-rollover-deployment"
May  7 11:02:23.501: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  7 11:02:25.557: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  7 11:02:25.566: INFO: Ensure that both replica sets have 1 created replica
May  7 11:02:25.579: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  7 11:02:25.591: INFO: Updating deployment test-rollover-deployment
May  7 11:02:25.591: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  7 11:02:27.669: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  7 11:02:27.681: INFO: Make sure deployment "test-rollover-deployment" is complete
May  7 11:02:27.692: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:27.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:29.721: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:29.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:31.707: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:31.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:33.707: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:33.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:35.718: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:35.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:37.713: INFO: all replica sets need to contain the pod-template-hash label
May  7 11:02:37.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:02:39.703: INFO: 
May  7 11:02:39.703: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:02:39.713: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2675  85a81fb1-5879-4e50-8ebf-75af60dc33b4 17170 2 2022-05-07 11:02:23 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-07 11:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c1028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-07 11:02:23 +0000 UTC,LastTransitionTime:2022-05-07 11:02:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-05-07 11:02:39 +0000 UTC,LastTransitionTime:2022-05-07 11:02:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  7 11:02:39.718: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-2675  073dda82-a978-4778-89d6-ada2d9e6e2ce 17160 2 2022-05-07 11:02:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 85a81fb1-5879-4e50-8ebf-75af60dc33b4 0xc0052c1517 0xc0052c1518}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85a81fb1-5879-4e50-8ebf-75af60dc33b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:02:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c15c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  7 11:02:39.718: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  7 11:02:39.718: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2675  92d50d51-4701-489e-b7eb-4a00c2590c26 17169 2 2022-05-07 11:02:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 85a81fb1-5879-4e50-8ebf-75af60dc33b4 0xc0052c13e7 0xc0052c13e8}] []  [{e2e.test Update apps/v1 2022-05-07 11:02:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85a81fb1-5879-4e50-8ebf-75af60dc33b4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:02:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0052c14a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:02:39.718: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-2675  3e1c84e7-2476-4c14-8e91-2b128acc9273 17126 2 2022-05-07 11:02:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 85a81fb1-5879-4e50-8ebf-75af60dc33b4 0xc0052c1637 0xc0052c1638}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85a81fb1-5879-4e50-8ebf-75af60dc33b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:02:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c16e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:02:39.725: INFO: Pod "test-rollover-deployment-668b7f667d-mvqmv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-mvqmv test-rollover-deployment-668b7f667d- deployment-2675  22758495-2d99-4fa5-a809-7a64419ff59d 17141 0 2022-05-07 11:02:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 073dda82-a978-4778-89d6-ada2d9e6e2ce 0xc0052c1c37 0xc0052c1c38}] []  [{kube-controller-manager Update v1 2022-05-07 11:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"073dda82-a978-4778-89d6-ada2d9e6e2ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:02:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.11.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsxmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsxmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.11.4,StartTime:2022-05-07 11:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:02:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://133d324b7165d73b76cc0c8a343f6954ee2215720b897b1e309f936bf3e225c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.11.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:39.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2675" for this suite.

• [SLOW TEST:23.333 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":145,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:39.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  7 11:02:39.811: INFO: Waiting up to 5m0s for pod "pod-7e10ecc2-9493-4924-a8ae-22c208e7c598" in namespace "emptydir-1803" to be "Succeeded or Failed"
May  7 11:02:39.828: INFO: Pod "pod-7e10ecc2-9493-4924-a8ae-22c208e7c598": Phase="Pending", Reason="", readiness=false. Elapsed: 16.809894ms
May  7 11:02:41.836: INFO: Pod "pod-7e10ecc2-9493-4924-a8ae-22c208e7c598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025304296s
May  7 11:02:43.843: INFO: Pod "pod-7e10ecc2-9493-4924-a8ae-22c208e7c598": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032380123s
STEP: Saw pod success
May  7 11:02:43.844: INFO: Pod "pod-7e10ecc2-9493-4924-a8ae-22c208e7c598" satisfied condition "Succeeded or Failed"
May  7 11:02:43.848: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-7e10ecc2-9493-4924-a8ae-22c208e7c598 container test-container: <nil>
STEP: delete the pod
May  7 11:02:43.885: INFO: Waiting for pod pod-7e10ecc2-9493-4924-a8ae-22c208e7c598 to disappear
May  7 11:02:43.889: INFO: Pod pod-7e10ecc2-9493-4924-a8ae-22c208e7c598 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:43.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1803" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2782,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:02:44.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:02:46.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:02:49.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:02:49.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1921-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:02:53.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8241" for this suite.
STEP: Destroying namespace "webhook-8241-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.334 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":147,"skipped":2788,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:02:53.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:02:54.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:02:56.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 2, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 2, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:02:59.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:02:59.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1422" for this suite.
STEP: Destroying namespace "webhook-1422-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.554 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":148,"skipped":2794,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:02.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:03:02.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a" in namespace "projected-4521" to be "Succeeded or Failed"
May  7 11:03:02.895: INFO: Pod "downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607073ms
May  7 11:03:04.905: INFO: Pod "downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014462549s
May  7 11:03:06.918: INFO: Pod "downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02783543s
STEP: Saw pod success
May  7 11:03:06.918: INFO: Pod "downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a" satisfied condition "Succeeded or Failed"
May  7 11:03:06.925: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a container client-container: <nil>
STEP: delete the pod
May  7 11:03:06.966: INFO: Waiting for pod downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a to disappear
May  7 11:03:06.975: INFO: Pod downwardapi-volume-d1275fa1-ec60-4fb7-8dba-1bfb77b14c2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:06.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4521" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2815,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:07.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May  7 11:03:07.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17453 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:07.090: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17453 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May  7 11:03:07.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17454 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:07.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17454 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May  7 11:03:07.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17455 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:07.107: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17455 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May  7 11:03:07.113: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17456 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:07.113: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9956  19b6d0b0-02e3-4bbf-8f9d-dbc5d138409d 17456 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May  7 11:03:07.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9956  af12c44c-705f-427a-83d1-27777a81e8df 17457 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:07.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9956  af12c44c-705f-427a-83d1-27777a81e8df 17457 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May  7 11:03:17.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9956  af12c44c-705f-427a-83d1-27777a81e8df 17502 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:03:17.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9956  af12c44c-705f-427a-83d1-27777a81e8df 17502 0 2022-05-07 11:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-07 11:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:27.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9956" for this suite.

• [SLOW TEST:20.158 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":150,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:27.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
May  7 11:03:27.261: INFO: Waiting up to 5m0s for pod "pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5" in namespace "emptydir-7357" to be "Succeeded or Failed"
May  7 11:03:27.293: INFO: Pod "pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.135254ms
May  7 11:03:29.300: INFO: Pod "pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039193672s
May  7 11:03:31.312: INFO: Pod "pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0506417s
STEP: Saw pod success
May  7 11:03:31.312: INFO: Pod "pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5" satisfied condition "Succeeded or Failed"
May  7 11:03:31.319: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5 container test-container: <nil>
STEP: delete the pod
May  7 11:03:31.349: INFO: Waiting for pod pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5 to disappear
May  7 11:03:31.354: INFO: Pod pod-bf1cff3f-08e0-4fc6-9ae9-7971850630f5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:31.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7357" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2849,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:31.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:03:31.959: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:03:33.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 3, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 3, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 3, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 3, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:03:37.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4462" for this suite.
STEP: Destroying namespace "webhook-4462-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.880 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":152,"skipped":2851,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:49.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May  7 11:03:49.306: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:03:54.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7983" for this suite.

• [SLOW TEST:5.059 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":153,"skipped":2853,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:03:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:04:10.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8501" for this suite.

• [SLOW TEST:16.274 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":154,"skipped":2872,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:04:10.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
May  7 11:04:10.713: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6794 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:04:10.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6794" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":155,"skipped":2874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:04:10.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-0c0e382f-534f-4d0b-8609-3b20a4fef2a3 in namespace container-probe-4236
May  7 11:04:14.961: INFO: Started pod test-webserver-0c0e382f-534f-4d0b-8609-3b20a4fef2a3 in namespace container-probe-4236
STEP: checking the pod's current state and verifying that restartCount is present
May  7 11:04:14.964: INFO: Initial restart count of pod test-webserver-0c0e382f-534f-4d0b-8609-3b20a4fef2a3 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:08:16.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4236" for this suite.

• [SLOW TEST:245.398 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2898,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:08:16.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2596.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2596.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 79.225.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.225.79_udp@PTR;check="$$(dig +tcp +noall +answer +search 79.225.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.225.79_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2596.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2596.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2596.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2596.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2596.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 79.225.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.225.79_udp@PTR;check="$$(dig +tcp +noall +answer +search 79.225.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.225.79_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:08:22.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.483: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.487: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.516: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.524: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.528: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:22.546: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:27.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.557: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.562: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.568: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.596: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.601: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.605: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.609: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:27.626: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:32.569: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.589: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.615: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.618: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.626: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:32.643: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:37.553: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.557: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.562: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.566: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.588: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.593: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.598: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.602: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:37.619: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:42.553: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.562: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.567: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.590: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.594: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.598: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.603: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:42.621: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:47.554: INFO: Unable to read wheezy_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.565: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.569: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.594: INFO: Unable to read jessie_udp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.597: INFO: Unable to read jessie_tcp@dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.601: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local from pod dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17: the server could not find the requested resource (get pods dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17)
May  7 11:08:47.619: INFO: Lookups using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 failed for: [wheezy_udp@dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@dns-test-service.dns-2596.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_udp@dns-test-service.dns-2596.svc.cluster.local jessie_tcp@dns-test-service.dns-2596.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2596.svc.cluster.local]

May  7 11:08:52.621: INFO: DNS probes using dns-2596/dns-test-da3ef4f0-7c2d-426e-92a0-8364d514dc17 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:08:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2596" for this suite.

• [SLOW TEST:36.541 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":157,"skipped":2908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:08:52.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May  7 11:08:52.876: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 11:08:54.885: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 11:08:56.887: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  7 11:08:58.888: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
May  7 11:08:58.941: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 11:09:00.949: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  7 11:09:02.951: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  7 11:09:02.985: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  7 11:09:02.989: INFO: Pod pod-with-poststart-http-hook still exists
May  7 11:09:04.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  7 11:09:05.000: INFO: Pod pod-with-poststart-http-hook still exists
May  7 11:09:06.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  7 11:09:06.995: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:06.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3717" for this suite.

• [SLOW TEST:14.202 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:07.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:09:07.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2" in namespace "downward-api-1475" to be "Succeeded or Failed"
May  7 11:09:07.079: INFO: Pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.84417ms
May  7 11:09:09.088: INFO: Pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024754057s
May  7 11:09:11.102: INFO: Pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037995697s
May  7 11:09:13.128: INFO: Pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064547956s
STEP: Saw pod success
May  7 11:09:13.128: INFO: Pod "downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2" satisfied condition "Succeeded or Failed"
May  7 11:09:13.133: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2 container client-container: <nil>
STEP: delete the pod
May  7 11:09:13.154: INFO: Waiting for pod downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2 to disappear
May  7 11:09:13.157: INFO: Pod downwardapi-volume-88ebf74f-0696-4c70-a565-0230774efcd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:13.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1475" for this suite.

• [SLOW TEST:6.160 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:13.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May  7 11:09:13.264: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May  7 11:09:13.270: INFO: starting watch
STEP: patching
STEP: updating
May  7 11:09:13.305: INFO: waiting for watch events with expected annotations
May  7 11:09:13.305: INFO: missing expected annotations, waiting: map[string]string{"ncp/error.loadbalancer":"INVALID_INGRESS"}
May  7 11:09:13.305: INFO: missing expected annotations, waiting: map[string]string{"ncp/error.loadbalancer":"INVALID_INGRESS"}
May  7 11:09:13.305: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:13.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3519" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":160,"skipped":3010,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:29.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5335" for this suite.

• [SLOW TEST:16.220 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":161,"skipped":3014,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:29.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:09:29.662: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May  7 11:09:30.712: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:30.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4192" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":162,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:30.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:09:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  7 11:09:33.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-1075 --namespace=crd-publish-openapi-1075 create -f -'
May  7 11:09:35.237: INFO: stderr: ""
May  7 11:09:35.237: INFO: stdout: "e2e-test-crd-publish-openapi-2523-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  7 11:09:35.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-1075 --namespace=crd-publish-openapi-1075 delete e2e-test-crd-publish-openapi-2523-crds test-cr'
May  7 11:09:35.313: INFO: stderr: ""
May  7 11:09:35.313: INFO: stdout: "e2e-test-crd-publish-openapi-2523-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  7 11:09:35.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-1075 --namespace=crd-publish-openapi-1075 apply -f -'
May  7 11:09:35.558: INFO: stderr: ""
May  7 11:09:35.558: INFO: stdout: "e2e-test-crd-publish-openapi-2523-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  7 11:09:35.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-1075 --namespace=crd-publish-openapi-1075 delete e2e-test-crd-publish-openapi-2523-crds test-cr'
May  7 11:09:35.640: INFO: stderr: ""
May  7 11:09:35.640: INFO: stdout: "e2e-test-crd-publish-openapi-2523-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May  7 11:09:35.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-1075 explain e2e-test-crd-publish-openapi-2523-crds'
May  7 11:09:35.943: INFO: stderr: ""
May  7 11:09:35.943: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2523-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:09:38.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1075" for this suite.

• [SLOW TEST:7.786 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":163,"skipped":3042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:09:38.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-756d381e-2b90-4520-92f1-2cf6b2703419 in namespace container-probe-8130
May  7 11:09:42.636: INFO: Started pod liveness-756d381e-2b90-4520-92f1-2cf6b2703419 in namespace container-probe-8130
STEP: checking the pod's current state and verifying that restartCount is present
May  7 11:09:42.640: INFO: Initial restart count of pod liveness-756d381e-2b90-4520-92f1-2cf6b2703419 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:13:43.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8130" for this suite.

• [SLOW TEST:245.391 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:13:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
May  7 11:13:44.088: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
May  7 11:13:46.111: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:13:48.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-885" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":165,"skipped":3093,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:13:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-5f2153dd-1fe4-4e57-99ea-b9657520b507
STEP: Creating the pod
May  7 11:13:48.267: INFO: The status of Pod pod-configmaps-3167d423-558e-4a21-be55-14dbf01b0a0f is Pending, waiting for it to be Running (with Ready = true)
May  7 11:13:50.273: INFO: The status of Pod pod-configmaps-3167d423-558e-4a21-be55-14dbf01b0a0f is Pending, waiting for it to be Running (with Ready = true)
May  7 11:13:52.274: INFO: The status of Pod pod-configmaps-3167d423-558e-4a21-be55-14dbf01b0a0f is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-5f2153dd-1fe4-4e57-99ea-b9657520b507
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:13:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6858" for this suite.

• [SLOW TEST:6.156 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:13:54.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1739/configmap-test-84255c52-d33a-43a3-8406-9b0d4907c320
STEP: Creating a pod to test consume configMaps
May  7 11:13:54.393: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000" in namespace "configmap-1739" to be "Succeeded or Failed"
May  7 11:13:54.401: INFO: Pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000": Phase="Pending", Reason="", readiness=false. Elapsed: 8.174465ms
May  7 11:13:56.411: INFO: Pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018159123s
May  7 11:13:58.422: INFO: Pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028558932s
May  7 11:14:00.435: INFO: Pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042041808s
STEP: Saw pod success
May  7 11:14:00.435: INFO: Pod "pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000" satisfied condition "Succeeded or Failed"
May  7 11:14:00.438: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000 container env-test: <nil>
STEP: delete the pod
May  7 11:14:00.457: INFO: Waiting for pod pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000 to disappear
May  7 11:14:00.464: INFO: Pod pod-configmaps-4ba61bde-4df9-4704-bfed-6a4703968000 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:00.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1739" for this suite.

• [SLOW TEST:6.140 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3124,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:00.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:14:00.534: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5" in namespace "security-context-test-2498" to be "Succeeded or Failed"
May  7 11:14:00.548: INFO: Pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.915972ms
May  7 11:14:02.559: INFO: Pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0251753s
May  7 11:14:04.567: INFO: Pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033041233s
May  7 11:14:06.576: INFO: Pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041818624s
May  7 11:14:06.576: INFO: Pod "alpine-nnp-false-0161b47d-138f-4942-88e4-587cd26d76c5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:06.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2498" for this suite.

• [SLOW TEST:6.125 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3135,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:06.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:14:06.667: INFO: Creating pod...
May  7 11:14:06.712: INFO: Pod Quantity: 1 Status: Pending
May  7 11:14:07.723: INFO: Pod Quantity: 1 Status: Pending
May  7 11:14:08.722: INFO: Pod Quantity: 1 Status: Pending
May  7 11:14:09.728: INFO: Pod Quantity: 1 Status: Pending
May  7 11:14:10.727: INFO: Pod Quantity: 1 Status: Pending
May  7 11:14:11.719: INFO: Pod Status: Running
May  7 11:14:11.720: INFO: Creating service...
May  7 11:14:11.830: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/DELETE
May  7 11:14:11.912: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  7 11:14:11.912: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/GET
May  7 11:14:11.926: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  7 11:14:11.926: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/HEAD
May  7 11:14:11.933: INFO: http.Client request:HEAD | StatusCode:200
May  7 11:14:11.933: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/OPTIONS
May  7 11:14:11.942: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  7 11:14:11.942: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/PATCH
May  7 11:14:11.947: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  7 11:14:11.947: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/POST
May  7 11:14:11.952: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  7 11:14:11.952: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/pods/agnhost/proxy/some/path/with/PUT
May  7 11:14:11.957: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  7 11:14:11.957: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/DELETE
May  7 11:14:11.965: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  7 11:14:11.965: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/GET
May  7 11:14:11.975: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  7 11:14:11.975: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/HEAD
May  7 11:14:11.992: INFO: http.Client request:HEAD | StatusCode:200
May  7 11:14:11.992: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/OPTIONS
May  7 11:14:12.033: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  7 11:14:12.033: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/PATCH
May  7 11:14:12.046: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  7 11:14:12.046: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/POST
May  7 11:14:12.052: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  7 11:14:12.052: INFO: Starting http.Client for https://10.150.0.1:443/api/v1/namespaces/proxy-5391/services/test-service/proxy/some/path/with/PUT
May  7 11:14:12.058: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:12.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5391" for this suite.

• [SLOW TEST:5.476 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":169,"skipped":3142,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:12.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:12.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-837" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":170,"skipped":3149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:12.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-439af2d9-e01e-4931-9a82-ee93084ae604
STEP: Creating a pod to test consume configMaps
May  7 11:14:12.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b" in namespace "configmap-6765" to be "Succeeded or Failed"
May  7 11:14:12.458: INFO: Pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.660614ms
May  7 11:14:14.468: INFO: Pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020769338s
May  7 11:14:16.477: INFO: Pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03028819s
May  7 11:14:18.492: INFO: Pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045216587s
STEP: Saw pod success
May  7 11:14:18.492: INFO: Pod "pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b" satisfied condition "Succeeded or Failed"
May  7 11:14:18.499: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b container agnhost-container: <nil>
STEP: delete the pod
May  7 11:14:18.539: INFO: Waiting for pod pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b to disappear
May  7 11:14:18.544: INFO: Pod pod-configmaps-eb456215-2245-4541-ba21-30a1d7a9f56b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6765" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3202,"failed":0}
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:18.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9349" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":172,"skipped":3202,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:18.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5609
STEP: creating service affinity-nodeport in namespace services-5609
STEP: creating replication controller affinity-nodeport in namespace services-5609
I0507 11:14:18.748479      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5609, replica count: 3
I0507 11:14:21.805735      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:14:24.806137      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:14:24.834: INFO: Creating new exec pod
May  7 11:14:31.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5609 exec execpod-affinity6jxkg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May  7 11:14:32.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  7 11:14:32.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:14:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5609 exec execpod-affinity6jxkg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.79.39 80'
May  7 11:14:32.349: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.79.39 80\nConnection to 10.150.79.39 80 port [tcp/http] succeeded!\n"
May  7 11:14:32.349: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:14:32.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5609 exec execpod-affinity6jxkg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.5 30770'
May  7 11:14:32.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.5 30770\nConnection to 30.1.0.5 30770 port [tcp/*] succeeded!\n"
May  7 11:14:32.565: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:14:32.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5609 exec execpod-affinity6jxkg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.4 30770'
May  7 11:14:32.926: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 30.1.0.4 30770\nConnection to 30.1.0.4 30770 port [tcp/*] succeeded!\n"
May  7 11:14:32.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:14:32.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-5609 exec execpod-affinity6jxkg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.1.0.3:30770/ ; done'
May  7 11:14:33.304: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:30770/\n"
May  7 11:14:33.304: INFO: stdout: "\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn\naffinity-nodeport-spndn"
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Received response from host: affinity-nodeport-spndn
May  7 11:14:33.304: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5609, will wait for the garbage collector to delete the pods
May  7 11:14:33.402: INFO: Deleting ReplicationController affinity-nodeport took: 7.181454ms
May  7 11:14:33.503: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.344377ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:36.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5609" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.561 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":173,"skipped":3209,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:36.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:14:36.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78948c58f6\""}}, CollisionCount:(*int32)(nil)}
May  7 11:14:38.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:14:40.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 14, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:14:43.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:14:43.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4296-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:47.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1689" for this suite.
STEP: Destroying namespace "webhook-1689-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":174,"skipped":3215,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
May  7 11:14:47.370: INFO: Waiting up to 5m0s for pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada" in namespace "emptydir-1488" to be "Succeeded or Failed"
May  7 11:14:47.383: INFO: Pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada": Phase="Pending", Reason="", readiness=false. Elapsed: 12.866034ms
May  7 11:14:49.397: INFO: Pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026634495s
May  7 11:14:51.404: INFO: Pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033466452s
May  7 11:14:53.500: INFO: Pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.130205961s
STEP: Saw pod success
May  7 11:14:53.501: INFO: Pod "pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada" satisfied condition "Succeeded or Failed"
May  7 11:14:53.504: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada container test-container: <nil>
STEP: delete the pod
May  7 11:14:53.537: INFO: Waiting for pod pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada to disappear
May  7 11:14:53.545: INFO: Pod pod-c26b60f2-d7dd-4895-bd4d-d50ceb9f6ada no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:53.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1488" for this suite.

• [SLOW TEST:6.282 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3217,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:53.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:14:59.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3040" for this suite.

• [SLOW TEST:6.143 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":176,"skipped":3236,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:14:59.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  7 11:14:59.806: INFO: Waiting up to 5m0s for pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca" in namespace "emptydir-1625" to be "Succeeded or Failed"
May  7 11:14:59.823: INFO: Pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 16.855945ms
May  7 11:15:01.936: INFO: Pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129920106s
May  7 11:15:03.943: INFO: Pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13735086s
May  7 11:15:05.952: INFO: Pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.146636517s
STEP: Saw pod success
May  7 11:15:05.953: INFO: Pod "pod-1e029985-b17d-414a-b6ab-4783a216a5ca" satisfied condition "Succeeded or Failed"
May  7 11:15:05.957: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-1e029985-b17d-414a-b6ab-4783a216a5ca container test-container: <nil>
STEP: delete the pod
May  7 11:15:05.984: INFO: Waiting for pod pod-1e029985-b17d-414a-b6ab-4783a216a5ca to disappear
May  7 11:15:05.991: INFO: Pod pod-1e029985-b17d-414a-b6ab-4783a216a5ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:05.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1625" for this suite.

• [SLOW TEST:6.301 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:06.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
May  7 11:15:06.066: INFO: The status of Pod pod-update-fadcea71-8c4b-4a9a-8b27-67b1f9d01251 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:15:08.077: INFO: The status of Pod pod-update-fadcea71-8c4b-4a9a-8b27-67b1f9d01251 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:15:10.075: INFO: The status of Pod pod-update-fadcea71-8c4b-4a9a-8b27-67b1f9d01251 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:15:12.076: INFO: The status of Pod pod-update-fadcea71-8c4b-4a9a-8b27-67b1f9d01251 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  7 11:15:12.625: INFO: Successfully updated pod "pod-update-fadcea71-8c4b-4a9a-8b27-67b1f9d01251"
STEP: verifying the updated pod is in kubernetes
May  7 11:15:12.637: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:12.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-57" for this suite.

• [SLOW TEST:6.647 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3269,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:12.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:15:12.722: INFO: The status of Pod busybox-readonly-fs3c646de3-2db0-4721-97a3-7b82c8b386dd is Pending, waiting for it to be Running (with Ready = true)
May  7 11:15:14.731: INFO: The status of Pod busybox-readonly-fs3c646de3-2db0-4721-97a3-7b82c8b386dd is Pending, waiting for it to be Running (with Ready = true)
May  7 11:15:16.729: INFO: The status of Pod busybox-readonly-fs3c646de3-2db0-4721-97a3-7b82c8b386dd is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:16.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2383" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3273,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:16.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:15:22.884: INFO: DNS probes using dns-test-0d0a4759-d2d5-4c52-a591-fb099574d48c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:15:28.967: INFO: DNS probes using dns-test-954ed1e9-e1fa-4282-b0fe-05a088acb908 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7291.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7291.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:15:35.069: INFO: DNS probes using dns-test-baf3928a-e85e-45d9-a211-4820628eee2f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:35.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7291" for this suite.

• [SLOW TEST:18.391 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":180,"skipped":3277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:35.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:15:35.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:15:37.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 15, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 15, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 15, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 15, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:15:40.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:40.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-377" for this suite.
STEP: Destroying namespace "webhook-377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.818 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":181,"skipped":3352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:40.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-29731296-a76d-4808-8e52-c98cbc115214
STEP: Creating a pod to test consume secrets
May  7 11:15:41.052: INFO: Waiting up to 5m0s for pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e" in namespace "secrets-1948" to be "Succeeded or Failed"
May  7 11:15:41.067: INFO: Pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.1574ms
May  7 11:15:43.075: INFO: Pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022612215s
May  7 11:15:45.079: INFO: Pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027312064s
May  7 11:15:47.086: INFO: Pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033607516s
STEP: Saw pod success
May  7 11:15:47.086: INFO: Pod "pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e" satisfied condition "Succeeded or Failed"
May  7 11:15:47.101: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e container secret-volume-test: <nil>
STEP: delete the pod
May  7 11:15:47.131: INFO: Waiting for pod pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e to disappear
May  7 11:15:47.138: INFO: Pod pod-secrets-18e9ed39-80be-4089-8b0b-6de1cec1727e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:47.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1948" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3395,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:47.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8582" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":183,"skipped":3398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:49.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
May  7 11:15:49.915: INFO: created pod pod-service-account-defaultsa
May  7 11:15:49.915: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  7 11:15:49.931: INFO: created pod pod-service-account-mountsa
May  7 11:15:49.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  7 11:15:49.940: INFO: created pod pod-service-account-nomountsa
May  7 11:15:49.940: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  7 11:15:49.959: INFO: created pod pod-service-account-defaultsa-mountspec
May  7 11:15:49.959: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  7 11:15:49.983: INFO: created pod pod-service-account-mountsa-mountspec
May  7 11:15:49.983: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  7 11:15:50.004: INFO: created pod pod-service-account-nomountsa-mountspec
May  7 11:15:50.004: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  7 11:15:50.030: INFO: created pod pod-service-account-defaultsa-nomountspec
May  7 11:15:50.030: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  7 11:15:50.043: INFO: created pod pod-service-account-mountsa-nomountspec
May  7 11:15:50.043: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  7 11:15:50.172: INFO: created pod pod-service-account-nomountsa-nomountspec
May  7 11:15:50.172: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:50.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4139" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":184,"skipped":3420,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May  7 11:15:50.247: INFO: Waiting up to 5m0s for pod "downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e" in namespace "downward-api-485" to be "Succeeded or Failed"
May  7 11:15:50.259: INFO: Pod "downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.668036ms
May  7 11:15:52.267: INFO: Pod "downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020256929s
May  7 11:15:54.275: INFO: Pod "downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028321515s
STEP: Saw pod success
May  7 11:15:54.275: INFO: Pod "downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e" satisfied condition "Succeeded or Failed"
May  7 11:15:54.280: INFO: Trying to get logs from node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 pod downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e container dapi-container: <nil>
STEP: delete the pod
May  7 11:15:54.323: INFO: Waiting for pod downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e to disappear
May  7 11:15:54.347: INFO: Pod downward-api-8fbee80c-4cc1-41e5-a4e0-8beee0a6645e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:54.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-485" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3429,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:54.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:15:54.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999" in namespace "projected-2352" to be "Succeeded or Failed"
May  7 11:15:54.461: INFO: Pod "downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999": Phase="Pending", Reason="", readiness=false. Elapsed: 15.829971ms
May  7 11:15:56.477: INFO: Pod "downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03226154s
May  7 11:15:58.487: INFO: Pod "downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041748307s
STEP: Saw pod success
May  7 11:15:58.487: INFO: Pod "downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999" satisfied condition "Succeeded or Failed"
May  7 11:15:58.490: INFO: Trying to get logs from node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 pod downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999 container client-container: <nil>
STEP: delete the pod
May  7 11:15:58.534: INFO: Waiting for pod downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999 to disappear
May  7 11:15:58.544: INFO: Pod downwardapi-volume-741c0826-b91d-4889-8b00-6b83955d3999 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:15:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2352" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:15:58.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May  7 11:15:58.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9755 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  7 11:15:58.700: INFO: stderr: ""
May  7 11:15:58.700: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May  7 11:15:58.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9755 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May  7 11:16:00.344: INFO: stderr: ""
May  7 11:16:00.344: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May  7 11:16:00.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9755 delete pods e2e-test-httpd-pod'
May  7 11:16:03.951: INFO: stderr: ""
May  7 11:16:03.951: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:16:03.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9755" for this suite.

• [SLOW TEST:5.403 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:926
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":187,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:16:03.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May  7 11:16:04.031: INFO: Waiting up to 5m0s for pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21" in namespace "emptydir-2265" to be "Succeeded or Failed"
May  7 11:16:04.044: INFO: Pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21": Phase="Pending", Reason="", readiness=false. Elapsed: 12.966226ms
May  7 11:16:06.056: INFO: Pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025380858s
May  7 11:16:08.066: INFO: Pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035656556s
May  7 11:16:10.076: INFO: Pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045283739s
STEP: Saw pod success
May  7 11:16:10.076: INFO: Pod "pod-21289dde-1104-4d87-a78c-dbc88e008c21" satisfied condition "Succeeded or Failed"
May  7 11:16:10.080: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-21289dde-1104-4d87-a78c-dbc88e008c21 container test-container: <nil>
STEP: delete the pod
May  7 11:16:10.134: INFO: Waiting for pod pod-21289dde-1104-4d87-a78c-dbc88e008c21 to disappear
May  7 11:16:10.139: INFO: Pod pod-21289dde-1104-4d87-a78c-dbc88e008c21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:16:10.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2265" for this suite.

• [SLOW TEST:6.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3539,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:16:10.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1819
STEP: creating service affinity-nodeport-transition in namespace services-1819
STEP: creating replication controller affinity-nodeport-transition in namespace services-1819
I0507 11:16:10.328033      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1819, replica count: 3
I0507 11:16:13.379592      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:16:16.379846      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:16:16.396: INFO: Creating new exec pod
May  7 11:16:21.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May  7 11:16:21.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  7 11:16:21.651: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:16:21.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.92.62 80'
May  7 11:16:21.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.92.62 80\nConnection to 10.150.92.62 80 port [tcp/http] succeeded!\n"
May  7 11:16:21.959: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:16:21.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.4 32098'
May  7 11:16:22.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.4 32098\nConnection to 30.1.0.4 32098 port [tcp/*] succeeded!\n"
May  7 11:16:22.148: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:16:22.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.3 32098'
May  7 11:16:22.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.3 32098\nConnection to 30.1.0.3 32098 port [tcp/*] succeeded!\n"
May  7 11:16:22.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:16:22.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.1.0.3:32098/ ; done'
May  7 11:16:22.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n"
May  7 11:16:22.692: INFO: stdout: "\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-s8dwq\naffinity-nodeport-transition-s8dwq\naffinity-nodeport-transition-s8dwq\naffinity-nodeport-transition-s8dwq\naffinity-nodeport-transition-ks7k2\naffinity-nodeport-transition-ks7k2\naffinity-nodeport-transition-s8dwq\naffinity-nodeport-transition-ks7k2\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-ks7k2\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-s8dwq"
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-ks7k2
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-ks7k2
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-ks7k2
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-ks7k2
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:22.692: INFO: Received response from host: affinity-nodeport-transition-s8dwq
May  7 11:16:22.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1819 exec execpod-affinitydgbd8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.1.0.3:32098/ ; done'
May  7 11:16:23.055: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.1.0.3:32098/\n"
May  7 11:16:23.055: INFO: stdout: "\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h\naffinity-nodeport-transition-8xh7h"
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Received response from host: affinity-nodeport-transition-8xh7h
May  7 11:16:23.055: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1819, will wait for the garbage collector to delete the pods
May  7 11:16:23.167: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.374729ms
May  7 11:16:23.368: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.670635ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:16:25.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1819" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.593 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":189,"skipped":3552,"failed":0}
S
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:16:25.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May  7 11:16:25.885: INFO: starting watch
STEP: patching
STEP: updating
May  7 11:16:25.900: INFO: waiting for watch events with expected annotations
May  7 11:16:25.900: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:16:25.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9261" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":190,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:16:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-6b39589a-74c3-4eb5-9b48-d2a87f920448
STEP: Creating secret with name s-test-opt-upd-8f9c9333-8bd4-4a08-9324-7f19be962423
STEP: Creating the pod
May  7 11:16:26.053: INFO: The status of Pod pod-secrets-42c94d50-a9db-4c51-ac55-9aba8e167f56 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:16:28.079: INFO: The status of Pod pod-secrets-42c94d50-a9db-4c51-ac55-9aba8e167f56 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:16:30.069: INFO: The status of Pod pod-secrets-42c94d50-a9db-4c51-ac55-9aba8e167f56 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:16:32.065: INFO: The status of Pod pod-secrets-42c94d50-a9db-4c51-ac55-9aba8e167f56 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-6b39589a-74c3-4eb5-9b48-d2a87f920448
STEP: Updating secret s-test-opt-upd-8f9c9333-8bd4-4a08-9324-7f19be962423
STEP: Creating secret with name s-test-opt-create-bdabb6ca-aa0c-47a5-be12-c80d83282ba4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:17:38.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5302" for this suite.

• [SLOW TEST:72.586 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:17:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:17:51.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6430" for this suite.

• [SLOW TEST:13.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":192,"skipped":3617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:17:51.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:17:51.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2622" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":193,"skipped":3641,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:17:51.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:17:52.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:17:54.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 17, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 17, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 17, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 17, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:17:57.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:17:57.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8440" for this suite.
STEP: Destroying namespace "webhook-8440-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.634 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":194,"skipped":3652,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:17:57.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May  7 11:17:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May  7 11:18:10.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:18:12.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:18:23.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2422" for this suite.

• [SLOW TEST:25.865 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":195,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:18:23.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
May  7 11:19:03.543: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  7 11:19:03.543: INFO: Deleting pod "simpletest.rc-27mwr" in namespace "gc-4572"
W0507 11:19:03.543271      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  7 11:19:03.574: INFO: Deleting pod "simpletest.rc-2jshb" in namespace "gc-4572"
May  7 11:19:03.608: INFO: Deleting pod "simpletest.rc-2rftd" in namespace "gc-4572"
May  7 11:19:03.640: INFO: Deleting pod "simpletest.rc-2zs8f" in namespace "gc-4572"
May  7 11:19:03.661: INFO: Deleting pod "simpletest.rc-47tvb" in namespace "gc-4572"
May  7 11:19:03.690: INFO: Deleting pod "simpletest.rc-49s9n" in namespace "gc-4572"
May  7 11:19:03.711: INFO: Deleting pod "simpletest.rc-4ffhl" in namespace "gc-4572"
May  7 11:19:03.724: INFO: Deleting pod "simpletest.rc-54lmp" in namespace "gc-4572"
May  7 11:19:03.746: INFO: Deleting pod "simpletest.rc-5mnxn" in namespace "gc-4572"
May  7 11:19:03.813: INFO: Deleting pod "simpletest.rc-5q5nd" in namespace "gc-4572"
May  7 11:19:03.834: INFO: Deleting pod "simpletest.rc-5zccn" in namespace "gc-4572"
May  7 11:19:03.856: INFO: Deleting pod "simpletest.rc-62m29" in namespace "gc-4572"
May  7 11:19:03.886: INFO: Deleting pod "simpletest.rc-67m28" in namespace "gc-4572"
May  7 11:19:03.896: INFO: Deleting pod "simpletest.rc-69mvr" in namespace "gc-4572"
May  7 11:19:03.997: INFO: Deleting pod "simpletest.rc-6w87v" in namespace "gc-4572"
May  7 11:19:04.021: INFO: Deleting pod "simpletest.rc-7nhjm" in namespace "gc-4572"
May  7 11:19:04.037: INFO: Deleting pod "simpletest.rc-7qwdb" in namespace "gc-4572"
May  7 11:19:04.113: INFO: Deleting pod "simpletest.rc-88p4h" in namespace "gc-4572"
May  7 11:19:04.125: INFO: Deleting pod "simpletest.rc-8fsvv" in namespace "gc-4572"
May  7 11:19:04.249: INFO: Deleting pod "simpletest.rc-8wqg6" in namespace "gc-4572"
May  7 11:19:04.272: INFO: Deleting pod "simpletest.rc-929jn" in namespace "gc-4572"
May  7 11:19:04.289: INFO: Deleting pod "simpletest.rc-9fjs7" in namespace "gc-4572"
May  7 11:19:04.386: INFO: Deleting pod "simpletest.rc-9jzll" in namespace "gc-4572"
May  7 11:19:04.411: INFO: Deleting pod "simpletest.rc-9rcvx" in namespace "gc-4572"
May  7 11:19:04.660: INFO: Deleting pod "simpletest.rc-9t6k9" in namespace "gc-4572"
May  7 11:19:04.681: INFO: Deleting pod "simpletest.rc-b6bgt" in namespace "gc-4572"
May  7 11:19:04.701: INFO: Deleting pod "simpletest.rc-bvmng" in namespace "gc-4572"
May  7 11:19:04.722: INFO: Deleting pod "simpletest.rc-cjczp" in namespace "gc-4572"
May  7 11:19:04.862: INFO: Deleting pod "simpletest.rc-cmjnr" in namespace "gc-4572"
May  7 11:19:04.879: INFO: Deleting pod "simpletest.rc-cwbbs" in namespace "gc-4572"
May  7 11:19:04.911: INFO: Deleting pod "simpletest.rc-cwwvp" in namespace "gc-4572"
May  7 11:19:04.973: INFO: Deleting pod "simpletest.rc-d46pk" in namespace "gc-4572"
May  7 11:19:04.987: INFO: Deleting pod "simpletest.rc-d55ct" in namespace "gc-4572"
May  7 11:19:05.008: INFO: Deleting pod "simpletest.rc-d6rj2" in namespace "gc-4572"
May  7 11:19:05.035: INFO: Deleting pod "simpletest.rc-dfjhm" in namespace "gc-4572"
May  7 11:19:05.351: INFO: Deleting pod "simpletest.rc-dfw97" in namespace "gc-4572"
May  7 11:19:05.371: INFO: Deleting pod "simpletest.rc-dvzmj" in namespace "gc-4572"
May  7 11:19:05.400: INFO: Deleting pod "simpletest.rc-f8s96" in namespace "gc-4572"
May  7 11:19:05.482: INFO: Deleting pod "simpletest.rc-fjrwc" in namespace "gc-4572"
May  7 11:19:05.493: INFO: Deleting pod "simpletest.rc-fwsxb" in namespace "gc-4572"
May  7 11:19:05.508: INFO: Deleting pod "simpletest.rc-fxmc5" in namespace "gc-4572"
May  7 11:19:05.619: INFO: Deleting pod "simpletest.rc-g4qnr" in namespace "gc-4572"
May  7 11:19:05.630: INFO: Deleting pod "simpletest.rc-g6fw6" in namespace "gc-4572"
May  7 11:19:05.654: INFO: Deleting pod "simpletest.rc-g9d5q" in namespace "gc-4572"
May  7 11:19:05.688: INFO: Deleting pod "simpletest.rc-gbz8q" in namespace "gc-4572"
May  7 11:19:05.735: INFO: Deleting pod "simpletest.rc-gjwkn" in namespace "gc-4572"
May  7 11:19:05.762: INFO: Deleting pod "simpletest.rc-h4td7" in namespace "gc-4572"
May  7 11:19:05.800: INFO: Deleting pod "simpletest.rc-h4x8p" in namespace "gc-4572"
May  7 11:19:05.813: INFO: Deleting pod "simpletest.rc-hjllj" in namespace "gc-4572"
May  7 11:19:05.847: INFO: Deleting pod "simpletest.rc-htpqk" in namespace "gc-4572"
May  7 11:19:05.882: INFO: Deleting pod "simpletest.rc-j6bhr" in namespace "gc-4572"
May  7 11:19:05.913: INFO: Deleting pod "simpletest.rc-js7sq" in namespace "gc-4572"
May  7 11:19:05.929: INFO: Deleting pod "simpletest.rc-jt9cd" in namespace "gc-4572"
May  7 11:19:05.971: INFO: Deleting pod "simpletest.rc-jv9kc" in namespace "gc-4572"
May  7 11:19:05.985: INFO: Deleting pod "simpletest.rc-jx5f8" in namespace "gc-4572"
May  7 11:19:06.000: INFO: Deleting pod "simpletest.rc-kfwzk" in namespace "gc-4572"
May  7 11:19:06.021: INFO: Deleting pod "simpletest.rc-kszvr" in namespace "gc-4572"
May  7 11:19:06.473: INFO: Deleting pod "simpletest.rc-l6bm2" in namespace "gc-4572"
May  7 11:19:06.509: INFO: Deleting pod "simpletest.rc-lf5jt" in namespace "gc-4572"
May  7 11:19:06.527: INFO: Deleting pod "simpletest.rc-lpd5m" in namespace "gc-4572"
May  7 11:19:06.688: INFO: Deleting pod "simpletest.rc-lpldv" in namespace "gc-4572"
May  7 11:19:06.707: INFO: Deleting pod "simpletest.rc-lrptc" in namespace "gc-4572"
May  7 11:19:06.727: INFO: Deleting pod "simpletest.rc-m6bt5" in namespace "gc-4572"
May  7 11:19:06.754: INFO: Deleting pod "simpletest.rc-mnnl2" in namespace "gc-4572"
May  7 11:19:06.791: INFO: Deleting pod "simpletest.rc-msfgp" in namespace "gc-4572"
May  7 11:19:06.813: INFO: Deleting pod "simpletest.rc-nhspz" in namespace "gc-4572"
May  7 11:19:06.830: INFO: Deleting pod "simpletest.rc-nhtj4" in namespace "gc-4572"
May  7 11:19:06.843: INFO: Deleting pod "simpletest.rc-nqksp" in namespace "gc-4572"
May  7 11:19:06.878: INFO: Deleting pod "simpletest.rc-nxnf8" in namespace "gc-4572"
May  7 11:19:06.898: INFO: Deleting pod "simpletest.rc-p7whv" in namespace "gc-4572"
May  7 11:19:06.932: INFO: Deleting pod "simpletest.rc-p9x25" in namespace "gc-4572"
May  7 11:19:06.953: INFO: Deleting pod "simpletest.rc-pbvkg" in namespace "gc-4572"
May  7 11:19:06.978: INFO: Deleting pod "simpletest.rc-pvnxq" in namespace "gc-4572"
May  7 11:19:06.999: INFO: Deleting pod "simpletest.rc-q2wz6" in namespace "gc-4572"
May  7 11:19:07.026: INFO: Deleting pod "simpletest.rc-r2j5w" in namespace "gc-4572"
May  7 11:19:07.045: INFO: Deleting pod "simpletest.rc-rb4jp" in namespace "gc-4572"
May  7 11:19:07.056: INFO: Deleting pod "simpletest.rc-rfwsx" in namespace "gc-4572"
May  7 11:19:07.092: INFO: Deleting pod "simpletest.rc-rkgxf" in namespace "gc-4572"
May  7 11:19:07.107: INFO: Deleting pod "simpletest.rc-rlr27" in namespace "gc-4572"
May  7 11:19:07.133: INFO: Deleting pod "simpletest.rc-rqgnr" in namespace "gc-4572"
May  7 11:19:07.148: INFO: Deleting pod "simpletest.rc-s2nb7" in namespace "gc-4572"
May  7 11:19:07.168: INFO: Deleting pod "simpletest.rc-s8mn4" in namespace "gc-4572"
May  7 11:19:07.197: INFO: Deleting pod "simpletest.rc-smzvv" in namespace "gc-4572"
May  7 11:19:07.221: INFO: Deleting pod "simpletest.rc-sqszt" in namespace "gc-4572"
May  7 11:19:07.247: INFO: Deleting pod "simpletest.rc-t4prg" in namespace "gc-4572"
May  7 11:19:07.399: INFO: Deleting pod "simpletest.rc-t6v2b" in namespace "gc-4572"
May  7 11:19:07.421: INFO: Deleting pod "simpletest.rc-tfblx" in namespace "gc-4572"
May  7 11:19:07.448: INFO: Deleting pod "simpletest.rc-tt2zb" in namespace "gc-4572"
May  7 11:19:07.464: INFO: Deleting pod "simpletest.rc-v8bxn" in namespace "gc-4572"
May  7 11:19:07.489: INFO: Deleting pod "simpletest.rc-v9xtb" in namespace "gc-4572"
May  7 11:19:07.508: INFO: Deleting pod "simpletest.rc-vlz7g" in namespace "gc-4572"
May  7 11:19:07.525: INFO: Deleting pod "simpletest.rc-vsh2j" in namespace "gc-4572"
May  7 11:19:07.546: INFO: Deleting pod "simpletest.rc-vswq5" in namespace "gc-4572"
May  7 11:19:07.571: INFO: Deleting pod "simpletest.rc-w7h8b" in namespace "gc-4572"
May  7 11:19:07.586: INFO: Deleting pod "simpletest.rc-wdt4r" in namespace "gc-4572"
May  7 11:19:07.609: INFO: Deleting pod "simpletest.rc-x4j27" in namespace "gc-4572"
May  7 11:19:07.632: INFO: Deleting pod "simpletest.rc-x7vmj" in namespace "gc-4572"
May  7 11:19:07.646: INFO: Deleting pod "simpletest.rc-z5qwm" in namespace "gc-4572"
May  7 11:19:07.672: INFO: Deleting pod "simpletest.rc-z66mf" in namespace "gc-4572"
May  7 11:19:07.698: INFO: Deleting pod "simpletest.rc-z7k4k" in namespace "gc-4572"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:07.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4572" for this suite.

• [SLOW TEST:44.452 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":196,"skipped":3690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:07.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May  7 11:19:17.861: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2064 PodName:pod-sharedvolume-e9099ac1-e670-4269-b8b3-d0e3faafafb1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:19:17.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:19:17.862: INFO: ExecWithOptions: Clientset creation
May  7 11:19:17.863: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/emptydir-2064/pods/pod-sharedvolume-e9099ac1-e670-4269-b8b3-d0e3faafafb1/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
May  7 11:19:18.013: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:18.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2064" for this suite.

• [SLOW TEST:10.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":197,"skipped":3714,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:18.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-451" for this suite.

• [SLOW TEST:28.141 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":198,"skipped":3725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:19:46.602: INFO: Checking APIGroup: apiregistration.k8s.io
May  7 11:19:46.604: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  7 11:19:46.604: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  7 11:19:46.604: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  7 11:19:46.604: INFO: Checking APIGroup: apps
May  7 11:19:46.605: INFO: PreferredVersion.GroupVersion: apps/v1
May  7 11:19:46.606: INFO: Versions found [{apps/v1 v1}]
May  7 11:19:46.606: INFO: apps/v1 matches apps/v1
May  7 11:19:46.606: INFO: Checking APIGroup: events.k8s.io
May  7 11:19:46.607: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  7 11:19:46.607: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.607: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  7 11:19:46.607: INFO: Checking APIGroup: authentication.k8s.io
May  7 11:19:46.608: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  7 11:19:46.608: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  7 11:19:46.608: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  7 11:19:46.609: INFO: Checking APIGroup: authorization.k8s.io
May  7 11:19:46.610: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  7 11:19:46.610: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  7 11:19:46.610: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  7 11:19:46.610: INFO: Checking APIGroup: autoscaling
May  7 11:19:46.611: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  7 11:19:46.611: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May  7 11:19:46.611: INFO: autoscaling/v2 matches autoscaling/v2
May  7 11:19:46.611: INFO: Checking APIGroup: batch
May  7 11:19:46.613: INFO: PreferredVersion.GroupVersion: batch/v1
May  7 11:19:46.613: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May  7 11:19:46.613: INFO: batch/v1 matches batch/v1
May  7 11:19:46.613: INFO: Checking APIGroup: certificates.k8s.io
May  7 11:19:46.614: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  7 11:19:46.615: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  7 11:19:46.615: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  7 11:19:46.615: INFO: Checking APIGroup: networking.k8s.io
May  7 11:19:46.616: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  7 11:19:46.616: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  7 11:19:46.616: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  7 11:19:46.616: INFO: Checking APIGroup: policy
May  7 11:19:46.618: INFO: PreferredVersion.GroupVersion: policy/v1
May  7 11:19:46.618: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May  7 11:19:46.618: INFO: policy/v1 matches policy/v1
May  7 11:19:46.618: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  7 11:19:46.619: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  7 11:19:46.619: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  7 11:19:46.619: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  7 11:19:46.619: INFO: Checking APIGroup: storage.k8s.io
May  7 11:19:46.621: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  7 11:19:46.621: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.621: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  7 11:19:46.621: INFO: Checking APIGroup: admissionregistration.k8s.io
May  7 11:19:46.623: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  7 11:19:46.623: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  7 11:19:46.623: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  7 11:19:46.623: INFO: Checking APIGroup: apiextensions.k8s.io
May  7 11:19:46.624: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  7 11:19:46.624: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  7 11:19:46.624: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  7 11:19:46.624: INFO: Checking APIGroup: scheduling.k8s.io
May  7 11:19:46.626: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  7 11:19:46.626: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  7 11:19:46.626: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  7 11:19:46.626: INFO: Checking APIGroup: coordination.k8s.io
May  7 11:19:46.627: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  7 11:19:46.627: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  7 11:19:46.627: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  7 11:19:46.627: INFO: Checking APIGroup: node.k8s.io
May  7 11:19:46.628: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  7 11:19:46.628: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.628: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  7 11:19:46.628: INFO: Checking APIGroup: discovery.k8s.io
May  7 11:19:46.629: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  7 11:19:46.630: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.630: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  7 11:19:46.630: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  7 11:19:46.631: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May  7 11:19:46.631: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.631: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May  7 11:19:46.631: INFO: Checking APIGroup: nsx.vmware.com
May  7 11:19:46.632: INFO: PreferredVersion.GroupVersion: nsx.vmware.com/v1
May  7 11:19:46.632: INFO: Versions found [{nsx.vmware.com/v1 v1}]
May  7 11:19:46.632: INFO: nsx.vmware.com/v1 matches nsx.vmware.com/v1
May  7 11:19:46.632: INFO: Checking APIGroup: vmware.com
May  7 11:19:46.634: INFO: PreferredVersion.GroupVersion: vmware.com/v1alpha1
May  7 11:19:46.634: INFO: Versions found [{vmware.com/v1alpha1 v1alpha1}]
May  7 11:19:46.634: INFO: vmware.com/v1alpha1 matches vmware.com/v1alpha1
May  7 11:19:46.634: INFO: Checking APIGroup: pksapi.io
May  7 11:19:46.635: INFO: PreferredVersion.GroupVersion: pksapi.io/v1beta1
May  7 11:19:46.636: INFO: Versions found [{pksapi.io/v1beta1 v1beta1}]
May  7 11:19:46.636: INFO: pksapi.io/v1beta1 matches pksapi.io/v1beta1
May  7 11:19:46.636: INFO: Checking APIGroup: metrics.k8s.io
May  7 11:19:46.639: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May  7 11:19:46.639: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May  7 11:19:46.639: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:46.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6981" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":199,"skipped":3749,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:46.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
May  7 11:19:46.714: INFO: Waiting up to 5m0s for pod "downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6" in namespace "downward-api-6942" to be "Succeeded or Failed"
May  7 11:19:46.724: INFO: Pod "downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.792132ms
May  7 11:19:48.732: INFO: Pod "downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017125213s
May  7 11:19:50.740: INFO: Pod "downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02588045s
STEP: Saw pod success
May  7 11:19:50.740: INFO: Pod "downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6" satisfied condition "Succeeded or Failed"
May  7 11:19:50.745: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6 container dapi-container: <nil>
STEP: delete the pod
May  7 11:19:50.779: INFO: Waiting for pod downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6 to disappear
May  7 11:19:50.787: INFO: Pod downward-api-f516b86d-4f26-409a-bdf9-34ad90a26be6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:50.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6942" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3756,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:50.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:19:51.092: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:19:53.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 19, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 19, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 19, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 19, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:19:56.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:19:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5123" for this suite.
STEP: Destroying namespace "webhook-5123-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":201,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:19:56.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
May  7 11:19:56.556: INFO: Waiting up to 5m0s for pod "var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50" in namespace "var-expansion-5461" to be "Succeeded or Failed"
May  7 11:19:56.567: INFO: Pod "var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50": Phase="Pending", Reason="", readiness=false. Elapsed: 11.31729ms
May  7 11:19:58.573: INFO: Pod "var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017553083s
May  7 11:20:00.585: INFO: Pod "var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028675247s
STEP: Saw pod success
May  7 11:20:00.585: INFO: Pod "var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50" satisfied condition "Succeeded or Failed"
May  7 11:20:00.589: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50 container dapi-container: <nil>
STEP: delete the pod
May  7 11:20:00.609: INFO: Waiting for pod var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50 to disappear
May  7 11:20:00.616: INFO: Pod var-expansion-969ab88f-5a34-44af-8461-6b018af0fa50 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:00.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5461" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3790,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:00.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:20:04.710: INFO: Deleting pod "var-expansion-78159414-118c-49ce-9076-365cbdf3cfb1" in namespace "var-expansion-7464"
May  7 11:20:04.717: INFO: Wait up to 5m0s for pod "var-expansion-78159414-118c-49ce-9076-365cbdf3cfb1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:06.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7464" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":203,"skipped":3796,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:06.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-99f1166a-3442-4065-bfa9-53c793d562bb
STEP: Creating a pod to test consume secrets
May  7 11:20:06.870: INFO: Waiting up to 5m0s for pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829" in namespace "secrets-4915" to be "Succeeded or Failed"
May  7 11:20:06.897: INFO: Pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829": Phase="Pending", Reason="", readiness=false. Elapsed: 27.678521ms
May  7 11:20:08.904: INFO: Pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034584366s
May  7 11:20:10.917: INFO: Pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046898353s
May  7 11:20:12.929: INFO: Pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058960986s
STEP: Saw pod success
May  7 11:20:12.929: INFO: Pod "pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829" satisfied condition "Succeeded or Failed"
May  7 11:20:12.933: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829 container secret-volume-test: <nil>
STEP: delete the pod
May  7 11:20:12.969: INFO: Waiting for pod pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829 to disappear
May  7 11:20:12.972: INFO: Pod pod-secrets-07634e43-90ef-4c68-9db0-171e8487e829 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:12.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4915" for this suite.
STEP: Destroying namespace "secret-namespace-6851" for this suite.

• [SLOW TEST:6.252 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3798,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:12.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-4297
STEP: creating replication controller nodeport-test in namespace services-4297
I0507 11:20:13.092367      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4297, replica count: 2
I0507 11:20:16.142879      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:20:19.143: INFO: Creating new exec pod
I0507 11:20:19.143875      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:20:24.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  7 11:20:25.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  7 11:20:25.276: INFO: stdout: ""
May  7 11:20:26.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  7 11:20:26.424: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  7 11:20:26.424: INFO: stdout: "nodeport-test-wpkdf"
May  7 11:20:26.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.134.183 80'
May  7 11:20:26.597: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.134.183 80\nConnection to 10.150.134.183 80 port [tcp/http] succeeded!\n"
May  7 11:20:26.597: INFO: stdout: ""
May  7 11:20:27.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.134.183 80'
May  7 11:20:27.783: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.134.183 80\nConnection to 10.150.134.183 80 port [tcp/http] succeeded!\n"
May  7 11:20:27.783: INFO: stdout: "nodeport-test-6ktth"
May  7 11:20:27.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.5 32250'
May  7 11:20:27.932: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.5 32250\nConnection to 30.1.0.5 32250 port [tcp/*] succeeded!\n"
May  7 11:20:27.932: INFO: stdout: ""
May  7 11:20:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.5 32250'
May  7 11:20:29.098: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.5 32250\nConnection to 30.1.0.5 32250 port [tcp/*] succeeded!\n"
May  7 11:20:29.098: INFO: stdout: ""
May  7 11:20:29.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.5 32250'
May  7 11:20:30.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.5 32250\nConnection to 30.1.0.5 32250 port [tcp/*] succeeded!\n"
May  7 11:20:30.092: INFO: stdout: "nodeport-test-6ktth"
May  7 11:20:30.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-4297 exec execpodsjql5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 30.1.0.4 32250'
May  7 11:20:30.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 30.1.0.4 32250\nConnection to 30.1.0.4 32250 port [tcp/*] succeeded!\n"
May  7 11:20:30.245: INFO: stdout: "nodeport-test-6ktth"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:30.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4297" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.268 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":205,"skipped":3814,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:30.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:20:30.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:20:32.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:20:34.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:20:37.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:20:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4337-crds.webhook.example.com via the AdmissionRegistration API
May  7 11:20:38.358: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:41.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1976" for this suite.
STEP: Destroying namespace "webhook-1976-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.906 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":206,"skipped":3826,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-68610ee2-89c1-4838-a94c-6f03983526ce
STEP: Creating a pod to test consume secrets
May  7 11:20:41.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0" in namespace "projected-9890" to be "Succeeded or Failed"
May  7 11:20:41.294: INFO: Pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.453053ms
May  7 11:20:43.303: INFO: Pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023824904s
May  7 11:20:45.313: INFO: Pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032890365s
May  7 11:20:47.327: INFO: Pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047124037s
STEP: Saw pod success
May  7 11:20:47.327: INFO: Pod "pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0" satisfied condition "Succeeded or Failed"
May  7 11:20:47.331: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  7 11:20:47.352: INFO: Waiting for pod pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0 to disappear
May  7 11:20:47.356: INFO: Pod pod-projected-secrets-eccb1ef1-4f8b-44cd-8020-78a2ea006fa0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:47.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9890" for this suite.

• [SLOW TEST:6.204 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3832,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:47.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:20:47.512: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  7 11:20:47.531: INFO: Pod name sample-pod: Found 0 pods out of 1
May  7 11:20:52.542: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  7 11:20:52.542: INFO: Creating deployment "test-rolling-update-deployment"
May  7 11:20:52.550: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  7 11:20:52.582: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
May  7 11:20:54.596: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  7 11:20:54.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:20:56.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:20:58.607: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:20:58.620: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4994  968cc371-18c3-4f9a-bc05-da475c379a14 23492 1 2022-05-07 11:20:52 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-05-07 11:20:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:20:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054f3da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-07 11:20:52 +0000 UTC,LastTransitionTime:2022-05-07 11:20:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-05-07 11:20:56 +0000 UTC,LastTransitionTime:2022-05-07 11:20:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  7 11:20:58.625: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-4994  a6502bbc-249b-4043-8754-f368f06a17f3 23482 1 2022-05-07 11:20:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 968cc371-18c3-4f9a-bc05-da475c379a14 0xc0060ecf07 0xc0060ecf08}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:20:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"968cc371-18c3-4f9a-bc05-da475c379a14\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:20:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060ecfb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  7 11:20:58.625: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  7 11:20:58.625: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4994  f563a0fe-d676-427d-b255-ed8afa53a87a 23491 2 2022-05-07 11:20:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 968cc371-18c3-4f9a-bc05-da475c379a14 0xc0060ecdd7 0xc0060ecdd8}] []  [{e2e.test Update apps/v1 2022-05-07 11:20:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:20:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"968cc371-18c3-4f9a-bc05-da475c379a14\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:20:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060ece98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:20:58.630: INFO: Pod "test-rolling-update-deployment-796dbc4547-j8848" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-j8848 test-rolling-update-deployment-796dbc4547- deployment-4994  99ad14f5-8f8b-420c-ae0b-a0ffba683ac1 23481 0 2022-05-07 11:20:52 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 a6502bbc-249b-4043-8754-f368f06a17f3 0xc0060ed417 0xc0060ed418}] []  [{kube-controller-manager Update v1 2022-05-07 11:20:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6502bbc-249b-4043-8754-f368f06a17f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:20:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9djd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9djd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:20:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:20:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:20:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.7.3,StartTime:2022-05-07 11:20:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:20:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://47b61158da54814506b0bb57b6fd8a2f04558fd38b2db663e62a118d6718cdaf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:20:58.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4994" for this suite.

• [SLOW TEST:11.265 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":208,"skipped":3843,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:20:58.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  7 11:20:59.445: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  7 11:21:01.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:21:03.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:21:06.509: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:21:06.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:09.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-649" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.129 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":209,"skipped":3853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:09.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:21:13.887: INFO: Deleting pod "var-expansion-26c39e81-450a-4441-86ce-f1aa463f121e" in namespace "var-expansion-9"
May  7 11:21:13.900: INFO: Wait up to 5m0s for pod "var-expansion-26c39e81-450a-4441-86ce-f1aa463f121e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:15.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9" for this suite.

• [SLOW TEST:6.182 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":210,"skipped":4006,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:15.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
May  7 11:21:16.049: INFO: Waiting up to 5m0s for pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a" in namespace "containers-1710" to be "Succeeded or Failed"
May  7 11:21:16.061: INFO: Pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026244ms
May  7 11:21:18.075: INFO: Pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025412161s
May  7 11:21:20.084: INFO: Pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034518227s
May  7 11:21:22.093: INFO: Pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044052012s
STEP: Saw pod success
May  7 11:21:22.094: INFO: Pod "client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a" satisfied condition "Succeeded or Failed"
May  7 11:21:22.097: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a container agnhost-container: <nil>
STEP: delete the pod
May  7 11:21:22.122: INFO: Waiting for pod client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a to disappear
May  7 11:21:22.126: INFO: Pod client-containers-e6314016-708a-4e65-85a5-e8a7b47beb8a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:22.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1710" for this suite.

• [SLOW TEST:6.199 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":4014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:22.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
May  7 11:21:22.238: INFO: Pod name sample-pod: Found 0 pods out of 3
May  7 11:21:27.247: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
May  7 11:21:27.251: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:27.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-595" for this suite.

• [SLOW TEST:5.142 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":212,"skipped":4044,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
May  7 11:21:33.919: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2585 pod-service-account-71bd6ed1-1aeb-46cf-8278-a3a9c4e2e682 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May  7 11:21:34.099: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2585 pod-service-account-71bd6ed1-1aeb-46cf-8278-a3a9c4e2e682 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May  7 11:21:34.260: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2585 pod-service-account-71bd6ed1-1aeb-46cf-8278-a3a9c4e2e682 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:34.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2585" for this suite.

• [SLOW TEST:7.156 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":213,"skipped":4056,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:34.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ef26bd49-b8e3-44ce-8139-49709254fb9e
STEP: Creating a pod to test consume configMaps
May  7 11:21:34.535: INFO: Waiting up to 5m0s for pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f" in namespace "configmap-1366" to be "Succeeded or Failed"
May  7 11:21:34.569: INFO: Pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f": Phase="Pending", Reason="", readiness=false. Elapsed: 33.478447ms
May  7 11:21:36.581: INFO: Pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045413074s
May  7 11:21:38.586: INFO: Pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050131276s
May  7 11:21:40.600: INFO: Pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064831387s
STEP: Saw pod success
May  7 11:21:40.601: INFO: Pod "pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f" satisfied condition "Succeeded or Failed"
May  7 11:21:40.604: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f container agnhost-container: <nil>
STEP: delete the pod
May  7 11:21:40.640: INFO: Waiting for pod pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f to disappear
May  7 11:21:40.648: INFO: Pod pod-configmaps-06a557ae-61f0-4ab6-836d-22940b78723f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1366" for this suite.

• [SLOW TEST:6.216 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4071,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:40.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May  7 11:21:40.858: INFO: The status of Pod labelsupdate4191d76b-c0ba-4556-a14f-179c37339245 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:21:42.868: INFO: The status of Pod labelsupdate4191d76b-c0ba-4556-a14f-179c37339245 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:21:44.867: INFO: The status of Pod labelsupdate4191d76b-c0ba-4556-a14f-179c37339245 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:21:46.868: INFO: The status of Pod labelsupdate4191d76b-c0ba-4556-a14f-179c37339245 is Running (Ready = true)
May  7 11:21:47.406: INFO: Successfully updated pod "labelsupdate4191d76b-c0ba-4556-a14f-179c37339245"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:21:49.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7546" for this suite.

• [SLOW TEST:8.767 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":4076,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:21:49.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4938 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4938;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4938 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4938;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4938.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4938.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4938.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4938.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4938.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4938.svc;check="$$(dig +notcp +noall +answer +search 206.145.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.145.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.145.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.145.206_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4938 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4938;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4938 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4938;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4938.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4938.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4938.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4938.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4938.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4938.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4938.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4938.svc;check="$$(dig +notcp +noall +answer +search 206.145.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.145.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.145.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.145.206_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:21:55.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.629: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.640: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.647: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.653: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.676: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.683: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.687: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.695: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.701: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.706: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.711: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.719: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:21:55.743: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:00.752: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.758: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.784: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.792: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.797: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.847: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.853: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.863: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.867: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.871: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.875: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:00.902: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:05.753: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.759: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.765: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.781: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.807: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.813: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.845: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.850: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.856: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.867: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.873: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.878: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.883: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:05.901: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:10.752: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.868: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.877: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.887: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.894: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.903: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.957: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.963: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.968: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.987: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:10.995: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:11.001: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:11.009: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:11.029: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:15.749: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.753: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.757: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.767: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.771: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.816: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.825: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.834: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.838: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.842: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.846: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:15.865: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:20.755: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.762: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.783: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.788: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.793: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.812: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.817: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.822: INFO: Unable to read jessie_udp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938 from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.831: INFO: Unable to read jessie_udp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc from pod dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361: the server could not find the requested resource (get pods dns-test-264352f2-56c8-4840-9a46-b53540a2e361)
May  7 11:22:20.862: INFO: Lookups using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4938 wheezy_tcp@dns-test-service.dns-4938 wheezy_udp@dns-test-service.dns-4938.svc wheezy_tcp@dns-test-service.dns-4938.svc wheezy_udp@_http._tcp.dns-test-service.dns-4938.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4938.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4938 jessie_tcp@dns-test-service.dns-4938 jessie_udp@dns-test-service.dns-4938.svc jessie_tcp@dns-test-service.dns-4938.svc jessie_udp@_http._tcp.dns-test-service.dns-4938.svc jessie_tcp@_http._tcp.dns-test-service.dns-4938.svc]

May  7 11:22:25.892: INFO: DNS probes using dns-4938/dns-test-264352f2-56c8-4840-9a46-b53540a2e361 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:26.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4938" for this suite.

• [SLOW TEST:36.695 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":216,"skipped":4078,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:26.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7205
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7205
STEP: Waiting until pod test-pod will start running in namespace statefulset-7205
STEP: Creating statefulset with conflicting port in namespace statefulset-7205
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7205
May  7 11:22:30.321: INFO: Observed stateful pod in namespace: statefulset-7205, name: ss-0, uid: f3cf62ea-b0c6-4d96-b163-d682263f61a7, status phase: Pending. Waiting for statefulset controller to delete.
May  7 11:22:30.357: INFO: Observed stateful pod in namespace: statefulset-7205, name: ss-0, uid: f3cf62ea-b0c6-4d96-b163-d682263f61a7, status phase: Failed. Waiting for statefulset controller to delete.
May  7 11:22:30.386: INFO: Observed stateful pod in namespace: statefulset-7205, name: ss-0, uid: f3cf62ea-b0c6-4d96-b163-d682263f61a7, status phase: Failed. Waiting for statefulset controller to delete.
May  7 11:22:30.386: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7205
STEP: Removing pod with conflicting port in namespace statefulset-7205
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7205 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 11:22:34.473: INFO: Deleting all statefulset in ns statefulset-7205
May  7 11:22:34.478: INFO: Scaling statefulset ss to 0
May  7 11:22:44.510: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:22:44.514: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:44.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7205" for this suite.

• [SLOW TEST:18.390 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":217,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:44.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:44.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7665" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":218,"skipped":4129,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:44.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:22:44.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef" in namespace "downward-api-9023" to be "Succeeded or Failed"
May  7 11:22:44.868: INFO: Pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 28.89804ms
May  7 11:22:46.884: INFO: Pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044681724s
May  7 11:22:48.892: INFO: Pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef": Phase="Running", Reason="", readiness=true. Elapsed: 4.053515081s
May  7 11:22:50.899: INFO: Pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060012446s
STEP: Saw pod success
May  7 11:22:50.899: INFO: Pod "downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef" satisfied condition "Succeeded or Failed"
May  7 11:22:50.906: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef container client-container: <nil>
STEP: delete the pod
May  7 11:22:50.943: INFO: Waiting for pod downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef to disappear
May  7 11:22:50.949: INFO: Pod downwardapi-volume-5132214c-e3be-4911-a416-fb56a40bc4ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:50.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9023" for this suite.

• [SLOW TEST:6.191 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4132,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:50.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:22:51.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6796" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":220,"skipped":4145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:51.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:22:51.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd" in namespace "downward-api-258" to be "Succeeded or Failed"
May  7 11:22:51.647: INFO: Pod "downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.455283ms
May  7 11:22:53.653: INFO: Pod "downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01868994s
May  7 11:22:55.661: INFO: Pod "downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026499246s
STEP: Saw pod success
May  7 11:22:55.661: INFO: Pod "downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd" satisfied condition "Succeeded or Failed"
May  7 11:22:55.664: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd container client-container: <nil>
STEP: delete the pod
May  7 11:22:55.703: INFO: Waiting for pod downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd to disappear
May  7 11:22:55.706: INFO: Pod downwardapi-volume-d6bed433-d8ed-4c12-ab6c-0ccf0d7b44fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:22:55.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-258" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4171,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:22:55.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:24.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4526" for this suite.

• [SLOW TEST:28.463 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4178,"failed":0}
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:31.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-965" for this suite.

• [SLOW TEST:7.279 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":223,"skipped":4178,"failed":0}
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:31.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:23:31.509: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f" in namespace "security-context-test-904" to be "Succeeded or Failed"
May  7 11:23:31.514: INFO: Pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.4713ms
May  7 11:23:33.524: INFO: Pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014869101s
May  7 11:23:35.543: INFO: Pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034160572s
May  7 11:23:35.543: INFO: Pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f" satisfied condition "Succeeded or Failed"
May  7 11:23:35.552: INFO: Got logs for pod "busybox-privileged-false-6a2e23ea-8a55-4917-ac5f-2b3ff077c05f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:35.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-904" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":4184,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:35.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:39.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8132" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4188,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
May  7 11:23:39.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-152 create -f -'
May  7 11:23:40.906: INFO: stderr: ""
May  7 11:23:40.906: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May  7 11:23:40.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-152 diff -f -'
May  7 11:23:41.124: INFO: rc: 1
May  7 11:23:41.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-152 delete -f -'
May  7 11:23:41.214: INFO: stderr: ""
May  7 11:23:41.214: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:41.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-152" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":226,"skipped":4208,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:41.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May  7 11:23:41.976: INFO: starting watch
STEP: patching
STEP: updating
May  7 11:23:41.996: INFO: waiting for watch events with expected annotations
May  7 11:23:41.996: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:23:42.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4693" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":227,"skipped":4226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:23:42.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:02.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7705" for this suite.

• [SLOW TEST:80.126 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":228,"skipped":4262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:25:02.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 create -f -'
May  7 11:25:02.549: INFO: stderr: ""
May  7 11:25:02.549: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  7 11:25:02.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 create -f -'
May  7 11:25:02.760: INFO: stderr: ""
May  7 11:25:02.760: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May  7 11:25:03.766: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 11:25:03.767: INFO: Found 0 / 1
May  7 11:25:04.773: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 11:25:04.773: INFO: Found 0 / 1
May  7 11:25:05.769: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 11:25:05.769: INFO: Found 0 / 1
May  7 11:25:06.767: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 11:25:06.767: INFO: Found 1 / 1
May  7 11:25:06.767: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  7 11:25:06.772: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 11:25:06.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  7 11:25:06.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 describe pod agnhost-primary-jlx2g'
May  7 11:25:06.845: INFO: stderr: ""
May  7 11:25:06.845: INFO: stdout: "Name:         agnhost-primary-jlx2g\nNamespace:    kubectl-9629\nPriority:     0\nNode:         5615889a-d356-43b5-a818-02fb040c6965/30.1.0.5\nStart Time:   Sat, 07 May 2022 11:25:02 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           40.0.7.2\nIPs:\n  IP:           40.0.7.2\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b5922d114a22a622d244dcd3d2512d96fc676aa4b903377fb1ee4ec10e1dfdab\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 07 May 2022 11:25:06 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-65nsr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-65nsr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-9629/agnhost-primary-jlx2g to 5615889a-d356-43b5-a818-02fb040c6965\n  Normal  Pulled     0s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
May  7 11:25:06.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 describe rc agnhost-primary'
May  7 11:25:06.921: INFO: stderr: ""
May  7 11:25:06.921: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9629\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-jlx2g\n"
May  7 11:25:06.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 describe service agnhost-primary'
May  7 11:25:07.001: INFO: stderr: ""
May  7 11:25:07.001: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9629\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.150.247.238\nIPs:               10.150.247.238\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         40.0.7.2:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  7 11:25:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 describe node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7'
May  7 11:25:07.130: INFO: stderr: ""
May  7 11:25:07.130: INFO: stdout: "Name:               17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    bosh.id=5ff5f90f-a5ee-460b-867e-c6d04fdc4d1a\n                    bosh.zone=az-0\n                    failure-domain.beta.kubernetes.io/zone=az-0\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=30.1.0.3\n                    kubernetes.io/os=linux\n                    pks-system/cluster.name=59984d5b-090b-4fb8-99dd-f5cec14b7bf4-internal\n                    pks-system/cluster.uuid=service-instance_c3b8efe1-7091-42f0-86af-1a9463d72b5b\n                    spec.ip=30.1.0.3\n                    topology.kubernetes.io/zone=az-0\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 07 May 2022 09:48:39 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 07 May 2022 11:25:05 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 07 May 2022 11:21:03 +0000   Sat, 07 May 2022 09:48:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 07 May 2022 11:21:03 +0000   Sat, 07 May 2022 09:48:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 07 May 2022 11:21:03 +0000   Sat, 07 May 2022 09:48:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 07 May 2022 11:21:03 +0000   Sat, 07 May 2022 09:48:50 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  30.1.0.3\n  InternalIP:  30.1.0.3\n  Hostname:    30.1.0.3\nCapacity:\n  cpu:                4\n  ephemeral-storage:  32894832Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16425540Ki\n  pods:               110\nAllocatable:\n  cpu:                3500m\n  ephemeral-storage:  29242135298\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             14864964Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 e680a3d05d264eb2a8a864dafea6df3e\n  System UUID:                420CA942-DA88-4064-3FE5-2408ECE20E29\n  Boot ID:                    e9baa3f5-9d9a-4692-ad68-486ea77e6e30\n  Kernel Version:             4.15.0-176-generic\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.0\n  Kubelet Version:            v1.23.4+vmware.1\n  Kube-Proxy Version:         v1.23.4+vmware.1\nProviderID:                   vsphere://420ca942-da88-4064-3fe5-2408ece20e29\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787c57488d-xh45r                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     87m\n  pks-system                  event-controller-795755d67-lkvmj                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  pks-system                  fluent-bit-lgdqp                                           0 (0%)        0 (0%)      100Mi (0%)       100Mi (0%)     58m\n  pks-system                  node-exporter-cggqd                                        10m (0%)      0 (0%)      50Mi (0%)        150Mi (1%)     87m\n  pks-system                  observability-manager-764bb5b6d9-5pw7g                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  pks-system                  sink-controller-77c8c69d54-vmhrt                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  pks-system                  telegraf-8f7tz                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  pks-system                  telemetry-agent-6f6f75b47-qvqbh                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  pks-system                  wavefront-collector-6b7bf647bb-zvh8w                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  sonobuoy                    sonobuoy-e2e-job-fe2325511c6f494a                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                110m (3%)   0 (0%)\n  memory             220Mi (1%)  420Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                                            Age    From        Message\n  ----     ------                                            ----   ----        -------\n  Warning  listen tcp4 :30475: bind: address already in use  53m    kube-proxy  can't open port \"nodePort for services-456/externalname-service:http\" (:30475/tcp4), skipping it\n  Warning  listen tcp4 :30173: bind: address already in use  42m    kube-proxy  can't open port \"nodePort for services-6197/nodeport-service\" (:30173/tcp4), skipping it\n  Warning  listen tcp4 :30453: bind: address already in use  23m    kube-proxy  can't open port \"nodePort for services-6184/affinity-nodeport-timeout\" (:30453/tcp4), skipping it\n  Warning  listen tcp4 :30770: bind: address already in use  10m    kube-proxy  can't open port \"nodePort for services-5609/affinity-nodeport\" (:30770/tcp4), skipping it\n  Warning  listen tcp4 :32098: bind: address already in use  8m56s  kube-proxy  can't open port \"nodePort for services-1819/affinity-nodeport-transition\" (:32098/tcp4), skipping it\n  Warning  listen tcp4 :32250: bind: address already in use  4m54s  kube-proxy  can't open port \"nodePort for services-4297/nodeport-test:http\" (:32250/tcp4), skipping it\n"
May  7 11:25:07.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-9629 describe namespace kubectl-9629'
May  7 11:25:07.204: INFO: stderr: ""
May  7 11:25:07.204: INFO: stdout: "Name:         kubectl-9629\nLabels:       e2e-framework=kubectl\n              e2e-run=018beb03-baef-4b7c-b464-5fb8f5ad97e1\n              kubernetes.io/metadata.name=kubectl-9629\nAnnotations:  ncp/extpoolid: 25c55293-6bef-486f-8370-c62ef4df39dd\n              ncp/snat_ip: 192.168.160.111\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:07.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9629" for this suite.

• [SLOW TEST:5.009 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1107
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":229,"skipped":4306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:07.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:25:07.308: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May  7 11:25:07.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:07.330: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
May  7 11:25:07.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:07.360: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:08.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:08.368: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:09.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:09.366: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:10.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:10.366: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:11.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 11:25:11.367: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
May  7 11:25:11.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 11:25:11.395: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May  7 11:25:12.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:12.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May  7 11:25:12.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:12.431: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:13.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:13.441: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:14.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:14.439: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:15.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:15.441: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:16.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:16.440: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:25:17.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  7 11:25:17.441: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2536, will wait for the garbage collector to delete the pods
May  7 11:25:17.508: INFO: Deleting DaemonSet.extensions daemon-set took: 7.803041ms
May  7 11:25:17.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.768965ms
May  7 11:25:20.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:25:20.017: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 11:25:20.021: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24915"},"items":null}

May  7 11:25:20.023: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24915"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2536" for this suite.

• [SLOW TEST:12.838 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":230,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May  7 11:25:20.142: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5821  33742594-671e-4d90-984e-b69cc74d0302 24923 0 2022-05-07 11:25:20 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-05-07 11:25:20 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkm7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkm7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:25:20.154: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  7 11:25:22.162: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  7 11:25:24.160: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May  7 11:25:24.161: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5821 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:25:24.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:25:24.161: INFO: ExecWithOptions: Clientset creation
May  7 11:25:24.161: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/dns-5821/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
May  7 11:25:24.253: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5821 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:25:24.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:25:24.253: INFO: ExecWithOptions: Clientset creation
May  7 11:25:24.254: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/dns-5821/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 11:25:24.350: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:24.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5821" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":231,"skipped":4372,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:24.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  7 11:25:28.499: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:28.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5592" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4375,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:28.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:25:29.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:25:31.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 25, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 25, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:25:34.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:25:34.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4447" for this suite.
STEP: Destroying namespace "webhook-4447-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":233,"skipped":4376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:25:34.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May  7 11:25:34.600: INFO: Pod name wrapped-volume-race-efd329d9-abc1-49dd-8df7-3ba68cbc7d5a: Found 1 pods out of 5
May  7 11:25:39.614: INFO: Pod name wrapped-volume-race-efd329d9-abc1-49dd-8df7-3ba68cbc7d5a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-efd329d9-abc1-49dd-8df7-3ba68cbc7d5a in namespace emptydir-wrapper-4896, will wait for the garbage collector to delete the pods
May  7 11:25:51.715: INFO: Deleting ReplicationController wrapped-volume-race-efd329d9-abc1-49dd-8df7-3ba68cbc7d5a took: 9.098634ms
May  7 11:25:51.816: INFO: Terminating ReplicationController wrapped-volume-race-efd329d9-abc1-49dd-8df7-3ba68cbc7d5a pods took: 101.397599ms
STEP: Creating RC which spawns configmap-volume pods
May  7 11:25:55.041: INFO: Pod name wrapped-volume-race-35300473-962f-4762-aca2-726094bcce66: Found 0 pods out of 5
May  7 11:26:00.065: INFO: Pod name wrapped-volume-race-35300473-962f-4762-aca2-726094bcce66: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-35300473-962f-4762-aca2-726094bcce66 in namespace emptydir-wrapper-4896, will wait for the garbage collector to delete the pods
May  7 11:26:12.177: INFO: Deleting ReplicationController wrapped-volume-race-35300473-962f-4762-aca2-726094bcce66 took: 10.435579ms
May  7 11:26:12.377: INFO: Terminating ReplicationController wrapped-volume-race-35300473-962f-4762-aca2-726094bcce66 pods took: 200.72022ms
STEP: Creating RC which spawns configmap-volume pods
May  7 11:26:15.208: INFO: Pod name wrapped-volume-race-62a9a0fc-a9a3-471d-9ef3-85a7af65351d: Found 0 pods out of 5
May  7 11:26:20.229: INFO: Pod name wrapped-volume-race-62a9a0fc-a9a3-471d-9ef3-85a7af65351d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-62a9a0fc-a9a3-471d-9ef3-85a7af65351d in namespace emptydir-wrapper-4896, will wait for the garbage collector to delete the pods
May  7 11:26:32.412: INFO: Deleting ReplicationController wrapped-volume-race-62a9a0fc-a9a3-471d-9ef3-85a7af65351d took: 50.975762ms
May  7 11:26:32.513: INFO: Terminating ReplicationController wrapped-volume-race-62a9a0fc-a9a3-471d-9ef3-85a7af65351d pods took: 100.488391ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:26:36.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4896" for this suite.

• [SLOW TEST:61.896 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":234,"skipped":4418,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:26:36.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:26:36.922: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:26:38.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 26, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 26, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 26, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 26, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:26:41.976: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:26:42.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7381" for this suite.
STEP: Destroying namespace "webhook-7381-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":235,"skipped":4423,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:26:42.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-60775b78-85a7-4e1a-82db-e08dbde4f8a3
STEP: Creating a pod to test consume configMaps
May  7 11:26:42.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b" in namespace "configmap-7207" to be "Succeeded or Failed"
May  7 11:26:42.310: INFO: Pod "pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.210673ms
May  7 11:26:44.318: INFO: Pod "pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031275337s
May  7 11:26:46.326: INFO: Pod "pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038716849s
STEP: Saw pod success
May  7 11:26:46.326: INFO: Pod "pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b" satisfied condition "Succeeded or Failed"
May  7 11:26:46.330: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b container configmap-volume-test: <nil>
STEP: delete the pod
May  7 11:26:46.370: INFO: Waiting for pod pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b to disappear
May  7 11:26:46.375: INFO: Pod pod-configmaps-8f7248fa-d699-45a2-9e3a-ce7a7a6fb48b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:26:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7207" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4435,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:26:46.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-f1a6988d-51f5-431d-869f-129d7e2cc2bd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:26:50.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5285" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:26:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b741db45-90c8-4228-90f4-0f3f7eacb591
STEP: Creating a pod to test consume configMaps
May  7 11:26:50.621: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8" in namespace "projected-6745" to be "Succeeded or Failed"
May  7 11:26:50.628: INFO: Pod "pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.491846ms
May  7 11:26:52.639: INFO: Pod "pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017867242s
May  7 11:26:55.144: INFO: Pod "pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.523476126s
STEP: Saw pod success
May  7 11:26:55.144: INFO: Pod "pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8" satisfied condition "Succeeded or Failed"
May  7 11:26:55.149: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8 container agnhost-container: <nil>
STEP: delete the pod
May  7 11:26:55.176: INFO: Waiting for pod pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8 to disappear
May  7 11:26:55.179: INFO: Pod pod-projected-configmaps-1c9b1ab7-3f0e-4579-8420-ac9b516fafa8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:26:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6745" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:26:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May  7 11:26:55.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:26:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:08.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9016" for this suite.

• [SLOW TEST:13.572 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":239,"skipped":4577,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:08.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May  7 11:27:08.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:27:12.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6998" for this suite.

• [SLOW TEST:14.411 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":240,"skipped":4582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:23.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
May  7 11:27:23.233: INFO: Waiting up to 5m0s for pod "pod-61324a3a-b39d-4ba8-9590-5575aac4a78b" in namespace "emptydir-7538" to be "Succeeded or Failed"
May  7 11:27:23.236: INFO: Pod "pod-61324a3a-b39d-4ba8-9590-5575aac4a78b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.388589ms
May  7 11:27:25.241: INFO: Pod "pod-61324a3a-b39d-4ba8-9590-5575aac4a78b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008242032s
May  7 11:27:27.248: INFO: Pod "pod-61324a3a-b39d-4ba8-9590-5575aac4a78b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015669499s
STEP: Saw pod success
May  7 11:27:27.248: INFO: Pod "pod-61324a3a-b39d-4ba8-9590-5575aac4a78b" satisfied condition "Succeeded or Failed"
May  7 11:27:27.252: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-61324a3a-b39d-4ba8-9590-5575aac4a78b container test-container: <nil>
STEP: delete the pod
May  7 11:27:27.277: INFO: Waiting for pod pod-61324a3a-b39d-4ba8-9590-5575aac4a78b to disappear
May  7 11:27:27.285: INFO: Pod pod-61324a3a-b39d-4ba8-9590-5575aac4a78b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:27.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7538" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4617,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:27.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-bf6fda5e-423e-4267-87bb-7454bf57b7bf
STEP: Creating configMap with name cm-test-opt-upd-b3909a12-1f9a-4c13-8cf4-a138ccb3e55a
STEP: Creating the pod
May  7 11:27:27.389: INFO: The status of Pod pod-projected-configmaps-2bdaaa94-1975-413a-bd13-738a9cb62c5f is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:29.400: INFO: The status of Pod pod-projected-configmaps-2bdaaa94-1975-413a-bd13-738a9cb62c5f is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:31.396: INFO: The status of Pod pod-projected-configmaps-2bdaaa94-1975-413a-bd13-738a9cb62c5f is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-bf6fda5e-423e-4267-87bb-7454bf57b7bf
STEP: Updating configmap cm-test-opt-upd-b3909a12-1f9a-4c13-8cf4-a138ccb3e55a
STEP: Creating configMap with name cm-test-opt-create-5da9efcd-e3f7-4bac-8b75-5033861f2d6c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:33.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6337" for this suite.

• [SLOW TEST:6.207 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4620,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May  7 11:27:33.643: INFO: The status of Pod labelsupdatec4748ef1-7f32-49e8-a1ee-043e3447c3b5 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:35.649: INFO: The status of Pod labelsupdatec4748ef1-7f32-49e8-a1ee-043e3447c3b5 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:37.654: INFO: The status of Pod labelsupdatec4748ef1-7f32-49e8-a1ee-043e3447c3b5 is Running (Ready = true)
May  7 11:27:38.193: INFO: Successfully updated pod "labelsupdatec4748ef1-7f32-49e8-a1ee-043e3447c3b5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:40.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1762" for this suite.

• [SLOW TEST:6.729 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:40.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  7 11:27:40.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:27:40.363: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:27:41.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:27:41.381: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:27:42.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:27:42.378: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:27:43.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:27:43.388: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:27:44.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 11:27:44.376: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
May  7 11:27:44.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:44.403: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:45.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:45.420: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:46.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:46.419: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:47.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:47.418: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:48.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:48.417: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:49.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:49.419: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:50.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:27:50.415: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:27:51.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 11:27:51.416: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4734, will wait for the garbage collector to delete the pods
May  7 11:27:51.482: INFO: Deleting DaemonSet.extensions daemon-set took: 8.935753ms
May  7 11:27:51.584: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.366389ms
May  7 11:27:54.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:27:54.395: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 11:27:54.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26605"},"items":null}

May  7 11:27:54.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26605"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:27:54.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4734" for this suite.

• [SLOW TEST:14.206 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":244,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:27:54.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
May  7 11:27:54.513: INFO: The status of Pod annotationupdateaa0c90fd-cd81-4bfa-a106-0edc6303dedb is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:56.520: INFO: The status of Pod annotationupdateaa0c90fd-cd81-4bfa-a106-0edc6303dedb is Pending, waiting for it to be Running (with Ready = true)
May  7 11:27:58.519: INFO: The status of Pod annotationupdateaa0c90fd-cd81-4bfa-a106-0edc6303dedb is Running (Ready = true)
May  7 11:27:59.073: INFO: Successfully updated pod "annotationupdateaa0c90fd-cd81-4bfa-a106-0edc6303dedb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:28:01.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6911" for this suite.

• [SLOW TEST:6.658 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4708,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:28:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9398
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9398
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9398
May  7 11:28:01.202: INFO: Found 0 stateful pods, waiting for 1
May  7 11:28:11.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May  7 11:28:11.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:28:11.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:28:11.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:28:11.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 11:28:11.409: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  7 11:28:21.423: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  7 11:28:21.423: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:28:21.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999771s
May  7 11:28:22.454: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991892283s
May  7 11:28:23.460: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984707463s
May  7 11:28:24.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978889271s
May  7 11:28:25.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969343754s
May  7 11:28:26.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963406639s
May  7 11:28:27.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.953050703s
May  7 11:28:28.496: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.948073332s
May  7 11:28:29.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.942806314s
May  7 11:28:30.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 935.107969ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9398
May  7 11:28:31.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:28:31.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:28:31.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:28:31.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:28:31.671: INFO: Found 1 stateful pods, waiting for 3
May  7 11:28:41.683: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:28:41.683: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:28:41.683: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May  7 11:28:41.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:28:41.844: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:28:41.844: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:28:41.844: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 11:28:41.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:28:42.035: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:28:42.035: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:28:42.035: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 11:28:42.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:28:42.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:28:42.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:28:42.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 11:28:42.198: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:28:42.203: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  7 11:28:52.217: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  7 11:28:52.217: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  7 11:28:52.217: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  7 11:28:52.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999692s
May  7 11:28:53.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995067248s
May  7 11:28:54.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988048065s
May  7 11:28:55.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977528818s
May  7 11:28:56.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970627302s
May  7 11:28:57.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963989375s
May  7 11:28:58.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950952858s
May  7 11:28:59.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94058856s
May  7 11:29:00.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933777551s
May  7 11:29:01.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 922.378752ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9398
May  7 11:29:02.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:29:02.482: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:29:02.482: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:29:02.482: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:29:02.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:29:02.607: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:29:02.607: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:29:02.607: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:29:02.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-9398 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:29:02.760: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:29:02.760: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:29:02.760: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:29:02.760: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 11:29:12.789: INFO: Deleting all statefulset in ns statefulset-9398
May  7 11:29:12.792: INFO: Scaling statefulset ss to 0
May  7 11:29:12.802: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:29:12.804: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:29:12.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9398" for this suite.

• [SLOW TEST:71.739 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":246,"skipped":4725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:29:12.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:29:12.920: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May  7 11:29:12.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:29:12.933: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:29:13.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:29:13.948: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:29:14.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:29:14.946: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:29:15.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:29:15.944: INFO: Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 is running 0 daemon pod, expected 1
May  7 11:29:16.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:29:16.947: INFO: Node 5615889a-d356-43b5-a818-02fb040c6965 is running 0 daemon pod, expected 1
May  7 11:29:17.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 11:29:17.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May  7 11:29:18.017: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:18.017: INFO: Wrong image for pod: daemon-set-djslr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:18.017: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:19.043: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:19.043: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:20.044: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:20.044: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:21.041: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:21.041: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:21.041: INFO: Pod daemon-set-v8nz5 is not available
May  7 11:29:22.047: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:22.047: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:22.047: INFO: Pod daemon-set-v8nz5 is not available
May  7 11:29:23.041: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:23.041: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:23.041: INFO: Pod daemon-set-v8nz5 is not available
May  7 11:29:24.048: INFO: Wrong image for pod: daemon-set-bgblr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:24.048: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:24.048: INFO: Pod daemon-set-v8nz5 is not available
May  7 11:29:25.042: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:26.043: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:26.043: INFO: Pod daemon-set-lvq62 is not available
May  7 11:29:27.049: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:27.049: INFO: Pod daemon-set-lvq62 is not available
May  7 11:29:28.043: INFO: Wrong image for pod: daemon-set-g2mgw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May  7 11:29:28.043: INFO: Pod daemon-set-lvq62 is not available
May  7 11:29:31.042: INFO: Pod daemon-set-mgjjd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May  7 11:29:31.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:29:31.055: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:29:32.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:29:32.076: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:29:33.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  7 11:29:33.071: INFO: Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 is running 0 daemon pod, expected 1
May  7 11:29:34.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  7 11:29:34.066: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2987, will wait for the garbage collector to delete the pods
May  7 11:29:34.150: INFO: Deleting DaemonSet.extensions daemon-set took: 10.031245ms
May  7 11:29:34.250: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.669864ms
May  7 11:29:36.458: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  7 11:29:36.458: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  7 11:29:36.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27176"},"items":null}

May  7 11:29:36.465: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27176"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:29:36.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2987" for this suite.

• [SLOW TEST:23.644 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":247,"skipped":4755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:29:36.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2886
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May  7 11:29:36.579: INFO: Found 0 stateful pods, waiting for 3
May  7 11:29:46.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:29:46.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:29:46.590: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  7 11:29:56.588: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:29:56.588: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:29:56.588: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May  7 11:29:56.618: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May  7 11:30:06.666: INFO: Updating stateful set ss2
May  7 11:30:06.673: INFO: Waiting for Pod statefulset-2886/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
May  7 11:30:16.816: INFO: Found 2 stateful pods, waiting for 3
May  7 11:30:26.825: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:30:26.825: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:30:26.825: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May  7 11:30:26.852: INFO: Updating stateful set ss2
May  7 11:30:26.868: INFO: Waiting for Pod statefulset-2886/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May  7 11:30:36.908: INFO: Updating stateful set ss2
May  7 11:30:36.946: INFO: Waiting for StatefulSet statefulset-2886/ss2 to complete update
May  7 11:30:36.946: INFO: Waiting for Pod statefulset-2886/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 11:30:46.966: INFO: Deleting all statefulset in ns statefulset-2886
May  7 11:30:46.970: INFO: Scaling statefulset ss2 to 0
May  7 11:30:57.006: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:30:57.011: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:30:57.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2886" for this suite.

• [SLOW TEST:80.584 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":248,"skipped":4781,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:30:57.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:14.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6621" for this suite.

• [SLOW TEST:17.145 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":249,"skipped":4790,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-c83d154f-f503-47d3-b4a9-6657a0349b75
STEP: Creating a pod to test consume configMaps
May  7 11:31:14.283: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec" in namespace "configmap-5968" to be "Succeeded or Failed"
May  7 11:31:14.303: INFO: Pod "pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 20.406452ms
May  7 11:31:16.310: INFO: Pod "pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026586602s
May  7 11:31:18.319: INFO: Pod "pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03608418s
STEP: Saw pod success
May  7 11:31:18.319: INFO: Pod "pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec" satisfied condition "Succeeded or Failed"
May  7 11:31:18.327: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec container agnhost-container: <nil>
STEP: delete the pod
May  7 11:31:18.358: INFO: Waiting for pod pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec to disappear
May  7 11:31:18.360: INFO: Pod pod-configmaps-ba1d0285-bf10-40d9-9cb5-e864b342b8ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:18.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5968" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:18.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:18.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8202" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":251,"skipped":4820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:18.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May  7 11:31:18.651: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.652: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.666: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.666: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.734: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.734: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.766: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:18.766: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  7 11:31:21.732: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  7 11:31:21.732: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  7 11:31:21.874: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May  7 11:31:21.889: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 0
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.892: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.893: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.893: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.908: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.908: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.944: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.944: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:21.970: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:21.970: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:21.997: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:21.997: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:24.934: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:24.934: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:24.972: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
STEP: listing Deployments
May  7 11:31:24.982: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May  7 11:31:24.998: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May  7 11:31:25.015: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:25.024: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:25.049: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:25.121: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:25.128: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:27.931: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:27.952: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:27.974: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:27.993: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  7 11:31:30.766: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May  7 11:31:30.851: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:30.852: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:30.852: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:30.852: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:30.852: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 1
May  7 11:31:30.853: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:30.853: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:30.853: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:30.853: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 2
May  7 11:31:30.853: INFO: observed Deployment test-deployment in namespace deployment-951 with ReadyReplicas 3
STEP: deleting the Deployment
May  7 11:31:30.876: INFO: observed event type MODIFIED
May  7 11:31:30.876: INFO: observed event type MODIFIED
May  7 11:31:30.876: INFO: observed event type MODIFIED
May  7 11:31:30.876: INFO: observed event type MODIFIED
May  7 11:31:30.876: INFO: observed event type MODIFIED
May  7 11:31:30.877: INFO: observed event type MODIFIED
May  7 11:31:30.877: INFO: observed event type MODIFIED
May  7 11:31:30.877: INFO: observed event type MODIFIED
May  7 11:31:30.877: INFO: observed event type MODIFIED
May  7 11:31:30.877: INFO: observed event type MODIFIED
May  7 11:31:30.878: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:31:30.884: INFO: Log out all the ReplicaSets if there is no deployment created
May  7 11:31:30.891: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-951  35e0ba99-f982-4154-af78-1423b682e2d0 27952 4 2022-05-07 11:31:21 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment f9cceb50-f5f0-4829-8025-6e6952fd29b8 0xc0060ed5b7 0xc0060ed5b8}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:31:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9cceb50-f5f0-4829-8025-6e6952fd29b8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:31:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060ed650 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  7 11:31:30.898: INFO: pod: "test-deployment-5ddd8b47d8-m9mmp":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-m9mmp test-deployment-5ddd8b47d8- deployment-951  ffabde5f-30a5-4b13-b3b1-e3d39de0b85d 27948 0 2022-05-07 11:31:21 +0000 UTC 2022-05-07 11:31:31 +0000 UTC 0xc0060edad8 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 35e0ba99-f982-4154-af78-1423b682e2d0 0xc0060edb07 0xc0060edb08}] []  [{kube-controller-manager Update v1 2022-05-07 11:31:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35e0ba99-f982-4154-af78-1423b682e2d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:31:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.10.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcsxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcsxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.10.4,StartTime:2022-05-07 11:31:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:31:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:containerd://d5c65257ce30faffd21327ae0e164f502b45dd05dd847f10f0bdea3893d01fe9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.10.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  7 11:31:30.898: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-951  9a727782-33a6-4a35-ad44-ce694a6ef644 27868 3 2022-05-07 11:31:18 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment f9cceb50-f5f0-4829-8025-6e6952fd29b8 0xc0060ed6b7 0xc0060ed6b8}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9cceb50-f5f0-4829-8025-6e6952fd29b8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:31:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060ed740 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  7 11:31:30.907: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-951  f3f4e299-07ec-4318-9e86-cddf7f902a51 27944 2 2022-05-07 11:31:25 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment f9cceb50-f5f0-4829-8025-6e6952fd29b8 0xc0060ed7a7 0xc0060ed7a8}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:31:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9cceb50-f5f0-4829-8025-6e6952fd29b8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:31:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060ed830 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May  7 11:31:30.919: INFO: pod: "test-deployment-854fdc678-tl6m6":
&Pod{ObjectMeta:{test-deployment-854fdc678-tl6m6 test-deployment-854fdc678- deployment-951  8744ea12-0dbe-4bd6-990e-4a5bae98b6f2 27958 0 2022-05-07 11:31:27 +0000 UTC 2022-05-07 11:31:31 +0000 UTC 0xc003b3f108 map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 f3f4e299-07ec-4318-9e86-cddf7f902a51 0xc003b3f137 0xc003b3f138}] []  [{kube-controller-manager Update v1 2022-05-07 11:31:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3f4e299-07ec-4318-9e86-cddf7f902a51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:31:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.10.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8zhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8zhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:40.0.10.7,StartTime:2022-05-07 11:31:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:31:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://097988765600f790070e6669fbd8d64cc0fd7c05045725fe71f4dc6a2b9d739e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.10.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  7 11:31:30.919: INFO: pod: "test-deployment-854fdc678-zfxhp":
&Pod{ObjectMeta:{test-deployment-854fdc678-zfxhp test-deployment-854fdc678- deployment-951  82c15752-32e5-4635-b916-2ca27a7d73f6 27909 0 2022-05-07 11:31:25 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 f3f4e299-07ec-4318-9e86-cddf7f902a51 0xc003b3f337 0xc003b3f338}] []  [{kube-controller-manager Update v1 2022-05-07 11:31:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3f4e299-07ec-4318-9e86-cddf7f902a51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:31:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.10.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvxh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvxh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:31:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.10.6,StartTime:2022-05-07 11:31:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:31:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://62de224d3de53b54f08f4186762088c819460fa5681019b26d3087c85419c22f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.10.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:30.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-951" for this suite.

• [SLOW TEST:12.348 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":252,"skipped":4844,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:30.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May  7 11:31:31.011: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May  7 11:31:31.032: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:31.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4272" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":253,"skipped":4854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:31.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
May  7 11:31:31.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 create -f -'
May  7 11:31:33.566: INFO: stderr: ""
May  7 11:31:33.566: INFO: stdout: "pod/pause created\n"
May  7 11:31:33.566: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  7 11:31:33.566: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1579" to be "running and ready"
May  7 11:31:33.588: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.027277ms
May  7 11:31:35.597: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030454521s
May  7 11:31:37.607: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.040593691s
May  7 11:31:37.607: INFO: Pod "pause" satisfied condition "running and ready"
May  7 11:31:37.607: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
May  7 11:31:37.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 label pods pause testing-label=testing-label-value'
May  7 11:31:37.699: INFO: stderr: ""
May  7 11:31:37.699: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May  7 11:31:37.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 get pod pause -L testing-label'
May  7 11:31:37.776: INFO: stderr: ""
May  7 11:31:37.776: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May  7 11:31:37.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 label pods pause testing-label-'
May  7 11:31:37.858: INFO: stderr: ""
May  7 11:31:37.858: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
May  7 11:31:37.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 get pod pause -L testing-label'
May  7 11:31:37.925: INFO: stderr: ""
May  7 11:31:37.925: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
May  7 11:31:37.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 delete --grace-period=0 --force -f -'
May  7 11:31:38.010: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 11:31:38.010: INFO: stdout: "pod \"pause\" force deleted\n"
May  7 11:31:38.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 get rc,svc -l name=pause --no-headers'
May  7 11:31:38.097: INFO: stderr: "No resources found in kubectl-1579 namespace.\n"
May  7 11:31:38.097: INFO: stdout: ""
May  7 11:31:38.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-1579 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  7 11:31:38.158: INFO: stderr: ""
May  7 11:31:38.158: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:38.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1579" for this suite.

• [SLOW TEST:7.089 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1329
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":254,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:38.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May  7 11:31:38.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28085 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:31:38.238: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28086 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:31:38.238: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28087 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May  7 11:31:48.288: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28118 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:31:48.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28121 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  7 11:31:48.288: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4043  8c420438-9531-42aa-9797-e86be7a3e8dd 28122 0 2022-05-07 11:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-07 11:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:48.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4043" for this suite.

• [SLOW TEST:10.131 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":255,"skipped":4903,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:48.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:31:48.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a" in namespace "projected-5525" to be "Succeeded or Failed"
May  7 11:31:48.382: INFO: Pod "downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.217966ms
May  7 11:31:50.395: INFO: Pod "downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031613814s
May  7 11:31:52.405: INFO: Pod "downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041686382s
STEP: Saw pod success
May  7 11:31:52.405: INFO: Pod "downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a" satisfied condition "Succeeded or Failed"
May  7 11:31:52.409: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a container client-container: <nil>
STEP: delete the pod
May  7 11:31:52.439: INFO: Waiting for pod downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a to disappear
May  7 11:31:52.442: INFO: Pod downwardapi-volume-1b2cd08e-7719-4f7c-852b-ce6b188d4a6a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:31:52.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5525" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:31:52.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:03.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1453" for this suite.

• [SLOW TEST:11.272 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":257,"skipped":4955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:03.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:32:03.815: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May  7 11:32:07.853: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May  7 11:32:07.899: INFO: observed ReplicaSet test-rs in namespace replicaset-9714 with ReadyReplicas 1, AvailableReplicas 1
May  7 11:32:07.911: INFO: observed ReplicaSet test-rs in namespace replicaset-9714 with ReadyReplicas 1, AvailableReplicas 1
May  7 11:32:07.923: INFO: observed ReplicaSet test-rs in namespace replicaset-9714 with ReadyReplicas 1, AvailableReplicas 1
May  7 11:32:10.873: INFO: observed ReplicaSet test-rs in namespace replicaset-9714 with ReadyReplicas 2, AvailableReplicas 2
May  7 11:32:11.338: INFO: observed Replicaset test-rs in namespace replicaset-9714 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:11.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9714" for this suite.

• [SLOW TEST:7.616 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":258,"skipped":5018,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 11:32:17.451: INFO: DNS probes using dns-6076/dns-test-bb4910b6-a90b-4b10-8135-8bac4e69ad0e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:17.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6076" for this suite.

• [SLOW TEST:6.139 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":259,"skipped":5019,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:17.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  7 11:32:21.656: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:21.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4601" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":5025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:21.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:32:21.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a" in namespace "projected-2135" to be "Succeeded or Failed"
May  7 11:32:21.769: INFO: Pod "downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437638ms
May  7 11:32:23.775: INFO: Pod "downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015222786s
May  7 11:32:25.781: INFO: Pod "downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020614804s
STEP: Saw pod success
May  7 11:32:25.781: INFO: Pod "downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a" satisfied condition "Succeeded or Failed"
May  7 11:32:25.785: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a container client-container: <nil>
STEP: delete the pod
May  7 11:32:25.812: INFO: Waiting for pod downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a to disappear
May  7 11:32:25.816: INFO: Pod downwardapi-volume-2924be54-ef3c-4eeb-9f30-e5f14f1c961a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:25.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2135" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":5094,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:25.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  7 11:32:25.894: INFO: Waiting up to 5m0s for pod "pod-92fc9b35-4da2-4c77-be0a-575d563fb47f" in namespace "emptydir-3014" to be "Succeeded or Failed"
May  7 11:32:25.899: INFO: Pod "pod-92fc9b35-4da2-4c77-be0a-575d563fb47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675225ms
May  7 11:32:27.912: INFO: Pod "pod-92fc9b35-4da2-4c77-be0a-575d563fb47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016970313s
May  7 11:32:29.923: INFO: Pod "pod-92fc9b35-4da2-4c77-be0a-575d563fb47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028152253s
STEP: Saw pod success
May  7 11:32:29.923: INFO: Pod "pod-92fc9b35-4da2-4c77-be0a-575d563fb47f" satisfied condition "Succeeded or Failed"
May  7 11:32:29.927: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-92fc9b35-4da2-4c77-be0a-575d563fb47f container test-container: <nil>
STEP: delete the pod
May  7 11:32:29.953: INFO: Waiting for pod pod-92fc9b35-4da2-4c77-be0a-575d563fb47f to disappear
May  7 11:32:29.956: INFO: Pod pod-92fc9b35-4da2-4c77-be0a-575d563fb47f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3014" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":5097,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:29.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
May  7 11:32:30.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-3208 cluster-info'
May  7 11:32:30.106: INFO: stderr: ""
May  7 11:32:30.106: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:30.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3208" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":263,"skipped":5102,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:30.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:30.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-113" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":264,"skipped":5121,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:30.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
May  7 11:32:30.256: INFO: Waiting up to 5m0s for pod "test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d" in namespace "svcaccounts-8247" to be "Succeeded or Failed"
May  7 11:32:30.271: INFO: Pod "test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.718899ms
May  7 11:32:32.286: INFO: Pod "test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030513139s
May  7 11:32:34.291: INFO: Pod "test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035539753s
STEP: Saw pod success
May  7 11:32:34.292: INFO: Pod "test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d" satisfied condition "Succeeded or Failed"
May  7 11:32:34.295: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d container agnhost-container: <nil>
STEP: delete the pod
May  7 11:32:34.321: INFO: Waiting for pod test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d to disappear
May  7 11:32:34.324: INFO: Pod test-pod-8422b081-bdef-41cf-bbc8-b31d92a7d30d no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:34.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8247" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":265,"skipped":5135,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:34.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-sm8l
STEP: Creating a pod to test atomic-volume-subpath
May  7 11:32:34.416: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sm8l" in namespace "subpath-1268" to be "Succeeded or Failed"
May  7 11:32:34.427: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.556896ms
May  7 11:32:36.433: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01657742s
May  7 11:32:38.442: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 4.025750191s
May  7 11:32:40.455: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 6.038287796s
May  7 11:32:42.466: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 8.049315545s
May  7 11:32:44.471: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 10.054454352s
May  7 11:32:46.478: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 12.061537136s
May  7 11:32:48.504: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 14.087411433s
May  7 11:32:50.518: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 16.102060772s
May  7 11:32:52.531: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 18.115044804s
May  7 11:32:54.536: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 20.120143017s
May  7 11:32:56.544: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Running", Reason="", readiness=true. Elapsed: 22.12819876s
May  7 11:32:58.559: INFO: Pod "pod-subpath-test-configmap-sm8l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.142681616s
STEP: Saw pod success
May  7 11:32:58.559: INFO: Pod "pod-subpath-test-configmap-sm8l" satisfied condition "Succeeded or Failed"
May  7 11:32:58.562: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-subpath-test-configmap-sm8l container test-container-subpath-configmap-sm8l: <nil>
STEP: delete the pod
May  7 11:32:58.605: INFO: Waiting for pod pod-subpath-test-configmap-sm8l to disappear
May  7 11:32:58.608: INFO: Pod pod-subpath-test-configmap-sm8l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sm8l
May  7 11:32:58.608: INFO: Deleting pod "pod-subpath-test-configmap-sm8l" in namespace "subpath-1268"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:32:58.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1268" for this suite.

• [SLOW TEST:24.287 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":266,"skipped":5151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:32:58.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May  7 11:32:58.669: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  7 11:32:58.687: INFO: Waiting for terminating namespaces to be deleted...
May  7 11:32:58.691: INFO: 
Logging pods the apiserver thinks is on node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 before test
May  7 11:32:58.725: INFO: coredns-787c57488d-xh45r from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container coredns ready: true, restart count 0
May  7 11:32:58.725: INFO: event-controller-795755d67-lkvmj from pks-system started at 2022-05-07 10:26:54 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container event-controller ready: true, restart count 0
May  7 11:32:58.725: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:32:58.725: INFO: fluent-bit-lgdqp from pks-system started at 2022-05-07 10:27:00 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:32:58.725: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:32:58.725: INFO: node-exporter-cggqd from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:32:58.725: INFO: observability-manager-764bb5b6d9-5pw7g from pks-system started at 2022-05-07 10:26:47 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container observability-manager ready: true, restart count 0
May  7 11:32:58.725: INFO: sink-controller-77c8c69d54-vmhrt from pks-system started at 2022-05-07 10:26:54 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container sink-controller ready: true, restart count 0
May  7 11:32:58.725: INFO: telegraf-8f7tz from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:32:58.725: INFO: telemetry-agent-6f6f75b47-qvqbh from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May  7 11:32:58.725: INFO: wavefront-collector-6b7bf647bb-zvh8w from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container wavefront-collector ready: true, restart count 0
May  7 11:32:58.725: INFO: sonobuoy from sonobuoy started at 2022-05-07 10:16:30 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  7 11:32:58.725: INFO: sonobuoy-e2e-job-fe2325511c6f494a from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container e2e ready: true, restart count 0
May  7 11:32:58.725: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:32:58.725: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.725: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:32:58.725: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 11:32:58.725: INFO: 
Logging pods the apiserver thinks is on node 5615889a-d356-43b5-a818-02fb040c6965 before test
May  7 11:32:58.744: INFO: coredns-787c57488d-2wsp6 from kube-system started at 2022-05-07 10:41:08 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.744: INFO: 	Container coredns ready: true, restart count 0
May  7 11:32:58.744: INFO: fluent-bit-7vdxl from pks-system started at 2022-05-07 10:40:58 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.744: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:32:58.744: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:32:58.744: INFO: node-exporter-2bp48 from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.744: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:32:58.744: INFO: telegraf-x4jms from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.744: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:32:58.744: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-8dzqr from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:32:58.744: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 11:32:58.744: INFO: 
Logging pods the apiserver thinks is on node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 before test
May  7 11:32:58.760: INFO: coredns-787c57488d-mhgg5 from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container coredns ready: true, restart count 0
May  7 11:32:58.760: INFO: metrics-server-66c5bff789-gl2ww from kube-system started at 2022-05-07 10:26:48 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container metrics-server ready: true, restart count 0
May  7 11:32:58.760: INFO: fluent-bit-pzwr7 from pks-system started at 2022-05-07 10:27:01 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:32:58.760: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:32:58.760: INFO: metric-controller-669f9bc57b-tzdqq from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container metric-controller ready: true, restart count 0
May  7 11:32:58.760: INFO: node-exporter-bs69m from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:32:58.760: INFO: telegraf-dp5wl from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:32:58.760: INFO: validator-8567d4b66-q8584 from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container validator ready: true, restart count 0
May  7 11:32:58.760: INFO: wavefront-proxy-54bfcd9d6b-4lb72 from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container wavefront-proxy ready: true, restart count 0
May  7 11:32:58.760: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-qtqsp from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:32:58.760: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:32:58.760: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
STEP: verifying the node has the label node 5615889a-d356-43b5-a818-02fb040c6965
STEP: verifying the node has the label node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod coredns-787c57488d-2wsp6 requesting resource cpu=100m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.866: INFO: Pod coredns-787c57488d-mhgg5 requesting resource cpu=100m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod coredns-787c57488d-xh45r requesting resource cpu=100m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod metrics-server-66c5bff789-gl2ww requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod event-controller-795755d67-lkvmj requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod fluent-bit-7vdxl requesting resource cpu=0m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.866: INFO: Pod fluent-bit-lgdqp requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod fluent-bit-pzwr7 requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod metric-controller-669f9bc57b-tzdqq requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod node-exporter-2bp48 requesting resource cpu=10m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.866: INFO: Pod node-exporter-bs69m requesting resource cpu=10m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod node-exporter-cggqd requesting resource cpu=10m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod observability-manager-764bb5b6d9-5pw7g requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod sink-controller-77c8c69d54-vmhrt requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod telegraf-8f7tz requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod telegraf-dp5wl requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod telegraf-x4jms requesting resource cpu=0m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.866: INFO: Pod telemetry-agent-6f6f75b47-qvqbh requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod validator-8567d4b66-q8584 requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod wavefront-collector-6b7bf647bb-zvh8w requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod wavefront-proxy-54bfcd9d6b-4lb72 requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.866: INFO: Pod sonobuoy requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod sonobuoy-e2e-job-fe2325511c6f494a requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.866: INFO: Pod sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-8dzqr requesting resource cpu=0m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.866: INFO: Pod sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn requesting resource cpu=0m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
May  7 11:32:58.867: INFO: Pod sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-qtqsp requesting resource cpu=0m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
STEP: Starting Pods to consume most of the cluster CPU.
May  7 11:32:58.867: INFO: Creating a pod which consumes cpu=2373m on Node 5615889a-d356-43b5-a818-02fb040c6965
May  7 11:32:58.878: INFO: Creating a pod which consumes cpu=2373m on Node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
May  7 11:32:58.885: INFO: Creating a pod which consumes cpu=2373m on Node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb.16eccf6102e700c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7856/filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb to 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb.16eccf619543a608], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb.16eccf61997f6f02], Reason = [Created], Message = [Created container filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb.16eccf61a4e514cb], Reason = [Started], Message = [Started container filler-pod-3bec4a76-081b-45e5-b03f-61b46803c9fb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1.16eccf6103a62be4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7856/filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1 to 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1.16eccf61a8b118d0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1.16eccf61ac4d9cb9], Reason = [Created], Message = [Created container filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1.16eccf61b7cb230a], Reason = [Started], Message = [Started container filler-pod-4752fb8e-4e1e-428c-b8fa-491243c4a6c1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f.16eccf61028d4474], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7856/filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f to 5615889a-d356-43b5-a818-02fb040c6965]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f.16eccf619f483c08], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f.16eccf61a2bcb459], Reason = [Created], Message = [Created container filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f.16eccf61aba1bdf1], Reason = [Started], Message = [Started container filler-pod-a0c408fd-7b55-4991-93e9-1f9d7c07461f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16eccf61f5150e58], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 5615889a-d356-43b5-a818-02fb040c6965
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:33:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7856" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.473 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":267,"skipped":5196,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:33:04.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b62078b8-39fa-4b6f-bf7b-56bc7dd98838
STEP: Creating a pod to test consume configMaps
May  7 11:33:04.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f" in namespace "projected-9643" to be "Succeeded or Failed"
May  7 11:33:04.178: INFO: Pod "pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362902ms
May  7 11:33:06.189: INFO: Pod "pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014971325s
May  7 11:33:08.202: INFO: Pod "pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027778281s
STEP: Saw pod success
May  7 11:33:08.202: INFO: Pod "pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f" satisfied condition "Succeeded or Failed"
May  7 11:33:08.206: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f container agnhost-container: <nil>
STEP: delete the pod
May  7 11:33:08.241: INFO: Waiting for pod pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f to disappear
May  7 11:33:08.250: INFO: Pod pod-projected-configmaps-098747c8-a30e-4778-85c3-15bad07d532f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:33:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9643" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5199,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:33:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
May  7 11:33:08.366: INFO: created test-event-1
May  7 11:33:08.371: INFO: created test-event-2
May  7 11:33:08.378: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May  7 11:33:08.388: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May  7 11:33:08.410: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:33:08.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5085" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":269,"skipped":5212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:33:08.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6016
STEP: creating service affinity-clusterip-transition in namespace services-6016
STEP: creating replication controller affinity-clusterip-transition in namespace services-6016
I0507 11:33:08.529870      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6016, replica count: 3
I0507 11:33:11.585300      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:33:14.585709      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:33:14.596: INFO: Creating new exec pod
May  7 11:33:19.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May  7 11:33:19.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  7 11:33:19.863: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:33:19.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.78.46 80'
May  7 11:33:20.038: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.78.46 80\nConnection to 10.150.78.46 80 port [tcp/http] succeeded!\n"
May  7 11:33:20.038: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:33:20.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.78.46:80/ ; done'
May  7 11:33:20.425: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n"
May  7 11:33:20.425: INFO: stdout: "\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v"
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:20.425: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.78.46:80/ ; done'
May  7 11:33:50.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n"
May  7 11:33:50.739: INFO: stdout: "\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v"
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:50.739: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:50.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.78.46:80/ ; done'
May  7 11:33:51.037: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n"
May  7 11:33:51.037: INFO: stdout: "\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-jkfqb"
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:33:51.037: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:34:21.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.78.46:80/ ; done'
May  7 11:34:21.302: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n"
May  7 11:34:21.302: INFO: stdout: "\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-gbz7b\naffinity-clusterip-transition-jkfqb\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v"
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-gbz7b
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-jkfqb
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:21.302: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-6016 exec execpod-affinitybjn5p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.78.46:80/ ; done'
May  7 11:34:51.338: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.78.46:80/\n"
May  7 11:34:51.338: INFO: stdout: "\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v\naffinity-clusterip-transition-q7c8v"
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Received response from host: affinity-clusterip-transition-q7c8v
May  7 11:34:51.339: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6016, will wait for the garbage collector to delete the pods
May  7 11:34:51.461: INFO: Deleting ReplicationController affinity-clusterip-transition took: 24.729985ms
May  7 11:34:51.562: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.689823ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:34:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6016" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:105.564 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":270,"skipped":5240,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:34:54.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May  7 11:34:54.126: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 11:35:54.180: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
May  7 11:35:54.213: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  7 11:35:54.238: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  7 11:35:54.278: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  7 11:35:54.301: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  7 11:35:54.329: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  7 11:35:54.338: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:36:10.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-524" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.626 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":271,"skipped":5258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:36:10.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6496
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
May  7 11:36:10.736: INFO: Found 0 stateful pods, waiting for 3
May  7 11:36:20.744: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:36:20.744: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:36:20.744: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May  7 11:36:30.747: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:36:30.747: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:36:30.747: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  7 11:36:30.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-6496 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:36:30.967: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:36:30.967: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:36:30.967: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May  7 11:36:41.021: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May  7 11:36:51.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-6496 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:36:51.252: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:36:51.252: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:36:51.252: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:37:01.293: INFO: Waiting for StatefulSet statefulset-6496/ss2 to complete update
May  7 11:37:01.293: INFO: Waiting for Pod statefulset-6496/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Rolling back to a previous revision
May  7 11:37:11.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-6496 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  7 11:37:11.518: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  7 11:37:11.518: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  7 11:37:11.518: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  7 11:37:21.570: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May  7 11:37:31.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=statefulset-6496 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  7 11:37:31.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  7 11:37:31.807: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  7 11:37:31.807: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  7 11:37:41.847: INFO: Waiting for StatefulSet statefulset-6496/ss2 to complete update
May  7 11:37:41.848: INFO: Waiting for Pod statefulset-6496/ss2-0 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 11:37:51.868: INFO: Deleting all statefulset in ns statefulset-6496
May  7 11:37:51.874: INFO: Scaling statefulset ss2 to 0
May  7 11:38:01.913: INFO: Waiting for statefulset status.replicas updated to 0
May  7 11:38:01.917: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:01.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6496" for this suite.

• [SLOW TEST:111.313 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":272,"skipped":5298,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:01.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May  7 11:38:22.231: INFO: EndpointSlice for Service endpointslice-1836/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:32.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1836" for this suite.

• [SLOW TEST:30.322 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":273,"skipped":5302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:32.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-1a86b7d8-06e7-452a-b10b-d66e28df4a6a
STEP: Creating secret with name secret-projected-all-test-volume-ee97b349-b4b3-4846-9ab2-b1edf3d709e0
STEP: Creating a pod to test Check all projections for projected volume plugin
May  7 11:38:32.368: INFO: Waiting up to 5m0s for pod "projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d" in namespace "projected-5110" to be "Succeeded or Failed"
May  7 11:38:32.380: INFO: Pod "projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.814435ms
May  7 11:38:34.389: INFO: Pod "projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021159162s
May  7 11:38:36.396: INFO: Pod "projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0279828s
STEP: Saw pod success
May  7 11:38:36.396: INFO: Pod "projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d" satisfied condition "Succeeded or Failed"
May  7 11:38:36.401: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d container projected-all-volume-test: <nil>
STEP: delete the pod
May  7 11:38:36.434: INFO: Waiting for pod projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d to disappear
May  7 11:38:36.439: INFO: Pod projected-volume-4b1c5811-de6d-4c5e-8b88-77292c92d43d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:36.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5110" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":274,"skipped":5380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:36.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-843d6d4c-dc38-4818-a0e6-2ee61ead0f56
STEP: Creating a pod to test consume secrets
May  7 11:38:36.524: INFO: Waiting up to 5m0s for pod "pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e" in namespace "secrets-3103" to be "Succeeded or Failed"
May  7 11:38:36.529: INFO: Pod "pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723583ms
May  7 11:38:38.538: INFO: Pod "pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013736373s
May  7 11:38:40.548: INFO: Pod "pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023977068s
STEP: Saw pod success
May  7 11:38:40.548: INFO: Pod "pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e" satisfied condition "Succeeded or Failed"
May  7 11:38:40.551: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e container secret-env-test: <nil>
STEP: delete the pod
May  7 11:38:40.571: INFO: Waiting for pod pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e to disappear
May  7 11:38:40.574: INFO: Pod pod-secrets-8dce4cc8-78a6-4f59-9794-d253640f734e no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:40.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3103" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:40.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-f8b3bb34-4dfd-4fec-af12-fff97fbff663
STEP: Creating a pod to test consume configMaps
May  7 11:38:40.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696" in namespace "projected-8707" to be "Succeeded or Failed"
May  7 11:38:40.661: INFO: Pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846145ms
May  7 11:38:42.675: INFO: Pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017663043s
May  7 11:38:44.683: INFO: Pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696": Phase="Running", Reason="", readiness=true. Elapsed: 4.025451629s
May  7 11:38:46.690: INFO: Pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032031317s
STEP: Saw pod success
May  7 11:38:46.690: INFO: Pod "pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696" satisfied condition "Succeeded or Failed"
May  7 11:38:46.694: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696 container agnhost-container: <nil>
STEP: delete the pod
May  7 11:38:46.724: INFO: Waiting for pod pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696 to disappear
May  7 11:38:46.734: INFO: Pod pod-projected-configmaps-057c2f96-117d-45db-90ce-04346ec57696 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:46.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8707" for this suite.

• [SLOW TEST:6.157 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5496,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:46.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:38:46.815: INFO: Creating deployment "webserver-deployment"
May  7 11:38:46.824: INFO: Waiting for observed generation 1
May  7 11:38:48.846: INFO: Waiting for all required pods to come up
May  7 11:38:48.861: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May  7 11:38:52.893: INFO: Waiting for deployment "webserver-deployment" to complete
May  7 11:38:52.905: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  7 11:38:52.916: INFO: Updating deployment webserver-deployment
May  7 11:38:52.916: INFO: Waiting for observed generation 2
May  7 11:38:54.926: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  7 11:38:54.931: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  7 11:38:54.935: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  7 11:38:54.946: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  7 11:38:54.946: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  7 11:38:54.948: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  7 11:38:54.956: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  7 11:38:54.956: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  7 11:38:54.967: INFO: Updating deployment webserver-deployment
May  7 11:38:54.967: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  7 11:38:54.992: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  7 11:38:57.020: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:38:57.031: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5593  986d06ab-71b3-4bd6-b8b1-23863953a649 30472 3 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a8a048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-07 11:38:54 +0000 UTC,LastTransitionTime:2022-05-07 11:38:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-05-07 11:38:55 +0000 UTC,LastTransitionTime:2022-05-07 11:38:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  7 11:38:57.036: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-5593  ec168af9-ed38-4464-9cb8-c1209788ad8a 30467 3 2022-05-07 11:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 986d06ab-71b3-4bd6-b8b1-23863953a649 0xc002a8a467 0xc002a8a468}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986d06ab-71b3-4bd6-b8b1-23863953a649\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a8a508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:38:57.036: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  7 11:38:57.036: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-5593  bb3a1af7-2808-4d61-9510-8103816d2f18 30449 3 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 986d06ab-71b3-4bd6-b8b1-23863953a649 0xc002a8a567 0xc002a8a568}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"986d06ab-71b3-4bd6-b8b1-23863953a649\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a8a5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  7 11:38:57.060: INFO: Pod "webserver-deployment-566f96c878-b26pv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-b26pv webserver-deployment-566f96c878- deployment-5593  1fb5c92f-74ee-47ac-98a7-da71ad5a2db8 30464 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0e6d7 0xc004e0e6d8}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4pdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4pdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.060: INFO: Pod "webserver-deployment-566f96c878-dgzf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-dgzf5 webserver-deployment-566f96c878- deployment-5593  9b6bc0d4-2ee8-4fb0-81a9-b3166adc0c95 30364 0 2022-05-07 11:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0e840 0xc004e0e841}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4bnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4bnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.061: INFO: Pod "webserver-deployment-566f96c878-djzxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-djzxq webserver-deployment-566f96c878- deployment-5593  b6e10478-08d9-4a6f-b41e-b5920776dc71 30376 0 2022-05-07 11:38:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0ea20 0xc004e0ea21}] []  [{Go-http-client Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtl4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtl4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.061: INFO: Pod "webserver-deployment-566f96c878-fzxw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-fzxw2 webserver-deployment-566f96c878- deployment-5593  f3360663-3376-462b-a563-9ffa04b0adcf 30505 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0ebf0 0xc004e0ebf1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cd58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cd58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.062: INFO: Pod "webserver-deployment-566f96c878-jwrpb" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-jwrpb webserver-deployment-566f96c878- deployment-5593  b881c44b-6207-4d8c-8592-a4e87d64d555 30497 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0edf0 0xc004e0edf1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp7lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp7lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.062: INFO: Pod "webserver-deployment-566f96c878-kq6gv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-kq6gv webserver-deployment-566f96c878- deployment-5593  c2e42945-39af-41ef-a4fc-e273b12fe3d6 30480 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0efd0 0xc004e0efd1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5xdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5xdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.063: INFO: Pod "webserver-deployment-566f96c878-phcb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-phcb8 webserver-deployment-566f96c878- deployment-5593  f3640ae5-6411-4a6d-a992-c4a502137283 30452 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0f1b0 0xc004e0f1b1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgdmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgdmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.063: INFO: Pod "webserver-deployment-566f96c878-qr66g" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-qr66g webserver-deployment-566f96c878- deployment-5593  c947334f-1595-4b9d-ad88-96effb37ad45 30346 0 2022-05-07 11:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0f3d0 0xc004e0f3d1}] []  [{Go-http-client Update v1 2022-05-07 11:38:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqbjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqbjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.064: INFO: Pod "webserver-deployment-566f96c878-rwsws" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-rwsws webserver-deployment-566f96c878- deployment-5593  06af86d4-98b6-4ba8-a7a0-6f8b161cb22c 30380 0 2022-05-07 11:38:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0f5a0 0xc004e0f5a1}] []  [{Go-http-client Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn47v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn47v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.064: INFO: Pod "webserver-deployment-566f96c878-svr2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-svr2m webserver-deployment-566f96c878- deployment-5593  b0a598f9-5f5f-43fe-9611-45ee4ef994d6 30457 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0f770 0xc004e0f771}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkn59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.065: INFO: Pod "webserver-deployment-566f96c878-wr2f4" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wr2f4 webserver-deployment-566f96c878- deployment-5593  12a0f952-73a6-4e03-b00b-a692bde175c6 30361 0 2022-05-07 11:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0f940 0xc004e0f941}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rzpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rzpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.065: INFO: Pod "webserver-deployment-566f96c878-zlx7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-zlx7w webserver-deployment-566f96c878- deployment-5593  ab5a3b9e-6411-4aa9-a837-81f236e90531 30431 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0fb20 0xc004e0fb21}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtbh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtbh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.065: INFO: Pod "webserver-deployment-566f96c878-zwhz7" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-zwhz7 webserver-deployment-566f96c878- deployment-5593  0dcc4a7c-13fa-4087-b96b-003b6ae139b9 30439 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ec168af9-ed38-4464-9cb8-c1209788ad8a 0xc004e0fcf0 0xc004e0fcf1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec168af9-ed38-4464-9cb8-c1209788ad8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4pxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4pxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.066: INFO: Pod "webserver-deployment-5d9fdcc779-4tlc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4tlc5 webserver-deployment-5d9fdcc779- deployment-5593  bb9197d5-dafe-47f9-93e4-827ae696678f 30400 0 2022-05-07 11:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc004e0fe50 0xc004e0fe51}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvk2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvk2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.066: INFO: Pod "webserver-deployment-5d9fdcc779-5dgb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5dgb8 webserver-deployment-5d9fdcc779- deployment-5593  ad726680-264a-45ea-a0e0-594f8666ab73 30481 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383c010 0xc00383c011}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx7wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx7wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.067: INFO: Pod "webserver-deployment-5d9fdcc779-5jlgf" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5jlgf webserver-deployment-5d9fdcc779- deployment-5593  34e6ec0e-f27a-44f3-894f-2bcdb93a6718 30268 0 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383c470 0xc00383c471}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lx9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lx9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:40.0.7.9,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7c68714148256d2ea68984a9e8ebe890ed35ccd07bc984c35a77e891d94948db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.067: INFO: Pod "webserver-deployment-5d9fdcc779-8dlw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-8dlw2 webserver-deployment-5d9fdcc779- deployment-5593  439a5c6f-66f6-401e-909e-a9400b9d26eb 30494 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383c8e0 0xc00383c8e1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zprjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zprjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.068: INFO: Pod "webserver-deployment-5d9fdcc779-c5hfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-c5hfq webserver-deployment-5d9fdcc779- deployment-5593  8598cc72-2e28-421b-b811-9360c85727da 30475 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383cc50 0xc00383cc51}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjcmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjcmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.068: INFO: Pod "webserver-deployment-5d9fdcc779-cjws7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-cjws7 webserver-deployment-5d9fdcc779- deployment-5593  4999a022-4288-4b6e-a158-af7b1a98190c 30504 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383ce10 0xc00383ce11}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2q5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2q5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.068: INFO: Pod "webserver-deployment-5d9fdcc779-dpsp5" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dpsp5 webserver-deployment-5d9fdcc779- deployment-5593  d0d74897-a00c-4cfa-a00e-fdfd3120d714 30437 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383cff0 0xc00383cff1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6kgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6kgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.069: INFO: Pod "webserver-deployment-5d9fdcc779-g4z47" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-g4z47 webserver-deployment-5d9fdcc779- deployment-5593  e2b249de-f6a8-41f9-b48b-c80fc8b5072a 30270 0 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383d3c0 0xc00383d3c1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fwjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fwjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:40.0.7.3,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://486938648ae4fb447f42fbe9c8da5f9c86ffa7649f837ce85701bba2d378767c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.069: INFO: Pod "webserver-deployment-5d9fdcc779-k58zn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-k58zn webserver-deployment-5d9fdcc779- deployment-5593  902a1b14-aebf-4f85-9b4b-a6d8853c68a8 30476 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383d5d0 0xc00383d5d1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r2z8k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r2z8k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.070: INFO: Pod "webserver-deployment-5d9fdcc779-kdzfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kdzfr webserver-deployment-5d9fdcc779- deployment-5593  31f14d08-e682-4462-bd00-8833356e236f 30432 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383d7b0 0xc00383d7b1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tlnlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tlnlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.070: INFO: Pod "webserver-deployment-5d9fdcc779-khq57" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-khq57 webserver-deployment-5d9fdcc779- deployment-5593  c47a0bf0-f061-4da7-89a5-67e7693cab3b 30450 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383d990 0xc00383d991}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxqjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxqjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.071: INFO: Pod "webserver-deployment-5d9fdcc779-mp5sm" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-mp5sm webserver-deployment-5d9fdcc779- deployment-5593  33a12bf1-f935-43b2-aa84-9b115e7ba708 30302 0 2022-05-07 11:38:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383db00 0xc00383db01}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4rzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4rzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:40.0.7.8,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eec9cfd2e4d49f7e464fdda897f4ec0a4ea02304ff07a2713615ea687d46755d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.071: INFO: Pod "webserver-deployment-5d9fdcc779-p24b8" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p24b8 webserver-deployment-5d9fdcc779- deployment-5593  42b3b51d-e461-4016-8e30-0d67eb1348cc 30299 0 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383dce0 0xc00383dce1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrqb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrqb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:40.0.7.5,StartTime:2022-05-07 11:38:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5df3133980e13b230ac15eed7ed81554e3b3bdbab09c8567a649c8485cf19d15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.072: INFO: Pod "webserver-deployment-5d9fdcc779-qnv6k" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qnv6k webserver-deployment-5d9fdcc779- deployment-5593  ab3006f6-53a7-4098-a031-04ce78ca8b36 30293 0 2022-05-07 11:38:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc00383dec0 0xc00383dec1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csqks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csqks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:40.0.7.10,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6737ad4fe1e21e05e23ca43fda093e5e702dfd56edb7998d78035641d8b65b21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.072: INFO: Pod "webserver-deployment-5d9fdcc779-stffk" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-stffk webserver-deployment-5d9fdcc779- deployment-5593  06b0ee0e-87a3-47f2-9fea-9348f8530c0d 30277 0 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e0e0 0xc002f5e0e1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfs96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfs96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.7.7,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://04f42c57a96b860f8746c126429e57d8fe798ac125853a6f7858ae9a62337028,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.073: INFO: Pod "webserver-deployment-5d9fdcc779-sw7qm" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-sw7qm webserver-deployment-5d9fdcc779- deployment-5593  4902e27d-6dba-4cec-a7b3-c3ab44d28ba2 30478 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e2b0 0xc002f5e2b1}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sncql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sncql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.073: INFO: Pod "webserver-deployment-5d9fdcc779-tmq2f" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-tmq2f webserver-deployment-5d9fdcc779- deployment-5593  c8955497-dfc8-44f6-a241-3575d5d02775 30448 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e460 0xc002f5e461}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27jvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27jvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.073: INFO: Pod "webserver-deployment-5d9fdcc779-x7b6h" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-x7b6h webserver-deployment-5d9fdcc779- deployment-5593  c813bf5f-fd97-467f-bcf8-03bef5575d6a 30306 0 2022-05-07 11:38:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e610 0xc002f5e611}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjrkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjrkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.7.11,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://489aacb1b1136befb1131480e79447e36f7a6757729ee984ba7d33b97284a0a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.074: INFO: Pod "webserver-deployment-5d9fdcc779-xpc7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xpc7g webserver-deployment-5d9fdcc779- deployment-5593  e4b26a61-47b2-4034-9886-eedb99ca1f0a 30486 0 2022-05-07 11:38:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e800 0xc002f5e801}] []  [{Go-http-client Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5lth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5lth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.4,PodIP:,StartTime:2022-05-07 11:38:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  7 11:38:57.074: INFO: Pod "webserver-deployment-5d9fdcc779-z5gjr" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-z5gjr webserver-deployment-5d9fdcc779- deployment-5593  cd59e18f-ef10-4e94-82db-bfaba38739fd 30296 0 2022-05-07 11:38:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 bb3a1af7-2808-4d61-9510-8103816d2f18 0xc002f5e9b0 0xc002f5e9b1}] []  [{kube-controller-manager Update v1 2022-05-07 11:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb3a1af7-2808-4d61-9510-8103816d2f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.7.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hvrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hvrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:38:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.3,PodIP:40.0.7.4,StartTime:2022-05-07 11:38:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:38:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8a9277a4a780971daaaee548b49121bbfa193a0b38295e220d9fffee4679c0a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.7.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:38:57.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5593" for this suite.

• [SLOW TEST:10.339 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":277,"skipped":5498,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:38:57.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May  7 11:38:57.189: INFO: Pod name sample-pod: Found 0 pods out of 1
May  7 11:39:02.200: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:39:04.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3038" for this suite.

• [SLOW TEST:7.408 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":278,"skipped":5504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:39:04.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May  7 11:39:04.585: INFO: Waiting up to 1m0s for all nodes to be ready
May  7 11:40:04.652: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
May  7 11:40:04.691: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  7 11:40:04.705: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  7 11:40:04.735: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  7 11:40:04.745: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  7 11:40:04.820: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  7 11:40:04.831: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:40:16.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6100" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.523 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":279,"skipped":5536,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:40:17.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:40:17.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9046
I0507 11:40:17.090166      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9046, replica count: 1
I0507 11:40:18.143489      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:40:19.145423      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:40:20.145801      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:40:21.146482      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:40:21.266: INFO: Created: latency-svc-gwgfm
May  7 11:40:21.280: INFO: Got endpoints: latency-svc-gwgfm [33.420487ms]
May  7 11:40:21.303: INFO: Created: latency-svc-wl544
May  7 11:40:21.325: INFO: Got endpoints: latency-svc-wl544 [43.7777ms]
May  7 11:40:21.325: INFO: Created: latency-svc-d58qr
May  7 11:40:21.340: INFO: Got endpoints: latency-svc-d58qr [58.619992ms]
May  7 11:40:21.351: INFO: Created: latency-svc-7sqns
May  7 11:40:21.359: INFO: Got endpoints: latency-svc-7sqns [76.226301ms]
May  7 11:40:21.378: INFO: Created: latency-svc-4qfg7
May  7 11:40:21.400: INFO: Created: latency-svc-m6b2k
May  7 11:40:21.400: INFO: Got endpoints: latency-svc-4qfg7 [117.166748ms]
May  7 11:40:21.416: INFO: Got endpoints: latency-svc-m6b2k [132.631597ms]
May  7 11:40:21.424: INFO: Created: latency-svc-ndnb2
May  7 11:40:21.434: INFO: Got endpoints: latency-svc-ndnb2 [150.569043ms]
May  7 11:40:21.440: INFO: Created: latency-svc-cvnqv
May  7 11:40:21.457: INFO: Got endpoints: latency-svc-cvnqv [173.899784ms]
May  7 11:40:21.458: INFO: Created: latency-svc-rf5h7
May  7 11:40:21.486: INFO: Got endpoints: latency-svc-rf5h7 [201.055724ms]
May  7 11:40:21.498: INFO: Created: latency-svc-fm68l
May  7 11:40:21.516: INFO: Got endpoints: latency-svc-fm68l [232.41613ms]
May  7 11:40:21.655: INFO: Created: latency-svc-wkrl6
May  7 11:40:21.656: INFO: Created: latency-svc-mhkzr
May  7 11:40:21.658: INFO: Created: latency-svc-qwzx7
May  7 11:40:21.659: INFO: Created: latency-svc-g2646
May  7 11:40:21.658: INFO: Created: latency-svc-2w482
May  7 11:40:21.659: INFO: Created: latency-svc-z4v8z
May  7 11:40:21.659: INFO: Created: latency-svc-b4rvz
May  7 11:40:21.659: INFO: Created: latency-svc-m7bjq
May  7 11:40:21.659: INFO: Created: latency-svc-clvkg
May  7 11:40:21.682: INFO: Created: latency-svc-67wn6
May  7 11:40:21.682: INFO: Created: latency-svc-bzdm5
May  7 11:40:21.682: INFO: Created: latency-svc-jx7l2
May  7 11:40:21.682: INFO: Created: latency-svc-qnghf
May  7 11:40:21.682: INFO: Created: latency-svc-9m47k
May  7 11:40:21.683: INFO: Created: latency-svc-wcmt2
May  7 11:40:21.736: INFO: Got endpoints: latency-svc-clvkg [452.045604ms]
May  7 11:40:21.737: INFO: Got endpoints: latency-svc-qwzx7 [219.767189ms]
May  7 11:40:21.737: INFO: Got endpoints: latency-svc-2w482 [452.679618ms]
May  7 11:40:21.746: INFO: Got endpoints: latency-svc-b4rvz [462.267623ms]
May  7 11:40:21.770: INFO: Got endpoints: latency-svc-g2646 [485.562612ms]
May  7 11:40:21.771: INFO: Got endpoints: latency-svc-z4v8z [486.193664ms]
May  7 11:40:21.774: INFO: Got endpoints: latency-svc-m7bjq [489.443468ms]
May  7 11:40:21.780: INFO: Got endpoints: latency-svc-wkrl6 [455.303638ms]
May  7 11:40:21.801: INFO: Got endpoints: latency-svc-mhkzr [460.600861ms]
May  7 11:40:21.819: INFO: Got endpoints: latency-svc-bzdm5 [403.094216ms]
May  7 11:40:21.819: INFO: Got endpoints: latency-svc-wcmt2 [459.884802ms]
May  7 11:40:21.820: INFO: Got endpoints: latency-svc-jx7l2 [385.510821ms]
May  7 11:40:21.820: INFO: Got endpoints: latency-svc-9m47k [419.78901ms]
May  7 11:40:21.845: INFO: Created: latency-svc-7zhnx
May  7 11:40:21.846: INFO: Got endpoints: latency-svc-67wn6 [359.639398ms]
May  7 11:40:21.846: INFO: Got endpoints: latency-svc-qnghf [387.307472ms]
May  7 11:40:21.855: INFO: Created: latency-svc-mbtvf
May  7 11:40:21.870: INFO: Got endpoints: latency-svc-mbtvf [124.298343ms]
May  7 11:40:21.871: INFO: Got endpoints: latency-svc-7zhnx [134.700435ms]
May  7 11:40:21.879: INFO: Created: latency-svc-mc2cf
May  7 11:40:21.898: INFO: Got endpoints: latency-svc-mc2cf [160.755888ms]
May  7 11:40:21.919: INFO: Created: latency-svc-xfhjb
May  7 11:40:21.937: INFO: Got endpoints: latency-svc-xfhjb [200.543432ms]
May  7 11:40:21.944: INFO: Created: latency-svc-9w54z
May  7 11:40:21.961: INFO: Got endpoints: latency-svc-9w54z [190.99846ms]
May  7 11:40:21.980: INFO: Created: latency-svc-k9wbx
May  7 11:40:21.983: INFO: Got endpoints: latency-svc-k9wbx [212.667787ms]
May  7 11:40:22.020: INFO: Created: latency-svc-vp5bw
May  7 11:40:22.059: INFO: Created: latency-svc-9dcr5
May  7 11:40:22.065: INFO: Got endpoints: latency-svc-vp5bw [291.331093ms]
May  7 11:40:22.096: INFO: Got endpoints: latency-svc-9dcr5 [315.502983ms]
May  7 11:40:22.110: INFO: Created: latency-svc-89gtf
May  7 11:40:22.121: INFO: Created: latency-svc-b5ztg
May  7 11:40:22.126: INFO: Got endpoints: latency-svc-89gtf [325.127104ms]
May  7 11:40:22.139: INFO: Got endpoints: latency-svc-b5ztg [319.591765ms]
May  7 11:40:22.531: INFO: Created: latency-svc-cjksr
May  7 11:40:22.531: INFO: Created: latency-svc-f5pwp
May  7 11:40:22.543: INFO: Created: latency-svc-7lmdt
May  7 11:40:22.543: INFO: Created: latency-svc-bv48n
May  7 11:40:22.543: INFO: Created: latency-svc-dm66w
May  7 11:40:22.543: INFO: Created: latency-svc-jjk56
May  7 11:40:22.543: INFO: Created: latency-svc-rv7hc
May  7 11:40:22.543: INFO: Created: latency-svc-2bzqb
May  7 11:40:22.565: INFO: Created: latency-svc-ltzdj
May  7 11:40:22.565: INFO: Created: latency-svc-tlrfv
May  7 11:40:22.565: INFO: Created: latency-svc-98fkc
May  7 11:40:22.565: INFO: Created: latency-svc-g8s22
May  7 11:40:22.565: INFO: Created: latency-svc-xhw77
May  7 11:40:22.565: INFO: Created: latency-svc-fsmkq
May  7 11:40:22.565: INFO: Created: latency-svc-sjwt2
May  7 11:40:22.629: INFO: Got endpoints: latency-svc-rv7hc [490.233571ms]
May  7 11:40:22.629: INFO: Got endpoints: latency-svc-f5pwp [809.88718ms]
May  7 11:40:22.630: INFO: Got endpoints: latency-svc-7lmdt [783.474761ms]
May  7 11:40:22.635: INFO: Got endpoints: latency-svc-bv48n [815.278283ms]
May  7 11:40:22.636: INFO: Got endpoints: latency-svc-2bzqb [815.901187ms]
May  7 11:40:22.681: INFO: Got endpoints: latency-svc-cjksr [811.021146ms]
May  7 11:40:22.682: INFO: Got endpoints: latency-svc-jjk56 [835.976528ms]
May  7 11:40:22.682: INFO: Got endpoints: latency-svc-g8s22 [784.504757ms]
May  7 11:40:22.687: INFO: Got endpoints: latency-svc-98fkc [815.870283ms]
May  7 11:40:22.688: INFO: Got endpoints: latency-svc-dm66w [621.266656ms]
May  7 11:40:22.718: INFO: Got endpoints: latency-svc-tlrfv [781.259474ms]
May  7 11:40:22.724: INFO: Created: latency-svc-hr9df
May  7 11:40:22.739: INFO: Got endpoints: latency-svc-ltzdj [643.038997ms]
May  7 11:40:22.739: INFO: Got endpoints: latency-svc-xhw77 [613.014989ms]
May  7 11:40:22.739: INFO: Got endpoints: latency-svc-fsmkq [755.89569ms]
May  7 11:40:22.740: INFO: Got endpoints: latency-svc-sjwt2 [778.655525ms]
May  7 11:40:22.756: INFO: Got endpoints: latency-svc-hr9df [120.101026ms]
May  7 11:40:22.782: INFO: Created: latency-svc-7hg9c
May  7 11:40:22.806: INFO: Created: latency-svc-4qg4z
May  7 11:40:22.813: INFO: Got endpoints: latency-svc-4qg4z [183.53332ms]
May  7 11:40:22.816: INFO: Got endpoints: latency-svc-7hg9c [186.988526ms]
May  7 11:40:22.828: INFO: Created: latency-svc-8grp2
May  7 11:40:22.839: INFO: Got endpoints: latency-svc-8grp2 [208.884757ms]
May  7 11:40:22.847: INFO: Created: latency-svc-nsbhd
May  7 11:40:22.859: INFO: Got endpoints: latency-svc-nsbhd [223.458931ms]
May  7 11:40:22.861: INFO: Created: latency-svc-gcrd2
May  7 11:40:22.886: INFO: Got endpoints: latency-svc-gcrd2 [204.784752ms]
May  7 11:40:22.898: INFO: Created: latency-svc-t5lhn
May  7 11:40:22.915: INFO: Got endpoints: latency-svc-t5lhn [232.858642ms]
May  7 11:40:22.922: INFO: Created: latency-svc-4wl2t
May  7 11:40:22.943: INFO: Created: latency-svc-mv4d8
May  7 11:40:22.951: INFO: Got endpoints: latency-svc-4wl2t [268.962352ms]
May  7 11:40:22.959: INFO: Created: latency-svc-dq2mq
May  7 11:40:22.968: INFO: Got endpoints: latency-svc-mv4d8 [280.78372ms]
May  7 11:40:22.984: INFO: Created: latency-svc-r2hrv
May  7 11:40:22.992: INFO: Got endpoints: latency-svc-dq2mq [304.785348ms]
May  7 11:40:23.008: INFO: Created: latency-svc-xv499
May  7 11:40:23.011: INFO: Created: latency-svc-kcvx7
May  7 11:40:23.018: INFO: Got endpoints: latency-svc-r2hrv [299.43826ms]
May  7 11:40:23.018: INFO: Got endpoints: latency-svc-xv499 [278.878553ms]
May  7 11:40:23.029: INFO: Got endpoints: latency-svc-kcvx7 [289.922436ms]
May  7 11:40:23.036: INFO: Created: latency-svc-5mt5t
May  7 11:40:23.051: INFO: Got endpoints: latency-svc-5mt5t [311.735596ms]
May  7 11:40:23.059: INFO: Created: latency-svc-5v65c
May  7 11:40:23.073: INFO: Got endpoints: latency-svc-5v65c [333.320001ms]
May  7 11:40:23.096: INFO: Created: latency-svc-5pcxm
May  7 11:40:23.110: INFO: Created: latency-svc-zqh25
May  7 11:40:23.114: INFO: Got endpoints: latency-svc-5pcxm [358.396487ms]
May  7 11:40:23.128: INFO: Got endpoints: latency-svc-zqh25 [314.522903ms]
May  7 11:40:23.135: INFO: Created: latency-svc-wn6v2
May  7 11:40:23.159: INFO: Created: latency-svc-prx54
May  7 11:40:23.204: INFO: Got endpoints: latency-svc-wn6v2 [388.272855ms]
May  7 11:40:23.209: INFO: Created: latency-svc-jq7xw
May  7 11:40:23.239: INFO: Created: latency-svc-djx6d
May  7 11:40:23.255: INFO: Got endpoints: latency-svc-prx54 [416.252021ms]
May  7 11:40:23.269: INFO: Created: latency-svc-kfqxr
May  7 11:40:23.284: INFO: Got endpoints: latency-svc-jq7xw [424.61971ms]
May  7 11:40:23.328: INFO: Created: latency-svc-n6sw7
May  7 11:40:23.336: INFO: Got endpoints: latency-svc-djx6d [449.874666ms]
May  7 11:40:23.344: INFO: Created: latency-svc-hpg75
May  7 11:40:23.360: INFO: Created: latency-svc-976fb
May  7 11:40:23.366: INFO: Created: latency-svc-6dz6r
May  7 11:40:23.382: INFO: Created: latency-svc-gwjbw
May  7 11:40:23.394: INFO: Got endpoints: latency-svc-kfqxr [479.670966ms]
May  7 11:40:23.395: INFO: Created: latency-svc-mnzkj
May  7 11:40:23.408: INFO: Created: latency-svc-x8knz
May  7 11:40:23.439: INFO: Got endpoints: latency-svc-n6sw7 [487.721445ms]
May  7 11:40:23.439: INFO: Created: latency-svc-9f5jk
May  7 11:40:23.454: INFO: Created: latency-svc-nwxnv
May  7 11:40:23.466: INFO: Created: latency-svc-qvnb4
May  7 11:40:23.477: INFO: Got endpoints: latency-svc-hpg75 [508.39352ms]
May  7 11:40:23.483: INFO: Created: latency-svc-slrrj
May  7 11:40:23.495: INFO: Created: latency-svc-xqqnn
May  7 11:40:23.524: INFO: Created: latency-svc-gkfbw
May  7 11:40:23.536: INFO: Got endpoints: latency-svc-976fb [543.408442ms]
May  7 11:40:23.548: INFO: Created: latency-svc-pcn5l
May  7 11:40:23.549: INFO: Created: latency-svc-wjzvz
May  7 11:40:23.576: INFO: Created: latency-svc-8bhxk
May  7 11:40:23.588: INFO: Got endpoints: latency-svc-6dz6r [570.15463ms]
May  7 11:40:23.624: INFO: Created: latency-svc-8k5xx
May  7 11:40:23.649: INFO: Got endpoints: latency-svc-gwjbw [631.043539ms]
May  7 11:40:23.660: INFO: Created: latency-svc-b4lbw
May  7 11:40:23.663: INFO: Created: latency-svc-9vfbn
May  7 11:40:23.681: INFO: Got endpoints: latency-svc-mnzkj [652.261998ms]
May  7 11:40:23.684: INFO: Created: latency-svc-fwwdl
May  7 11:40:23.701: INFO: Created: latency-svc-xgv47
May  7 11:40:23.765: INFO: Got endpoints: latency-svc-x8knz [714.20001ms]
May  7 11:40:23.772: INFO: Got endpoints: latency-svc-9f5jk [698.612666ms]
May  7 11:40:23.788: INFO: Created: latency-svc-vgntm
May  7 11:40:23.809: INFO: Created: latency-svc-ggkzv
May  7 11:40:23.827: INFO: Got endpoints: latency-svc-nwxnv [713.055977ms]
May  7 11:40:23.843: INFO: Created: latency-svc-jtmvt
May  7 11:40:23.910: INFO: Got endpoints: latency-svc-qvnb4 [781.919157ms]
May  7 11:40:23.960: INFO: Got endpoints: latency-svc-slrrj [755.883854ms]
May  7 11:40:23.975: INFO: Created: latency-svc-rddb5
May  7 11:40:23.987: INFO: Got endpoints: latency-svc-xqqnn [732.102418ms]
May  7 11:40:23.988: INFO: Created: latency-svc-qhjv4
May  7 11:40:24.027: INFO: Created: latency-svc-h6t7n
May  7 11:40:24.034: INFO: Got endpoints: latency-svc-gkfbw [750.581053ms]
May  7 11:40:24.064: INFO: Created: latency-svc-mntcf
May  7 11:40:24.081: INFO: Got endpoints: latency-svc-wjzvz [744.7787ms]
May  7 11:40:24.106: INFO: Created: latency-svc-q2gtf
May  7 11:40:24.123: INFO: Got endpoints: latency-svc-pcn5l [728.19864ms]
May  7 11:40:24.142: INFO: Created: latency-svc-twx5b
May  7 11:40:24.186: INFO: Got endpoints: latency-svc-8bhxk [747.23242ms]
May  7 11:40:24.225: INFO: Created: latency-svc-vqlll
May  7 11:40:24.226: INFO: Got endpoints: latency-svc-8k5xx [749.765496ms]
May  7 11:40:24.245: INFO: Created: latency-svc-f8lmx
May  7 11:40:24.283: INFO: Got endpoints: latency-svc-b4lbw [746.633276ms]
May  7 11:40:24.319: INFO: Created: latency-svc-gkf7h
May  7 11:40:24.358: INFO: Got endpoints: latency-svc-9vfbn [769.676287ms]
May  7 11:40:24.374: INFO: Got endpoints: latency-svc-fwwdl [724.346937ms]
May  7 11:40:24.380: INFO: Created: latency-svc-cf9wr
May  7 11:40:24.404: INFO: Created: latency-svc-plzbf
May  7 11:40:24.444: INFO: Got endpoints: latency-svc-xgv47 [762.079495ms]
May  7 11:40:24.463: INFO: Created: latency-svc-tgp99
May  7 11:40:24.480: INFO: Got endpoints: latency-svc-vgntm [715.130715ms]
May  7 11:40:24.519: INFO: Created: latency-svc-ww2j2
May  7 11:40:24.539: INFO: Got endpoints: latency-svc-ggkzv [767.766435ms]
May  7 11:40:24.561: INFO: Created: latency-svc-nr7h4
May  7 11:40:24.610: INFO: Got endpoints: latency-svc-jtmvt [782.966317ms]
May  7 11:40:24.632: INFO: Created: latency-svc-kzzs7
May  7 11:40:24.634: INFO: Got endpoints: latency-svc-rddb5 [724.471859ms]
May  7 11:40:24.665: INFO: Created: latency-svc-9cl7r
May  7 11:40:24.680: INFO: Got endpoints: latency-svc-qhjv4 [719.75586ms]
May  7 11:40:24.691: INFO: Created: latency-svc-rfcgq
May  7 11:40:24.734: INFO: Got endpoints: latency-svc-h6t7n [746.543464ms]
May  7 11:40:24.764: INFO: Created: latency-svc-6pzpp
May  7 11:40:24.776: INFO: Got endpoints: latency-svc-mntcf [741.563437ms]
May  7 11:40:24.790: INFO: Created: latency-svc-csn88
May  7 11:40:24.829: INFO: Got endpoints: latency-svc-q2gtf [748.094286ms]
May  7 11:40:24.866: INFO: Created: latency-svc-pckdv
May  7 11:40:24.882: INFO: Got endpoints: latency-svc-twx5b [758.90273ms]
May  7 11:40:24.904: INFO: Created: latency-svc-8nzgv
May  7 11:40:24.936: INFO: Got endpoints: latency-svc-vqlll [749.351038ms]
May  7 11:40:24.958: INFO: Created: latency-svc-f7cg5
May  7 11:40:24.975: INFO: Got endpoints: latency-svc-f8lmx [748.520371ms]
May  7 11:40:25.019: INFO: Created: latency-svc-9ckk9
May  7 11:40:25.036: INFO: Got endpoints: latency-svc-gkf7h [752.970204ms]
May  7 11:40:25.052: INFO: Created: latency-svc-856q7
May  7 11:40:25.076: INFO: Got endpoints: latency-svc-cf9wr [718.144188ms]
May  7 11:40:25.095: INFO: Created: latency-svc-dczkc
May  7 11:40:25.126: INFO: Got endpoints: latency-svc-plzbf [752.373963ms]
May  7 11:40:25.138: INFO: Created: latency-svc-zn9vl
May  7 11:40:25.178: INFO: Got endpoints: latency-svc-tgp99 [734.711029ms]
May  7 11:40:25.192: INFO: Created: latency-svc-k8xgp
May  7 11:40:25.227: INFO: Got endpoints: latency-svc-ww2j2 [746.097329ms]
May  7 11:40:25.252: INFO: Created: latency-svc-fmjm8
May  7 11:40:25.326: INFO: Got endpoints: latency-svc-nr7h4 [786.409835ms]
May  7 11:40:25.350: INFO: Got endpoints: latency-svc-kzzs7 [739.55139ms]
May  7 11:40:25.379: INFO: Created: latency-svc-lcxqj
May  7 11:40:25.402: INFO: Got endpoints: latency-svc-9cl7r [767.683074ms]
May  7 11:40:25.416: INFO: Created: latency-svc-h9p9f
May  7 11:40:25.436: INFO: Got endpoints: latency-svc-rfcgq [756.295631ms]
May  7 11:40:25.439: INFO: Created: latency-svc-cfwln
May  7 11:40:25.454: INFO: Created: latency-svc-js5jr
May  7 11:40:25.490: INFO: Got endpoints: latency-svc-6pzpp [755.730357ms]
May  7 11:40:25.528: INFO: Got endpoints: latency-svc-csn88 [751.45201ms]
May  7 11:40:25.531: INFO: Created: latency-svc-zdsnw
May  7 11:40:25.559: INFO: Created: latency-svc-97fjr
May  7 11:40:25.572: INFO: Got endpoints: latency-svc-pckdv [742.684722ms]
May  7 11:40:25.597: INFO: Created: latency-svc-m7pxk
May  7 11:40:25.656: INFO: Got endpoints: latency-svc-8nzgv [774.092002ms]
May  7 11:40:25.670: INFO: Created: latency-svc-qjz4m
May  7 11:40:25.687: INFO: Got endpoints: latency-svc-f7cg5 [751.270976ms]
May  7 11:40:25.718: INFO: Created: latency-svc-vqgsr
May  7 11:40:25.737: INFO: Got endpoints: latency-svc-9ckk9 [761.927225ms]
May  7 11:40:25.770: INFO: Created: latency-svc-c5twp
May  7 11:40:25.793: INFO: Got endpoints: latency-svc-856q7 [756.707137ms]
May  7 11:40:25.817: INFO: Created: latency-svc-cdlhh
May  7 11:40:25.832: INFO: Got endpoints: latency-svc-dczkc [754.930491ms]
May  7 11:40:25.877: INFO: Got endpoints: latency-svc-zn9vl [750.854571ms]
May  7 11:40:25.914: INFO: Created: latency-svc-gcwg5
May  7 11:40:25.916: INFO: Created: latency-svc-7mw5t
May  7 11:40:25.928: INFO: Got endpoints: latency-svc-k8xgp [749.184364ms]
May  7 11:40:25.956: INFO: Created: latency-svc-wbqr7
May  7 11:40:25.990: INFO: Got endpoints: latency-svc-fmjm8 [763.574312ms]
May  7 11:40:26.019: INFO: Created: latency-svc-n9mlv
May  7 11:40:26.024: INFO: Got endpoints: latency-svc-lcxqj [697.485708ms]
May  7 11:40:26.047: INFO: Created: latency-svc-zkfg8
May  7 11:40:26.080: INFO: Got endpoints: latency-svc-h9p9f [730.063574ms]
May  7 11:40:26.113: INFO: Created: latency-svc-bbmtp
May  7 11:40:26.138: INFO: Got endpoints: latency-svc-cfwln [735.45022ms]
May  7 11:40:26.164: INFO: Created: latency-svc-9ccl6
May  7 11:40:26.182: INFO: Got endpoints: latency-svc-js5jr [745.828085ms]
May  7 11:40:26.205: INFO: Created: latency-svc-n7cp8
May  7 11:40:26.257: INFO: Got endpoints: latency-svc-zdsnw [766.789886ms]
May  7 11:40:26.283: INFO: Got endpoints: latency-svc-97fjr [755.746223ms]
May  7 11:40:26.300: INFO: Created: latency-svc-5wfjl
May  7 11:40:26.311: INFO: Created: latency-svc-djz6c
May  7 11:40:26.327: INFO: Got endpoints: latency-svc-m7pxk [755.055181ms]
May  7 11:40:26.345: INFO: Created: latency-svc-fwvqj
May  7 11:40:26.382: INFO: Got endpoints: latency-svc-qjz4m [726.395333ms]
May  7 11:40:26.404: INFO: Created: latency-svc-r9kt7
May  7 11:40:26.433: INFO: Got endpoints: latency-svc-vqgsr [746.399205ms]
May  7 11:40:26.460: INFO: Created: latency-svc-tlxcm
May  7 11:40:26.477: INFO: Got endpoints: latency-svc-c5twp [739.692221ms]
May  7 11:40:26.525: INFO: Created: latency-svc-5ptw2
May  7 11:40:26.527: INFO: Got endpoints: latency-svc-cdlhh [734.048891ms]
May  7 11:40:26.545: INFO: Created: latency-svc-82smf
May  7 11:40:26.574: INFO: Got endpoints: latency-svc-gcwg5 [697.044878ms]
May  7 11:40:26.604: INFO: Created: latency-svc-629gw
May  7 11:40:26.621: INFO: Got endpoints: latency-svc-7mw5t [789.019722ms]
May  7 11:40:26.648: INFO: Created: latency-svc-t92z9
May  7 11:40:26.671: INFO: Got endpoints: latency-svc-wbqr7 [743.571836ms]
May  7 11:40:26.702: INFO: Created: latency-svc-8z5fj
May  7 11:40:26.746: INFO: Got endpoints: latency-svc-n9mlv [755.993483ms]
May  7 11:40:26.755: INFO: Created: latency-svc-w2lsw
May  7 11:40:26.772: INFO: Got endpoints: latency-svc-zkfg8 [748.470674ms]
May  7 11:40:26.784: INFO: Created: latency-svc-gg47b
May  7 11:40:26.829: INFO: Got endpoints: latency-svc-bbmtp [747.517626ms]
May  7 11:40:26.841: INFO: Created: latency-svc-w2xgk
May  7 11:40:26.929: INFO: Got endpoints: latency-svc-9ccl6 [789.511036ms]
May  7 11:40:26.973: INFO: Got endpoints: latency-svc-n7cp8 [790.919095ms]
May  7 11:40:27.004: INFO: Got endpoints: latency-svc-5wfjl [746.957974ms]
May  7 11:40:27.048: INFO: Created: latency-svc-p5qjn
May  7 11:40:27.071: INFO: Got endpoints: latency-svc-djz6c [787.611374ms]
May  7 11:40:27.106: INFO: Created: latency-svc-qxcbn
May  7 11:40:27.112: INFO: Got endpoints: latency-svc-fwvqj [784.653766ms]
May  7 11:40:27.169: INFO: Created: latency-svc-t5wjt
May  7 11:40:27.181: INFO: Got endpoints: latency-svc-r9kt7 [798.827697ms]
May  7 11:40:27.204: INFO: Created: latency-svc-w6hgk
May  7 11:40:27.231: INFO: Got endpoints: latency-svc-tlxcm [797.054827ms]
May  7 11:40:27.239: INFO: Got endpoints: latency-svc-5ptw2 [760.317579ms]
May  7 11:40:27.266: INFO: Created: latency-svc-6dpnt
May  7 11:40:27.306: INFO: Created: latency-svc-6q27l
May  7 11:40:27.315: INFO: Got endpoints: latency-svc-82smf [787.598589ms]
May  7 11:40:27.329: INFO: Created: latency-svc-qnjzs
May  7 11:40:27.341: INFO: Got endpoints: latency-svc-629gw [764.034288ms]
May  7 11:40:27.359: INFO: Created: latency-svc-cjrg9
May  7 11:40:27.373: INFO: Created: latency-svc-82qjq
May  7 11:40:27.395: INFO: Got endpoints: latency-svc-t92z9 [774.22726ms]
May  7 11:40:27.402: INFO: Created: latency-svc-gpwpt
May  7 11:40:27.411: INFO: Created: latency-svc-56mgf
May  7 11:40:27.432: INFO: Got endpoints: latency-svc-8z5fj [760.979116ms]
May  7 11:40:27.447: INFO: Created: latency-svc-xpx4h
May  7 11:40:27.473: INFO: Got endpoints: latency-svc-w2lsw [726.492196ms]
May  7 11:40:27.490: INFO: Created: latency-svc-lhtlk
May  7 11:40:27.525: INFO: Got endpoints: latency-svc-gg47b [752.585894ms]
May  7 11:40:27.539: INFO: Created: latency-svc-mbjjp
May  7 11:40:28.140: INFO: Got endpoints: latency-svc-w2xgk [1.311676062s]
May  7 11:40:28.141: INFO: Got endpoints: latency-svc-p5qjn [1.212110483s]
May  7 11:40:28.141: INFO: Got endpoints: latency-svc-qxcbn [1.167734381s]
May  7 11:40:28.141: INFO: Got endpoints: latency-svc-t5wjt [1.136701191s]
May  7 11:40:28.141: INFO: Got endpoints: latency-svc-w6hgk [1.069859814s]
May  7 11:40:28.170: INFO: Got endpoints: latency-svc-6dpnt [1.058435723s]
May  7 11:40:28.225: INFO: Got endpoints: latency-svc-qnjzs [994.600099ms]
May  7 11:40:28.225: INFO: Got endpoints: latency-svc-6q27l [1.044314929s]
May  7 11:40:28.226: INFO: Got endpoints: latency-svc-cjrg9 [986.962061ms]
May  7 11:40:28.227: INFO: Got endpoints: latency-svc-82qjq [911.912334ms]
May  7 11:40:28.228: INFO: Got endpoints: latency-svc-gpwpt [886.362137ms]
May  7 11:40:28.243: INFO: Created: latency-svc-ltpn2
May  7 11:40:28.248: INFO: Got endpoints: latency-svc-lhtlk [774.694002ms]
May  7 11:40:28.248: INFO: Got endpoints: latency-svc-56mgf [853.252982ms]
May  7 11:40:28.248: INFO: Got endpoints: latency-svc-mbjjp [723.559868ms]
May  7 11:40:28.267: INFO: Created: latency-svc-4td4m
May  7 11:40:28.293: INFO: Created: latency-svc-xbglr
May  7 11:40:28.299: INFO: Got endpoints: latency-svc-xpx4h [866.129989ms]
May  7 11:40:28.326: INFO: Created: latency-svc-hjrmj
May  7 11:40:28.331: INFO: Created: latency-svc-kr5vr
May  7 11:40:28.345: INFO: Got endpoints: latency-svc-ltpn2 [203.768701ms]
May  7 11:40:28.346: INFO: Created: latency-svc-rxhhb
May  7 11:40:28.366: INFO: Created: latency-svc-rhqvd
May  7 11:40:28.397: INFO: Got endpoints: latency-svc-4td4m [256.855419ms]
May  7 11:40:28.408: INFO: Created: latency-svc-thnrm
May  7 11:40:28.409: INFO: Created: latency-svc-c5nvb
May  7 11:40:28.421: INFO: Created: latency-svc-cfmxw
May  7 11:40:28.427: INFO: Got endpoints: latency-svc-xbglr [286.023273ms]
May  7 11:40:28.450: INFO: Created: latency-svc-m9lgt
May  7 11:40:28.476: INFO: Got endpoints: latency-svc-hjrmj [334.374984ms]
May  7 11:40:28.537: INFO: Created: latency-svc-76lxp
May  7 11:40:28.546: INFO: Created: latency-svc-4m9nd
May  7 11:40:28.547: INFO: Created: latency-svc-nsqpp
May  7 11:40:28.551: INFO: Created: latency-svc-b4m8g
May  7 11:40:28.551: INFO: Created: latency-svc-ljmb6
May  7 11:40:28.551: INFO: Created: latency-svc-r5wmx
May  7 11:40:28.553: INFO: Got endpoints: latency-svc-kr5vr [411.499039ms]
May  7 11:40:28.554: INFO: Created: latency-svc-2kpzf
May  7 11:40:28.554: INFO: Created: latency-svc-fnf5z
May  7 11:40:28.577: INFO: Created: latency-svc-28wvq
May  7 11:40:28.577: INFO: Got endpoints: latency-svc-rxhhb [407.006658ms]
May  7 11:40:28.613: INFO: Created: latency-svc-rqwsx
May  7 11:40:28.625: INFO: Got endpoints: latency-svc-rhqvd [399.488264ms]
May  7 11:40:28.644: INFO: Created: latency-svc-z2hz2
May  7 11:40:28.685: INFO: Got endpoints: latency-svc-thnrm [457.309507ms]
May  7 11:40:28.699: INFO: Created: latency-svc-d2fp5
May  7 11:40:28.728: INFO: Got endpoints: latency-svc-c5nvb [502.493246ms]
May  7 11:40:28.745: INFO: Created: latency-svc-pkzd4
May  7 11:40:28.776: INFO: Got endpoints: latency-svc-cfmxw [549.18594ms]
May  7 11:40:28.808: INFO: Created: latency-svc-hxzhw
May  7 11:40:28.828: INFO: Got endpoints: latency-svc-m9lgt [601.382234ms]
May  7 11:40:28.842: INFO: Created: latency-svc-dl8v6
May  7 11:40:28.875: INFO: Got endpoints: latency-svc-76lxp [626.529941ms]
May  7 11:40:28.890: INFO: Created: latency-svc-b8gjh
May  7 11:40:28.922: INFO: Got endpoints: latency-svc-nsqpp [446.395298ms]
May  7 11:40:28.942: INFO: Created: latency-svc-s2xkp
May  7 11:40:28.978: INFO: Got endpoints: latency-svc-4m9nd [679.702628ms]
May  7 11:40:28.989: INFO: Created: latency-svc-kjpmd
May  7 11:40:29.038: INFO: Got endpoints: latency-svc-b4m8g [692.81573ms]
May  7 11:40:29.044: INFO: Created: latency-svc-5d6w9
May  7 11:40:29.109: INFO: Got endpoints: latency-svc-ljmb6 [861.376482ms]
May  7 11:40:29.161: INFO: Created: latency-svc-nrc7g
May  7 11:40:29.171: INFO: Got endpoints: latency-svc-r5wmx [773.0836ms]
May  7 11:40:29.180: INFO: Got endpoints: latency-svc-2kpzf [931.699591ms]
May  7 11:40:29.234: INFO: Got endpoints: latency-svc-fnf5z [806.657902ms]
May  7 11:40:29.280: INFO: Got endpoints: latency-svc-28wvq [727.257561ms]
May  7 11:40:29.334: INFO: Got endpoints: latency-svc-rqwsx [756.6049ms]
May  7 11:40:29.378: INFO: Got endpoints: latency-svc-z2hz2 [750.849664ms]
May  7 11:40:29.428: INFO: Got endpoints: latency-svc-d2fp5 [742.405786ms]
May  7 11:40:29.476: INFO: Got endpoints: latency-svc-pkzd4 [747.768818ms]
May  7 11:40:29.526: INFO: Got endpoints: latency-svc-hxzhw [750.027352ms]
May  7 11:40:29.575: INFO: Got endpoints: latency-svc-dl8v6 [746.748624ms]
May  7 11:40:29.630: INFO: Got endpoints: latency-svc-b8gjh [755.527428ms]
May  7 11:40:29.679: INFO: Got endpoints: latency-svc-s2xkp [756.901252ms]
May  7 11:40:29.727: INFO: Got endpoints: latency-svc-kjpmd [748.146573ms]
May  7 11:40:29.772: INFO: Got endpoints: latency-svc-5d6w9 [734.844561ms]
May  7 11:40:29.824: INFO: Got endpoints: latency-svc-nrc7g [713.931808ms]
May  7 11:40:29.824: INFO: Latencies: [43.7777ms 58.619992ms 76.226301ms 117.166748ms 120.101026ms 124.298343ms 132.631597ms 134.700435ms 150.569043ms 160.755888ms 173.899784ms 183.53332ms 186.988526ms 190.99846ms 200.543432ms 201.055724ms 203.768701ms 204.784752ms 208.884757ms 212.667787ms 219.767189ms 223.458931ms 232.41613ms 232.858642ms 256.855419ms 268.962352ms 278.878553ms 280.78372ms 286.023273ms 289.922436ms 291.331093ms 299.43826ms 304.785348ms 311.735596ms 314.522903ms 315.502983ms 319.591765ms 325.127104ms 333.320001ms 334.374984ms 358.396487ms 359.639398ms 385.510821ms 387.307472ms 388.272855ms 399.488264ms 403.094216ms 407.006658ms 411.499039ms 416.252021ms 419.78901ms 424.61971ms 446.395298ms 449.874666ms 452.045604ms 452.679618ms 455.303638ms 457.309507ms 459.884802ms 460.600861ms 462.267623ms 479.670966ms 485.562612ms 486.193664ms 487.721445ms 489.443468ms 490.233571ms 502.493246ms 508.39352ms 543.408442ms 549.18594ms 570.15463ms 601.382234ms 613.014989ms 621.266656ms 626.529941ms 631.043539ms 643.038997ms 652.261998ms 679.702628ms 692.81573ms 697.044878ms 697.485708ms 698.612666ms 713.055977ms 713.931808ms 714.20001ms 715.130715ms 718.144188ms 719.75586ms 723.559868ms 724.346937ms 724.471859ms 726.395333ms 726.492196ms 727.257561ms 728.19864ms 730.063574ms 732.102418ms 734.048891ms 734.711029ms 734.844561ms 735.45022ms 739.55139ms 739.692221ms 741.563437ms 742.405786ms 742.684722ms 743.571836ms 744.7787ms 745.828085ms 746.097329ms 746.399205ms 746.543464ms 746.633276ms 746.748624ms 746.957974ms 747.23242ms 747.517626ms 747.768818ms 748.094286ms 748.146573ms 748.470674ms 748.520371ms 749.184364ms 749.351038ms 749.765496ms 750.027352ms 750.581053ms 750.849664ms 750.854571ms 751.270976ms 751.45201ms 752.373963ms 752.585894ms 752.970204ms 754.930491ms 755.055181ms 755.527428ms 755.730357ms 755.746223ms 755.883854ms 755.89569ms 755.993483ms 756.295631ms 756.6049ms 756.707137ms 756.901252ms 758.90273ms 760.317579ms 760.979116ms 761.927225ms 762.079495ms 763.574312ms 764.034288ms 766.789886ms 767.683074ms 767.766435ms 769.676287ms 773.0836ms 774.092002ms 774.22726ms 774.694002ms 778.655525ms 781.259474ms 781.919157ms 782.966317ms 783.474761ms 784.504757ms 784.653766ms 786.409835ms 787.598589ms 787.611374ms 789.019722ms 789.511036ms 790.919095ms 797.054827ms 798.827697ms 806.657902ms 809.88718ms 811.021146ms 815.278283ms 815.870283ms 815.901187ms 835.976528ms 853.252982ms 861.376482ms 866.129989ms 886.362137ms 911.912334ms 931.699591ms 986.962061ms 994.600099ms 1.044314929s 1.058435723s 1.069859814s 1.136701191s 1.167734381s 1.212110483s 1.311676062s]
May  7 11:40:29.824: INFO: 50 %ile: 734.711029ms
May  7 11:40:29.824: INFO: 90 %ile: 811.021146ms
May  7 11:40:29.824: INFO: 99 %ile: 1.212110483s
May  7 11:40:29.824: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:40:29.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9046" for this suite.

• [SLOW TEST:12.821 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":280,"skipped":5548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:40:29.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
May  7 11:40:29.920: INFO: Waiting up to 5m0s for pod "client-containers-963a425e-e46c-421d-ac3a-24da50714703" in namespace "containers-7582" to be "Succeeded or Failed"
May  7 11:40:29.931: INFO: Pod "client-containers-963a425e-e46c-421d-ac3a-24da50714703": Phase="Pending", Reason="", readiness=false. Elapsed: 10.832469ms
May  7 11:40:31.941: INFO: Pod "client-containers-963a425e-e46c-421d-ac3a-24da50714703": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020451673s
May  7 11:40:33.947: INFO: Pod "client-containers-963a425e-e46c-421d-ac3a-24da50714703": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027097699s
STEP: Saw pod success
May  7 11:40:33.947: INFO: Pod "client-containers-963a425e-e46c-421d-ac3a-24da50714703" satisfied condition "Succeeded or Failed"
May  7 11:40:33.951: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod client-containers-963a425e-e46c-421d-ac3a-24da50714703 container agnhost-container: <nil>
STEP: delete the pod
May  7 11:40:33.986: INFO: Waiting for pod client-containers-963a425e-e46c-421d-ac3a-24da50714703 to disappear
May  7 11:40:33.993: INFO: Pod client-containers-963a425e-e46c-421d-ac3a-24da50714703 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:40:33.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7582" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:40:34.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May  7 11:40:34.062: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  7 11:40:34.071: INFO: Waiting for terminating namespaces to be deleted...
May  7 11:40:34.075: INFO: 
Logging pods the apiserver thinks is on node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 before test
May  7 11:40:34.093: INFO: coredns-787c57488d-xh45r from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.093: INFO: 	Container coredns ready: true, restart count 0
May  7 11:40:34.093: INFO: event-controller-795755d67-lkvmj from pks-system started at 2022-05-07 10:26:54 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.093: INFO: 	Container event-controller ready: true, restart count 0
May  7 11:40:34.093: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:40:34.093: INFO: fluent-bit-lgdqp from pks-system started at 2022-05-07 10:27:00 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.094: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:40:34.094: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:40:34.094: INFO: node-exporter-cggqd from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.094: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:40:34.094: INFO: observability-manager-764bb5b6d9-5pw7g from pks-system started at 2022-05-07 10:26:47 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.094: INFO: 	Container observability-manager ready: true, restart count 0
May  7 11:40:34.094: INFO: sink-controller-77c8c69d54-vmhrt from pks-system started at 2022-05-07 10:26:54 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.094: INFO: 	Container sink-controller ready: true, restart count 0
May  7 11:40:34.094: INFO: telegraf-8f7tz from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.094: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:40:34.094: INFO: telemetry-agent-6f6f75b47-qvqbh from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.095: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May  7 11:40:34.095: INFO: wavefront-collector-6b7bf647bb-zvh8w from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.095: INFO: 	Container wavefront-collector ready: true, restart count 0
May  7 11:40:34.095: INFO: sonobuoy from sonobuoy started at 2022-05-07 10:16:30 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.095: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  7 11:40:34.095: INFO: sonobuoy-e2e-job-fe2325511c6f494a from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.095: INFO: 	Container e2e ready: true, restart count 0
May  7 11:40:34.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:40:34.096: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.096: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:40:34.096: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 11:40:34.096: INFO: 
Logging pods the apiserver thinks is on node 5615889a-d356-43b5-a818-02fb040c6965 before test
May  7 11:40:34.109: INFO: coredns-787c57488d-2wsp6 from kube-system started at 2022-05-07 10:41:08 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container coredns ready: true, restart count 0
May  7 11:40:34.109: INFO: fluent-bit-7vdxl from pks-system started at 2022-05-07 10:40:58 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:40:34.109: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:40:34.109: INFO: node-exporter-2bp48 from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:40:34.109: INFO: telegraf-x4jms from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:40:34.109: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-8dzqr from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:40:34.109: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 11:40:34.109: INFO: svc-latency-rc-5cngw from svc-latency-9046 started at 2022-05-07 11:40:17 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.109: INFO: 	Container svc-latency-rc ready: true, restart count 0
May  7 11:40:34.109: INFO: 
Logging pods the apiserver thinks is on node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 before test
May  7 11:40:34.127: INFO: coredns-787c57488d-mhgg5 from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.127: INFO: 	Container coredns ready: true, restart count 0
May  7 11:40:34.127: INFO: metrics-server-66c5bff789-gl2ww from kube-system started at 2022-05-07 10:26:48 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.127: INFO: 	Container metrics-server ready: true, restart count 0
May  7 11:40:34.127: INFO: fluent-bit-pzwr7 from pks-system started at 2022-05-07 10:27:01 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.127: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 11:40:34.127: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 11:40:34.127: INFO: metric-controller-669f9bc57b-tzdqq from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.127: INFO: 	Container metric-controller ready: true, restart count 0
May  7 11:40:34.127: INFO: node-exporter-bs69m from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.127: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 11:40:34.128: INFO: telegraf-dp5wl from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.128: INFO: 	Container telegraf ready: true, restart count 0
May  7 11:40:34.128: INFO: validator-8567d4b66-q8584 from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.129: INFO: 	Container validator ready: true, restart count 0
May  7 11:40:34.129: INFO: wavefront-proxy-54bfcd9d6b-4lb72 from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 11:40:34.129: INFO: 	Container wavefront-proxy ready: true, restart count 0
May  7 11:40:34.129: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-qtqsp from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 11:40:34.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 11:40:34.129: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fea8618f-6c55-41ca-93fd-3bc9446ffe91 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 30.1.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-fea8618f-6c55-41ca-93fd-3bc9446ffe91 off the node 5615889a-d356-43b5-a818-02fb040c6965
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fea8618f-6c55-41ca-93fd-3bc9446ffe91
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:45:42.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3787" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.457 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":282,"skipped":5601,"failed":0}
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:45:42.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:45:42.570: INFO: created pod
May  7 11:45:42.571: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9608" to be "Succeeded or Failed"
May  7 11:45:42.588: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.355811ms
May  7 11:45:44.594: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022968793s
May  7 11:45:46.604: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033772958s
May  7 11:45:48.611: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040367685s
STEP: Saw pod success
May  7 11:45:48.612: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  7 11:46:18.613: INFO: polling logs
May  7 11:46:18.638: INFO: Pod logs: 
2022/05/07 11:45:46 OK: Got token
2022/05/07 11:45:46 validating with in-cluster discovery
2022/05/07 11:45:46 OK: got issuer https://master.cfcr.internal:8443
2022/05/07 11:45:46 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://master.cfcr.internal:8443", Subject:"system:serviceaccount:svcaccounts-9608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1651924542, NotBefore:1651923942, IssuedAt:1651923942, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2d5db200-8f90-43ae-8ca4-b706d10a187b"}}}
2022/05/07 11:45:46 OK: Constructed OIDC provider for issuer https://master.cfcr.internal:8443
2022/05/07 11:45:46 OK: Validated signature on JWT
2022/05/07 11:45:46 OK: Got valid claims from token!
2022/05/07 11:45:46 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://master.cfcr.internal:8443", Subject:"system:serviceaccount:svcaccounts-9608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1651924542, NotBefore:1651923942, IssuedAt:1651923942, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2d5db200-8f90-43ae-8ca4-b706d10a187b"}}}

May  7 11:46:18.638: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9608" for this suite.

• [SLOW TEST:36.187 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":283,"skipped":5601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:18.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  7 11:46:23.754: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:23.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1677" for this suite.

• [SLOW TEST:5.154 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2561a8f2-2f71-40d7-a31d-86fbf5a2334b
STEP: Creating a pod to test consume secrets
May  7 11:46:23.901: INFO: Waiting up to 5m0s for pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1" in namespace "secrets-4894" to be "Succeeded or Failed"
May  7 11:46:23.913: INFO: Pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014777ms
May  7 11:46:25.919: INFO: Pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01853366s
May  7 11:46:27.926: INFO: Pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025636293s
May  7 11:46:29.940: INFO: Pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038945349s
STEP: Saw pod success
May  7 11:46:29.940: INFO: Pod "pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1" satisfied condition "Succeeded or Failed"
May  7 11:46:29.945: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1 container secret-volume-test: <nil>
STEP: delete the pod
May  7 11:46:29.974: INFO: Waiting for pod pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1 to disappear
May  7 11:46:29.978: INFO: Pod pod-secrets-c8525a82-63df-403a-9dc5-abd433fe66b1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:29.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4894" for this suite.

• [SLOW TEST:6.178 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5691,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:46:30.034: INFO: Creating deployment "test-recreate-deployment"
May  7 11:46:30.040: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  7 11:46:30.049: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
May  7 11:46:32.059: INFO: Waiting deployment "test-recreate-deployment" to complete
May  7 11:46:32.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d659f7dc9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:46:34.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d659f7dc9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:46:36.070: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  7 11:46:36.083: INFO: Updating deployment test-recreate-deployment
May  7 11:46:36.083: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:46:36.244: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9664  2142d0c0-cb43-47db-90a2-35d3a9721a4d 33699 2 2022-05-07 11:46:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003decf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-07 11:46:36 +0000 UTC,LastTransitionTime:2022-05-07 11:46:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-05-07 11:46:36 +0000 UTC,LastTransitionTime:2022-05-07 11:46:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  7 11:46:36.261: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-9664  34a62d81-8e48-442a-a412-ce0f15a2b26f 33698 1 2022-05-07 11:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2142d0c0-cb43-47db-90a2-35d3a9721a4d 0xc003ded337 0xc003ded338}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2142d0c0-cb43-47db-90a2-35d3a9721a4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ded3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:46:36.261: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  7 11:46:36.261: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-9664  f5e8564e-b38c-47ed-9596-0b51a50ea65c 33688 2 2022-05-07 11:46:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2142d0c0-cb43-47db-90a2-35d3a9721a4d 0xc003ded447 0xc003ded448}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:46:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2142d0c0-cb43-47db-90a2-35d3a9721a4d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ded4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  7 11:46:36.271: INFO: Pod "test-recreate-deployment-5b99bd5487-9wqmv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-9wqmv test-recreate-deployment-5b99bd5487- deployment-9664  087f1f4f-9c70-42dc-818d-89aa4489efc8 33700 0 2022-05-07 11:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 34a62d81-8e48-442a-a412-ce0f15a2b26f 0xc003ded987 0xc003ded988}] []  [{Go-http-client Update v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-05-07 11:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34a62d81-8e48-442a-a412-ce0f15a2b26f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcszj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcszj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:46:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:46:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:,StartTime:2022-05-07 11:46:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:36.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9664" for this suite.

• [SLOW TEST:6.294 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":286,"skipped":5701,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:46:36.353: INFO: The status of Pod server-envvars-af96e928-2073-4666-a901-8c42cbecbf96 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:46:38.363: INFO: The status of Pod server-envvars-af96e928-2073-4666-a901-8c42cbecbf96 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:46:40.358: INFO: The status of Pod server-envvars-af96e928-2073-4666-a901-8c42cbecbf96 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:46:42.374: INFO: The status of Pod server-envvars-af96e928-2073-4666-a901-8c42cbecbf96 is Running (Ready = true)
May  7 11:46:42.436: INFO: Waiting up to 5m0s for pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29" in namespace "pods-6226" to be "Succeeded or Failed"
May  7 11:46:42.449: INFO: Pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29": Phase="Pending", Reason="", readiness=false. Elapsed: 13.470647ms
May  7 11:46:44.461: INFO: Pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025256606s
May  7 11:46:46.471: INFO: Pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034574919s
May  7 11:46:48.484: INFO: Pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048338473s
STEP: Saw pod success
May  7 11:46:48.485: INFO: Pod "client-envvars-96561829-acbc-488c-b191-90d5a9321f29" satisfied condition "Succeeded or Failed"
May  7 11:46:48.491: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod client-envvars-96561829-acbc-488c-b191-90d5a9321f29 container env3cont: <nil>
STEP: delete the pod
May  7 11:46:48.521: INFO: Waiting for pod client-envvars-96561829-acbc-488c-b191-90d5a9321f29 to disappear
May  7 11:46:48.525: INFO: Pod client-envvars-96561829-acbc-488c-b191-90d5a9321f29 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:48.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6226" for this suite.

• [SLOW TEST:12.260 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5703,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:48.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May  7 11:46:48.627: INFO: observed Pod pod-test in namespace pods-5479 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  7 11:46:48.632: INFO: observed Pod pod-test in namespace pods-5479 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC  }]
May  7 11:46:48.643: INFO: observed Pod pod-test in namespace pods-5479 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC  }]
May  7 11:46:52.702: INFO: Found Pod pod-test in namespace pods-5479 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-07 11:46:48 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May  7 11:46:52.734: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May  7 11:46:52.767: INFO: observed event type ADDED
May  7 11:46:52.768: INFO: observed event type MODIFIED
May  7 11:46:52.768: INFO: observed event type MODIFIED
May  7 11:46:52.768: INFO: observed event type MODIFIED
May  7 11:46:52.768: INFO: observed event type MODIFIED
May  7 11:46:52.769: INFO: observed event type MODIFIED
May  7 11:46:52.769: INFO: observed event type MODIFIED
May  7 11:46:54.721: INFO: observed event type MODIFIED
May  7 11:46:55.739: INFO: observed event type MODIFIED
May  7 11:46:55.748: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:46:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5479" for this suite.

• [SLOW TEST:7.228 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":288,"skipped":5714,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:46:55.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:46:56.411: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:46:58.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 11:47:00.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 46, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:47:03.488: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May  7 11:47:07.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=webhook-2834 attach --namespace=webhook-2834 to-be-attached-pod -i -c=container1'
May  7 11:47:08.752: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:47:08.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2834" for this suite.
STEP: Destroying namespace "webhook-2834-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.101 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":289,"skipped":5714,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:47:08.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
May  7 11:47:08.936: INFO: Waiting up to 5m0s for pod "var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109" in namespace "var-expansion-3718" to be "Succeeded or Failed"
May  7 11:47:08.959: INFO: Pod "var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109": Phase="Pending", Reason="", readiness=false. Elapsed: 23.031729ms
May  7 11:47:10.971: INFO: Pod "var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035351346s
May  7 11:47:12.983: INFO: Pod "var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047005385s
STEP: Saw pod success
May  7 11:47:12.984: INFO: Pod "var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109" satisfied condition "Succeeded or Failed"
May  7 11:47:12.986: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109 container dapi-container: <nil>
STEP: delete the pod
May  7 11:47:13.007: INFO: Waiting for pod var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109 to disappear
May  7 11:47:13.014: INFO: Pod var-expansion-546d1ccb-bc91-4428-9dda-6a7b5f6d7109 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:47:13.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3718" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5734,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:47:13.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:13.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3069" for this suite.

• [SLOW TEST:300.098 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":291,"skipped":5745,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
May  7 11:52:13.218: INFO: Waiting up to 5m0s for pod "client-containers-f7652465-53cd-4e7c-b17b-95d48501276b" in namespace "containers-9385" to be "Succeeded or Failed"
May  7 11:52:13.236: INFO: Pod "client-containers-f7652465-53cd-4e7c-b17b-95d48501276b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024967ms
May  7 11:52:15.243: INFO: Pod "client-containers-f7652465-53cd-4e7c-b17b-95d48501276b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025231083s
May  7 11:52:17.260: INFO: Pod "client-containers-f7652465-53cd-4e7c-b17b-95d48501276b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041893341s
STEP: Saw pod success
May  7 11:52:17.260: INFO: Pod "client-containers-f7652465-53cd-4e7c-b17b-95d48501276b" satisfied condition "Succeeded or Failed"
May  7 11:52:17.264: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod client-containers-f7652465-53cd-4e7c-b17b-95d48501276b container agnhost-container: <nil>
STEP: delete the pod
May  7 11:52:17.310: INFO: Waiting for pod client-containers-f7652465-53cd-4e7c-b17b-95d48501276b to disappear
May  7 11:52:17.317: INFO: Pod client-containers-f7652465-53cd-4e7c-b17b-95d48501276b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:17.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9385" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5765,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:17.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:52:17.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437" in namespace "projected-2950" to be "Succeeded or Failed"
May  7 11:52:17.406: INFO: Pod "downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767751ms
May  7 11:52:19.415: INFO: Pod "downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01262783s
May  7 11:52:21.426: INFO: Pod "downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022834106s
STEP: Saw pod success
May  7 11:52:21.426: INFO: Pod "downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437" satisfied condition "Succeeded or Failed"
May  7 11:52:21.431: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437 container client-container: <nil>
STEP: delete the pod
May  7 11:52:21.455: INFO: Waiting for pod downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437 to disappear
May  7 11:52:21.458: INFO: Pod downwardapi-volume-88e41efc-9a70-48ac-a4ec-02df52a2c437 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:21.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2950" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:21.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
May  7 11:52:33.146: INFO: 68 pods remaining
May  7 11:52:33.146: INFO: 68 pods has nil DeletionTimestamp
May  7 11:52:33.146: INFO: 
STEP: Gathering metrics
May  7 11:52:38.146: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0507 11:52:38.145878      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  7 11:52:38.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c7fj" in namespace "gc-7604"
May  7 11:52:38.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n6z4" in namespace "gc-7604"
May  7 11:52:38.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kpzk" in namespace "gc-7604"
May  7 11:52:38.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-5495t" in namespace "gc-7604"
May  7 11:52:38.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pmch" in namespace "gc-7604"
May  7 11:52:38.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vswx" in namespace "gc-7604"
May  7 11:52:38.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x9s6" in namespace "gc-7604"
May  7 11:52:38.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zj6m" in namespace "gc-7604"
May  7 11:52:38.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bfkw" in namespace "gc-7604"
May  7 11:52:38.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p8ss" in namespace "gc-7604"
May  7 11:52:38.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-86dlv" in namespace "gc-7604"
May  7 11:52:38.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-874g4" in namespace "gc-7604"
May  7 11:52:38.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jfl4" in namespace "gc-7604"
May  7 11:52:38.414: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jsd4" in namespace "gc-7604"
May  7 11:52:38.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tl7f" in namespace "gc-7604"
May  7 11:52:38.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tlzr" in namespace "gc-7604"
May  7 11:52:38.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-bldhw" in namespace "gc-7604"
May  7 11:52:38.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-blzp5" in namespace "gc-7604"
May  7 11:52:38.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm8hn" in namespace "gc-7604"
May  7 11:52:38.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-bngwz" in namespace "gc-7604"
May  7 11:52:38.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-c487c" in namespace "gc-7604"
May  7 11:52:38.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-clv59" in namespace "gc-7604"
May  7 11:52:38.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqmbh" in namespace "gc-7604"
May  7 11:52:38.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-crcxl" in namespace "gc-7604"
May  7 11:52:38.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-dctxk" in namespace "gc-7604"
May  7 11:52:38.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnkmh" in namespace "gc-7604"
May  7 11:52:38.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpg6h" in namespace "gc-7604"
May  7 11:52:38.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-drw9k" in namespace "gc-7604"
May  7 11:52:38.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwg7z" in namespace "gc-7604"
May  7 11:52:38.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxkv4" in namespace "gc-7604"
May  7 11:52:38.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftsxh" in namespace "gc-7604"
May  7 11:52:38.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxbqc" in namespace "gc-7604"
May  7 11:52:38.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4jmp" in namespace "gc-7604"
May  7 11:52:38.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfc6p" in namespace "gc-7604"
May  7 11:52:38.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-grgbn" in namespace "gc-7604"
May  7 11:52:38.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2tf6" in namespace "gc-7604"
May  7 11:52:38.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfxk4" in namespace "gc-7604"
May  7 11:52:38.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqdts" in namespace "gc-7604"
May  7 11:52:38.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5bl2" in namespace "gc-7604"
May  7 11:52:38.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-jh5z8" in namespace "gc-7604"
May  7 11:52:39.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt7t7" in namespace "gc-7604"
May  7 11:52:39.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzrvl" in namespace "gc-7604"
May  7 11:52:39.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-kl9dh" in namespace "gc-7604"
May  7 11:52:39.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-kpdl9" in namespace "gc-7604"
May  7 11:52:39.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-ksm7s" in namespace "gc-7604"
May  7 11:52:39.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-l5vh6" in namespace "gc-7604"
May  7 11:52:39.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-ldr6h" in namespace "gc-7604"
May  7 11:52:39.147: INFO: Deleting pod "simpletest-rc-to-be-deleted-lh46v" in namespace "gc-7604"
May  7 11:52:39.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-lrtxm" in namespace "gc-7604"
May  7 11:52:39.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-m25ml" in namespace "gc-7604"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:39.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7604" for this suite.

• [SLOW TEST:17.739 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":294,"skipped":5828,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:39.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
May  7 11:52:39.298: INFO: Creating e2e-svc-a-rlw7l
May  7 11:52:39.328: INFO: Creating e2e-svc-b-vs29q
May  7 11:52:39.341: INFO: Creating e2e-svc-c-wzwh2
STEP: deleting service collection
May  7 11:52:39.416: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:39.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9263" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":295,"skipped":5839,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:39.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:52:39.528: INFO: The status of Pod busybox-host-aliases06739780-67fd-4cee-a891-9bab1e14aff0 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:52:41.540: INFO: The status of Pod busybox-host-aliases06739780-67fd-4cee-a891-9bab1e14aff0 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:52:43.535: INFO: The status of Pod busybox-host-aliases06739780-67fd-4cee-a891-9bab1e14aff0 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:52:45.547: INFO: The status of Pod busybox-host-aliases06739780-67fd-4cee-a891-9bab1e14aff0 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9153" for this suite.

• [SLOW TEST:6.148 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5855,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:45.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:45.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-8526
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5970" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:47.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8526" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":297,"skipped":5868,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:47.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:52:48.167: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:52:50.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 52, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 52, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 52, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 52, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 11:52:53.205: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May  7 11:52:53.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:53.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3849" for this suite.
STEP: Destroying namespace "webhook-3849-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.508 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":298,"skipped":5885,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:53.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:52:53.430: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3" in namespace "projected-2685" to be "Succeeded or Failed"
May  7 11:52:53.443: INFO: Pod "downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.160098ms
May  7 11:52:55.452: INFO: Pod "downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022098202s
May  7 11:52:57.621: INFO: Pod "downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.191110808s
STEP: Saw pod success
May  7 11:52:57.621: INFO: Pod "downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3" satisfied condition "Succeeded or Failed"
May  7 11:52:57.624: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3 container client-container: <nil>
STEP: delete the pod
May  7 11:52:57.653: INFO: Waiting for pod downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3 to disappear
May  7 11:52:57.656: INFO: Pod downwardapi-volume-a2c8547e-b689-4587-bf5c-a8b6babfd9a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:52:57.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2685" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5890,"failed":0}

------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:52:57.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May  7 11:52:57.757: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  7 11:52:59.768: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  7 11:53:01.767: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  7 11:53:03.764: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May  7 11:53:03.782: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  7 11:53:05.792: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May  7 11:53:05.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:05.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:05.797: INFO: ExecWithOptions: Clientset creation
May  7 11:53:05.797: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:05.894: INFO: Exec stderr: ""
May  7 11:53:05.895: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:05.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:05.896: INFO: ExecWithOptions: Clientset creation
May  7 11:53:05.897: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:05.985: INFO: Exec stderr: ""
May  7 11:53:05.985: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:05.987: INFO: ExecWithOptions: Clientset creation
May  7 11:53:05.987: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.084: INFO: Exec stderr: ""
May  7 11:53:06.085: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.086: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.086: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.172: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May  7 11:53:06.172: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.173: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.173: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.262: INFO: Exec stderr: ""
May  7 11:53:06.262: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.263: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.264: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.365: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May  7 11:53:06.365: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.367: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.367: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.446: INFO: Exec stderr: ""
May  7 11:53:06.446: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.447: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.447: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.531: INFO: Exec stderr: ""
May  7 11:53:06.531: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.537: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.537: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.623: INFO: Exec stderr: ""
May  7 11:53:06.623: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2781 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:53:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:53:06.624: INFO: ExecWithOptions: Clientset creation
May  7 11:53:06.625: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2781/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May  7 11:53:06.709: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:53:06.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2781" for this suite.

• [SLOW TEST:9.051 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5890,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:53:06.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:53:06.811: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  7 11:53:12.840: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:53:12.877: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4958  e52f4bd6-5eb7-4cf1-a3fe-486945fac17d 36300 1 2022-05-07 11:53:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-05-07 11:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ad6878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May  7 11:53:12.885: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May  7 11:53:12.885: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May  7 11:53:12.885: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4958  d8aa689a-093f-4d72-b4c9-45697183df48 36301 1 2022-05-07 11:53:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e52f4bd6-5eb7-4cf1-a3fe-486945fac17d 0xc004ceb537 0xc004ceb538}] []  [{e2e.test Update apps/v1 2022-05-07 11:53:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:53:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-05-07 11:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"e52f4bd6-5eb7-4cf1-a3fe-486945fac17d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ceb5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  7 11:53:12.941: INFO: Pod "test-cleanup-controller-v5h66" is available:
&Pod{ObjectMeta:{test-cleanup-controller-v5h66 test-cleanup-controller- deployment-4958  78eee2a0-4578-4d54-8fc7-10eb014864dc 36270 0 2022-05-07 11:53:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller d8aa689a-093f-4d72-b4c9-45697183df48 0xc002fd7e77 0xc002fd7e78}] []  [{kube-controller-manager Update v1 2022-05-07 11:53:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8aa689a-093f-4d72-b4c9-45697183df48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:53:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.8.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tvgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tvgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:53:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:53:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:53:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:53:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.8.2,StartTime:2022-05-07 11:53:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:53:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c88c8553c921e61c97944db0405b98fd46311eb3f70e5d5f533c017cc26e5825,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.8.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:53:12.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4958" for this suite.

• [SLOW TEST:6.248 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":301,"skipped":5894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:53:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-094a7b08-bee6-4df5-9066-2b245f77dcd2
STEP: Creating a pod to test consume secrets
May  7 11:53:13.144: INFO: Waiting up to 5m0s for pod "pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c" in namespace "secrets-1606" to be "Succeeded or Failed"
May  7 11:53:13.159: INFO: Pod "pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.910592ms
May  7 11:53:15.201: INFO: Pod "pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057222089s
May  7 11:53:17.213: INFO: Pod "pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069096843s
STEP: Saw pod success
May  7 11:53:17.214: INFO: Pod "pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c" satisfied condition "Succeeded or Failed"
May  7 11:53:17.221: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c container secret-volume-test: <nil>
STEP: delete the pod
May  7 11:53:17.246: INFO: Waiting for pod pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c to disappear
May  7 11:53:17.253: INFO: Pod pod-secrets-930792ed-f80c-45c0-af1b-53d420c0f51c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:53:17.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1606" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":6038,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:53:17.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:54:17.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3221" for this suite.

• [SLOW TEST:60.084 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":6059,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:54:17.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:54:21.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3650" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":6071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:54:21.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  7 11:54:25.605: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:54:25.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8784" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":6114,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:54:25.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
May  7 11:54:25.700: INFO: Creating simple deployment test-deployment-4t55s
May  7 11:54:25.714: INFO: deployment "test-deployment-4t55s" doesn't have the required revision set
May  7 11:54:27.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 54, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 54, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 54, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 54, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-4t55s-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
May  7 11:54:29.748: INFO: Deployment test-deployment-4t55s has Conditions: [{Available True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4t55s-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
May  7 11:54:29.763: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 54, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4t55s-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
May  7 11:54:29.768: INFO: Observed &Deployment event: ADDED
May  7 11:54:29.768: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4t55s-764bc7c4b7"}
May  7 11:54:29.768: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.768: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  7 11:54:29.769: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4t55s-764bc7c4b7" is progressing.}
May  7 11:54:29.769: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.769: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  7 11:54:29.770: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4t55s-764bc7c4b7" has successfully progressed.}
May  7 11:54:29.770: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.770: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  7 11:54:29.770: INFO: Observed Deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4t55s-764bc7c4b7" has successfully progressed.}
May  7 11:54:29.770: INFO: Found Deployment test-deployment-4t55s in namespace deployment-3084 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  7 11:54:29.770: INFO: Deployment test-deployment-4t55s has an updated status
STEP: patching the Statefulset Status
May  7 11:54:29.770: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  7 11:54:29.780: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
May  7 11:54:29.784: INFO: Observed &Deployment event: ADDED
May  7 11:54:29.784: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4t55s-764bc7c4b7"}
May  7 11:54:29.784: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.784: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  7 11:54:29.785: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:25 +0000 UTC 2022-05-07 11:54:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4t55s-764bc7c4b7" is progressing.}
May  7 11:54:29.785: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.785: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  7 11:54:29.785: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4t55s-764bc7c4b7" has successfully progressed.}
May  7 11:54:29.786: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.786: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  7 11:54:29.786: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-07 11:54:28 +0000 UTC 2022-05-07 11:54:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4t55s-764bc7c4b7" has successfully progressed.}
May  7 11:54:29.786: INFO: Observed deployment test-deployment-4t55s in namespace deployment-3084 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  7 11:54:29.786: INFO: Observed &Deployment event: MODIFIED
May  7 11:54:29.786: INFO: Found deployment test-deployment-4t55s in namespace deployment-3084 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  7 11:54:29.786: INFO: Deployment test-deployment-4t55s has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May  7 11:54:29.792: INFO: Deployment "test-deployment-4t55s":
&Deployment{ObjectMeta:{test-deployment-4t55s  deployment-3084  1eb7fff4-499f-490a-8664-e9c36bf12335 36620 1 2022-05-07 11:54:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-05-07 11:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-05-07 11:54:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-05-07 11:54:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00304d868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-4t55s-764bc7c4b7",LastUpdateTime:2022-05-07 11:54:29 +0000 UTC,LastTransitionTime:2022-05-07 11:54:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  7 11:54:29.796: INFO: New ReplicaSet "test-deployment-4t55s-764bc7c4b7" of Deployment "test-deployment-4t55s":
&ReplicaSet{ObjectMeta:{test-deployment-4t55s-764bc7c4b7  deployment-3084  6f6d1237-9bef-4921-8f8b-d056568b933c 36611 1 2022-05-07 11:54:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4t55s 1eb7fff4-499f-490a-8664-e9c36bf12335 0xc00304dc50 0xc00304dc51}] []  [{kube-controller-manager Update apps/v1 2022-05-07 11:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1eb7fff4-499f-490a-8664-e9c36bf12335\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-07 11:54:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00304dcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  7 11:54:29.803: INFO: Pod "test-deployment-4t55s-764bc7c4b7-vfch8" is available:
&Pod{ObjectMeta:{test-deployment-4t55s-764bc7c4b7-vfch8 test-deployment-4t55s-764bc7c4b7- deployment-3084  fc13c7c5-c7d8-4e02-9da0-08688a21e71b 36610 0 2022-05-07 11:54:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [{apps/v1 ReplicaSet test-deployment-4t55s-764bc7c4b7 6f6d1237-9bef-4921-8f8b-d056568b933c 0xc005b540a0 0xc005b540a1}] []  [{kube-controller-manager Update v1 2022-05-07 11:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f6d1237-9bef-4921-8f8b-d056568b933c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-05-07 11:54:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"40.0.9.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zth9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zth9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5615889a-d356-43b5-a818-02fb040c6965,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:54:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:54:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:54:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-07 11:54:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.1.0.5,PodIP:40.0.9.2,StartTime:2022-05-07 11:54:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-07 11:54:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f56b3451511e2be44347210fb9aae667b4dd79a1718200eb45738c15bf2596a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:40.0.9.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:54:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3084" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":306,"skipped":6118,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:54:29.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-e39804ca-3a4f-4ef8-bae3-a832317a3686
STEP: Creating configMap with name cm-test-opt-upd-32512de4-626b-48b8-889b-f88ea63f2fab
STEP: Creating the pod
May  7 11:54:29.915: INFO: The status of Pod pod-configmaps-dcff7552-05b4-46a0-af1e-baba5e093f8d is Pending, waiting for it to be Running (with Ready = true)
May  7 11:54:31.925: INFO: The status of Pod pod-configmaps-dcff7552-05b4-46a0-af1e-baba5e093f8d is Pending, waiting for it to be Running (with Ready = true)
May  7 11:54:33.923: INFO: The status of Pod pod-configmaps-dcff7552-05b4-46a0-af1e-baba5e093f8d is Pending, waiting for it to be Running (with Ready = true)
May  7 11:54:35.922: INFO: The status of Pod pod-configmaps-dcff7552-05b4-46a0-af1e-baba5e093f8d is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-e39804ca-3a4f-4ef8-bae3-a832317a3686
STEP: Updating configmap cm-test-opt-upd-32512de4-626b-48b8-889b-f88ea63f2fab
STEP: Creating configMap with name cm-test-opt-create-b80d0834-7b88-4e01-b830-9e76b07b7618
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:55:54.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4220" for this suite.

• [SLOW TEST:84.662 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":6131,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:55:54.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:55:54.565: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:55:56.577: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:55:58.570: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:00.593: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:02.575: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:04.573: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:06.576: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:08.571: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:10.578: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:12.577: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:14.572: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = false)
May  7 11:56:16.575: INFO: The status of Pod test-webserver-294c924b-e095-4a0e-983e-37df634d64e4 is Running (Ready = true)
May  7 11:56:16.579: INFO: Container started at 2022-05-07 11:55:57 +0000 UTC, pod became ready at 2022-05-07 11:56:14 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3303" for this suite.

• [SLOW TEST:22.116 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":6143,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:16.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6389
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  7 11:56:16.640: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  7 11:56:16.713: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:56:18.720: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  7 11:56:20.725: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:22.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:24.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:26.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:28.719: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:30.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:32.723: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:34.723: INFO: The status of Pod netserver-0 is Running (Ready = false)
May  7 11:56:36.725: INFO: The status of Pod netserver-0 is Running (Ready = true)
May  7 11:56:36.736: INFO: The status of Pod netserver-1 is Running (Ready = true)
May  7 11:56:36.744: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May  7 11:56:40.795: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  7 11:56:40.796: INFO: Going to poll 40.0.7.2 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  7 11:56:40.799: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 40.0.7.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6389 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:56:40.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:56:40.800: INFO: ExecWithOptions: Clientset creation
May  7 11:56:40.800: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-6389/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+40.0.7.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 11:56:41.907: INFO: Found all 1 expected endpoints: [netserver-0]
May  7 11:56:41.907: INFO: Going to poll 40.0.7.3 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  7 11:56:41.915: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 40.0.7.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6389 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:56:41.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:56:41.916: INFO: ExecWithOptions: Clientset creation
May  7 11:56:41.916: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-6389/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+40.0.7.3+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 11:56:43.005: INFO: Found all 1 expected endpoints: [netserver-1]
May  7 11:56:43.005: INFO: Going to poll 40.0.7.4 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  7 11:56:43.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 40.0.7.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6389 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 11:56:43.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 11:56:43.012: INFO: ExecWithOptions: Clientset creation
May  7 11:56:43.012: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/pod-network-test-6389/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+40.0.7.4+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May  7 11:56:44.101: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:44.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6389" for this suite.

• [SLOW TEST:27.518 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":309,"skipped":6147,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:44.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
May  7 11:56:44.205: INFO: Found Service test-service-2wdnn in namespace services-9282 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  7 11:56:44.205: INFO: Service test-service-2wdnn created
STEP: Getting /status
May  7 11:56:44.216: INFO: Service test-service-2wdnn has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May  7 11:56:44.240: INFO: observed Service test-service-2wdnn in namespace services-9282 with annotations: map[] & LoadBalancer: {[]}
May  7 11:56:44.240: INFO: Found Service test-service-2wdnn in namespace services-9282 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  7 11:56:44.241: INFO: Service test-service-2wdnn has service status patched
STEP: updating the ServiceStatus
May  7 11:56:44.269: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May  7 11:56:44.272: INFO: Observed Service test-service-2wdnn in namespace services-9282 with annotations: map[] & Conditions: {[]}
May  7 11:56:44.272: INFO: Observed event: &Service{ObjectMeta:{test-service-2wdnn  services-9282  533d919b-2167-4d50-8eab-5743dc14b447 37003 0 2022-05-07 11:56:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-05-07 11:56:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-05-07 11:56:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.150.24.55,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.150.24.55],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  7 11:56:44.273: INFO: Observed event: &Service{ObjectMeta:{test-service-2wdnn  services-9282  533d919b-2167-4d50-8eab-5743dc14b447 37004 0 2022-05-07 11:56:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-05-07 11:56:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-05-07 11:56:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.150.24.55,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.150.24.55],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
May  7 11:56:44.273: INFO: Found Service test-service-2wdnn in namespace services-9282 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  7 11:56:44.273: INFO: Service test-service-2wdnn has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May  7 11:56:44.308: INFO: observed Service test-service-2wdnn in namespace services-9282 with labels: map[test-service-static:true]
May  7 11:56:44.308: INFO: observed Service test-service-2wdnn in namespace services-9282 with labels: map[test-service-static:true]
May  7 11:56:44.309: INFO: observed Service test-service-2wdnn in namespace services-9282 with labels: map[test-service-static:true]
May  7 11:56:44.309: INFO: observed Service test-service-2wdnn in namespace services-9282 with labels: map[test-service-static:true]
May  7 11:56:44.310: INFO: Found Service test-service-2wdnn in namespace services-9282 with labels: map[test-service:patched test-service-static:true]
May  7 11:56:44.311: INFO: Service test-service-2wdnn patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May  7 11:56:44.346: INFO: Observed event: ADDED
May  7 11:56:44.347: INFO: Observed event: MODIFIED
May  7 11:56:44.347: INFO: Observed event: MODIFIED
May  7 11:56:44.347: INFO: Observed event: MODIFIED
May  7 11:56:44.347: INFO: Observed event: MODIFIED
May  7 11:56:44.347: INFO: Observed event: MODIFIED
May  7 11:56:44.348: INFO: Found Service test-service-2wdnn in namespace services-9282 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  7 11:56:44.348: INFO: Service test-service-2wdnn deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:44.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9282" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":310,"skipped":6158,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:44.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2535" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":311,"skipped":6172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:44.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:48.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-243" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":312,"skipped":6200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:48.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
May  7 11:56:48.660: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-2557 proxy --unix-socket=/tmp/kubectl-proxy-unix4210943014/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:48.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2557" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":313,"skipped":6231,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-85ed5d4c-8523-4c47-b29b-68006344ef31
STEP: Creating a pod to test consume secrets
May  7 11:56:48.889: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda" in namespace "projected-194" to be "Succeeded or Failed"
May  7 11:56:48.927: INFO: Pod "pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda": Phase="Pending", Reason="", readiness=false. Elapsed: 38.072676ms
May  7 11:56:50.946: INFO: Pod "pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05680982s
May  7 11:56:52.966: INFO: Pod "pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07750308s
STEP: Saw pod success
May  7 11:56:52.967: INFO: Pod "pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda" satisfied condition "Succeeded or Failed"
May  7 11:56:52.971: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda container secret-volume-test: <nil>
STEP: delete the pod
May  7 11:56:52.997: INFO: Waiting for pod pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda to disappear
May  7 11:56:53.000: INFO: Pod pod-projected-secrets-6ed7a9a7-73da-4987-bd30-164754b5ecda no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:56:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-194" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":6239,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:56:53.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 11:56:53.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May  7 11:56:57.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 create -f -'
May  7 11:56:59.453: INFO: stderr: ""
May  7 11:56:59.453: INFO: stdout: "e2e-test-crd-publish-openapi-4208-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  7 11:56:59.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 delete e2e-test-crd-publish-openapi-4208-crds test-foo'
May  7 11:56:59.555: INFO: stderr: ""
May  7 11:56:59.555: INFO: stdout: "e2e-test-crd-publish-openapi-4208-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  7 11:56:59.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 apply -f -'
May  7 11:56:59.807: INFO: stderr: ""
May  7 11:56:59.807: INFO: stdout: "e2e-test-crd-publish-openapi-4208-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  7 11:56:59.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 delete e2e-test-crd-publish-openapi-4208-crds test-foo'
May  7 11:56:59.878: INFO: stderr: ""
May  7 11:56:59.878: INFO: stdout: "e2e-test-crd-publish-openapi-4208-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
May  7 11:56:59.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 create -f -'
May  7 11:57:00.179: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May  7 11:57:00.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 create -f -'
May  7 11:57:00.483: INFO: rc: 1
May  7 11:57:00.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 apply -f -'
May  7 11:57:00.705: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May  7 11:57:00.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 create -f -'
May  7 11:57:00.906: INFO: rc: 1
May  7 11:57:00.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 --namespace=crd-publish-openapi-4432 apply -f -'
May  7 11:57:01.099: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May  7 11:57:01.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 explain e2e-test-crd-publish-openapi-4208-crds'
May  7 11:57:01.327: INFO: stderr: ""
May  7 11:57:01.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4208-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May  7 11:57:01.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 explain e2e-test-crd-publish-openapi-4208-crds.metadata'
May  7 11:57:01.571: INFO: stderr: ""
May  7 11:57:01.571: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4208-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  7 11:57:01.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 explain e2e-test-crd-publish-openapi-4208-crds.spec'
May  7 11:57:01.788: INFO: stderr: ""
May  7 11:57:01.788: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4208-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  7 11:57:01.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 explain e2e-test-crd-publish-openapi-4208-crds.spec.bars'
May  7 11:57:02.031: INFO: stderr: ""
May  7 11:57:02.031: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4208-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May  7 11:57:02.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=crd-publish-openapi-4432 explain e2e-test-crd-publish-openapi-4208-crds.spec.bars2'
May  7 11:57:02.321: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:57:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4432" for this suite.

• [SLOW TEST:11.961 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":315,"skipped":6239,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:57:04.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
May  7 11:57:05.039: INFO: Waiting up to 5m0s for pod "pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0" in namespace "emptydir-1372" to be "Succeeded or Failed"
May  7 11:57:05.054: INFO: Pod "pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527787ms
May  7 11:57:07.061: INFO: Pod "pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022362284s
May  7 11:57:09.070: INFO: Pod "pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031080997s
STEP: Saw pod success
May  7 11:57:09.070: INFO: Pod "pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0" satisfied condition "Succeeded or Failed"
May  7 11:57:09.073: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0 container test-container: <nil>
STEP: delete the pod
May  7 11:57:09.095: INFO: Waiting for pod pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0 to disappear
May  7 11:57:09.099: INFO: Pod pod-02c9bce6-70fc-4598-b583-0ecffde6b2e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:57:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1372" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:57:09.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 11:57:09.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187" in namespace "projected-3352" to be "Succeeded or Failed"
May  7 11:57:09.171: INFO: Pod "downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187": Phase="Pending", Reason="", readiness=false. Elapsed: 6.719412ms
May  7 11:57:11.180: INFO: Pod "downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015574246s
May  7 11:57:13.197: INFO: Pod "downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032699682s
STEP: Saw pod success
May  7 11:57:13.198: INFO: Pod "downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187" satisfied condition "Succeeded or Failed"
May  7 11:57:13.201: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187 container client-container: <nil>
STEP: delete the pod
May  7 11:57:13.234: INFO: Waiting for pod downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187 to disappear
May  7 11:57:13.241: INFO: Pod downwardapi-volume-88bbda99-8152-4697-94af-ca2e547c6187 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:57:13.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3352" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":6291,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:57:13.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1673
May  7 11:57:13.312: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  7 11:57:15.323: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May  7 11:57:15.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  7 11:57:15.503: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  7 11:57:15.503: INFO: stdout: "iptables"
May  7 11:57:15.503: INFO: proxyMode: iptables
May  7 11:57:15.517: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  7 11:57:15.521: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1673
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1673
I0507 11:57:15.565712      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1673, replica count: 3
I0507 11:57:18.617654      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0507 11:57:21.620446      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  7 11:57:21.628: INFO: Creating new exec pod
May  7 11:57:26.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May  7 11:57:26.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May  7 11:57:26.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:57:26.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.150.125.56 80'
May  7 11:57:27.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.150.125.56 80\nConnection to 10.150.125.56 80 port [tcp/http] succeeded!\n"
May  7 11:57:27.043: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  7 11:57:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:57:27.333: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:57:27.334: INFO: stdout: "\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs"
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:27.334: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:57.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:57:57.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:57:57.614: INFO: stdout: "\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs"
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:57:57.614: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:58:27.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:58:27.602: INFO: stdout: "\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp"
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:27.602: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:57.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:58:57.612: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:58:57.612: INFO: stdout: "\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs"
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:58:57.612: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:59:27.635: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:59:27.635: INFO: stdout: "\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-5scbs\naffinity-clusterip-timeout-vlc27\naffinity-clusterip-timeout-vlc27"
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-5scbs
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:59:27.635: INFO: Received response from host: affinity-clusterip-timeout-vlc27
May  7 11:59:27.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.150.125.56:80/ ; done'
May  7 11:59:27.911: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:59:27.912: INFO: stdout: "\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp\naffinity-clusterip-timeout-lwqjp"
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.912: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.913: INFO: Received response from host: affinity-clusterip-timeout-lwqjp
May  7 11:59:27.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.150.125.56:80/'
May  7 11:59:28.091: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:59:28.091: INFO: stdout: "affinity-clusterip-timeout-lwqjp"
May  7 11:59:48.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=services-1673 exec execpod-affinityg72ms -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.150.125.56:80/'
May  7 11:59:48.279: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.150.125.56:80/\n"
May  7 11:59:48.279: INFO: stdout: "affinity-clusterip-timeout-5scbs"
May  7 11:59:48.279: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1673, will wait for the garbage collector to delete the pods
May  7 11:59:48.380: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.215587ms
May  7 11:59:48.482: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.545709ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:59:51.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1673" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:157.981 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":318,"skipped":6303,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:59:51.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May  7 11:59:51.286: INFO: Waiting up to 5m0s for pod "security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad" in namespace "security-context-7941" to be "Succeeded or Failed"
May  7 11:59:51.292: INFO: Pod "security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.176125ms
May  7 11:59:53.304: INFO: Pod "security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01768657s
May  7 11:59:55.315: INFO: Pod "security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028742332s
STEP: Saw pod success
May  7 11:59:55.315: INFO: Pod "security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad" satisfied condition "Succeeded or Failed"
May  7 11:59:55.318: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad container test-container: <nil>
STEP: delete the pod
May  7 11:59:55.394: INFO: Waiting for pod security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad to disappear
May  7 11:59:55.398: INFO: Pod security-context-0c07aa79-cde4-45a4-8594-1f9055c169ad no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 11:59:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7941" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":319,"skipped":6324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 11:59:55.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  7 11:59:55.906: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  7 11:59:57.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 11, 59, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 59, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 11, 59, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 11, 59, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  7 12:00:00.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:01.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8561" for this suite.
STEP: Destroying namespace "webhook-8561-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.728 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":320,"skipped":6359,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:01.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
May  7 12:00:01.201: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:07.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4954" for this suite.

• [SLOW TEST:6.167 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":321,"skipped":6368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:07.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 12:00:07.435: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"aa286968-0e0e-4be3-b2d5-299be683343a", Controller:(*bool)(0xc004bd925e), BlockOwnerDeletion:(*bool)(0xc004bd925f)}}
May  7 12:00:07.454: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5bf14917-8f93-4b82-8df9-1b393bdce209", Controller:(*bool)(0xc004fd44e6), BlockOwnerDeletion:(*bool)(0xc004fd44e7)}}
May  7 12:00:07.468: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"06a3e537-04ec-4e7b-aa6f-2f1feab5856d", Controller:(*bool)(0xc004bd94de), BlockOwnerDeletion:(*bool)(0xc004bd94df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8302" for this suite.

• [SLOW TEST:5.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":322,"skipped":6400,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:12.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 12:00:12.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d" in namespace "downward-api-374" to be "Succeeded or Failed"
May  7 12:00:12.607: INFO: Pod "downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.064431ms
May  7 12:00:14.619: INFO: Pod "downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026659924s
May  7 12:00:16.626: INFO: Pod "downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033860852s
STEP: Saw pod success
May  7 12:00:16.626: INFO: Pod "downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d" satisfied condition "Succeeded or Failed"
May  7 12:00:16.631: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d container client-container: <nil>
STEP: delete the pod
May  7 12:00:16.663: INFO: Waiting for pod downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d to disappear
May  7 12:00:16.667: INFO: Pod downwardapi-volume-07fba0a5-d528-46cf-aa12-1446e21f2b6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:16.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-374" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6409,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:16.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May  7 12:00:16.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-2854 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
May  7 12:00:16.832: INFO: stderr: ""
May  7 12:00:16.832: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
May  7 12:00:16.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-2854 delete pods e2e-test-httpd-pod'
May  7 12:00:21.378: INFO: stderr: ""
May  7 12:00:21.378: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:21.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2854" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":324,"skipped":6414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:21.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:21.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3033" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":325,"skipped":6438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:21.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-823.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-823.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-823.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-823.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  7 12:00:25.669: INFO: DNS probes using dns-823/dns-test-d3e5abbd-af78-496c-bde8-324cfe5e06d4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:25.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-823" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":326,"skipped":6470,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
May  7 12:00:25.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6401 create -f -'
May  7 12:00:26.657: INFO: stderr: ""
May  7 12:00:26.658: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May  7 12:00:27.668: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:27.668: INFO: Found 0 / 1
May  7 12:00:28.667: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:28.667: INFO: Found 0 / 1
May  7 12:00:29.748: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:29.749: INFO: Found 0 / 1
May  7 12:00:30.667: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:30.667: INFO: Found 1 / 1
May  7 12:00:30.667: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May  7 12:00:30.674: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:30.674: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  7 12:00:30.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-6401 patch pod agnhost-primary-mnnrx -p {"metadata":{"annotations":{"x":"y"}}}'
May  7 12:00:30.789: INFO: stderr: ""
May  7 12:00:30.789: INFO: stdout: "pod/agnhost-primary-mnnrx patched\n"
STEP: checking annotations
May  7 12:00:30.792: INFO: Selector matched 1 pods for map[app:agnhost]
May  7 12:00:30.792: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:30.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6401" for this suite.

• [SLOW TEST:5.091 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1483
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":327,"skipped":6480,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:30.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
May  7 12:00:30.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:00:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4569" for this suite.

• [SLOW TEST:20.484 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":328,"skipped":6490,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:00:51.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9167
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9167
STEP: Deleting pre-stop pod
May  7 12:01:04.449: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:04.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9167" for this suite.

• [SLOW TEST:13.280 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":329,"skipped":6500,"failed":0}
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:04.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:04.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1758" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":330,"skipped":6503,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:04.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
May  7 12:01:04.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 create -f -'
May  7 12:01:05.877: INFO: stderr: ""
May  7 12:01:05.877: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  7 12:01:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 12:01:05.978: INFO: stderr: ""
May  7 12:01:05.979: INFO: stdout: "update-demo-nautilus-hqrnp update-demo-nautilus-mm48b "
May  7 12:01:05.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-hqrnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:06.069: INFO: stderr: ""
May  7 12:01:06.069: INFO: stdout: ""
May  7 12:01:06.069: INFO: update-demo-nautilus-hqrnp is created but not running
May  7 12:01:11.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 12:01:11.170: INFO: stderr: ""
May  7 12:01:11.170: INFO: stdout: "update-demo-nautilus-hqrnp update-demo-nautilus-mm48b "
May  7 12:01:11.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-hqrnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:11.247: INFO: stderr: ""
May  7 12:01:11.247: INFO: stdout: "true"
May  7 12:01:11.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-hqrnp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 12:01:11.343: INFO: stderr: ""
May  7 12:01:11.343: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 12:01:11.343: INFO: validating pod update-demo-nautilus-hqrnp
May  7 12:01:11.374: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 12:01:11.374: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 12:01:11.374: INFO: update-demo-nautilus-hqrnp is verified up and running
May  7 12:01:11.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:11.467: INFO: stderr: ""
May  7 12:01:11.467: INFO: stdout: "true"
May  7 12:01:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 12:01:11.544: INFO: stderr: ""
May  7 12:01:11.544: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 12:01:11.544: INFO: validating pod update-demo-nautilus-mm48b
May  7 12:01:11.559: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 12:01:11.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 12:01:11.559: INFO: update-demo-nautilus-mm48b is verified up and running
STEP: scaling down the replication controller
May  7 12:01:11.563: INFO: scanned /root for discovery docs: <nil>
May  7 12:01:11.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  7 12:01:12.661: INFO: stderr: ""
May  7 12:01:12.661: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  7 12:01:12.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 12:01:12.742: INFO: stderr: ""
May  7 12:01:12.742: INFO: stdout: "update-demo-nautilus-mm48b "
May  7 12:01:12.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:12.816: INFO: stderr: ""
May  7 12:01:12.816: INFO: stdout: "true"
May  7 12:01:12.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 12:01:12.889: INFO: stderr: ""
May  7 12:01:12.890: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 12:01:12.890: INFO: validating pod update-demo-nautilus-mm48b
May  7 12:01:12.897: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 12:01:12.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 12:01:12.897: INFO: update-demo-nautilus-mm48b is verified up and running
STEP: scaling up the replication controller
May  7 12:01:12.900: INFO: scanned /root for discovery docs: <nil>
May  7 12:01:12.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  7 12:01:14.032: INFO: stderr: ""
May  7 12:01:14.032: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  7 12:01:14.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 12:01:14.109: INFO: stderr: ""
May  7 12:01:14.109: INFO: stdout: "update-demo-nautilus-h76hf update-demo-nautilus-mm48b "
May  7 12:01:14.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-h76hf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:14.192: INFO: stderr: ""
May  7 12:01:14.192: INFO: stdout: ""
May  7 12:01:14.192: INFO: update-demo-nautilus-h76hf is created but not running
May  7 12:01:19.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  7 12:01:19.292: INFO: stderr: ""
May  7 12:01:19.292: INFO: stdout: "update-demo-nautilus-h76hf update-demo-nautilus-mm48b "
May  7 12:01:19.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-h76hf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:19.367: INFO: stderr: ""
May  7 12:01:19.367: INFO: stdout: "true"
May  7 12:01:19.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-h76hf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 12:01:19.447: INFO: stderr: ""
May  7 12:01:19.447: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 12:01:19.447: INFO: validating pod update-demo-nautilus-h76hf
May  7 12:01:19.455: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 12:01:19.455: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 12:01:19.455: INFO: update-demo-nautilus-h76hf is verified up and running
May  7 12:01:19.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  7 12:01:19.539: INFO: stderr: ""
May  7 12:01:19.539: INFO: stdout: "true"
May  7 12:01:19.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods update-demo-nautilus-mm48b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  7 12:01:19.617: INFO: stderr: ""
May  7 12:01:19.617: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May  7 12:01:19.617: INFO: validating pod update-demo-nautilus-mm48b
May  7 12:01:19.634: INFO: got data: {
  "image": "nautilus.jpg"
}

May  7 12:01:19.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  7 12:01:19.634: INFO: update-demo-nautilus-mm48b is verified up and running
STEP: using delete to clean up resources
May  7 12:01:19.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 delete --grace-period=0 --force -f -'
May  7 12:01:19.733: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  7 12:01:19.733: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  7 12:01:19.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get rc,svc -l name=update-demo --no-headers'
May  7 12:01:19.819: INFO: stderr: "No resources found in kubectl-275 namespace.\n"
May  7 12:01:19.819: INFO: stdout: ""
May  7 12:01:19.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1543283775 --namespace=kubectl-275 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  7 12:01:19.906: INFO: stderr: ""
May  7 12:01:19.906: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-275" for this suite.

• [SLOW TEST:15.136 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":331,"skipped":6504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May  7 12:01:19.986: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  7 12:01:19.998: INFO: Waiting for terminating namespaces to be deleted...
May  7 12:01:20.007: INFO: 
Logging pods the apiserver thinks is on node 17a1d27a-cdc7-4f76-997d-67ea6c2b9bf7 before test
May  7 12:01:20.029: INFO: coredns-787c57488d-xh45r from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.029: INFO: 	Container coredns ready: true, restart count 0
May  7 12:01:20.029: INFO: event-controller-795755d67-lkvmj from pks-system started at 2022-05-07 10:26:54 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.029: INFO: 	Container event-controller ready: true, restart count 0
May  7 12:01:20.029: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 12:01:20.029: INFO: fluent-bit-lgdqp from pks-system started at 2022-05-07 10:27:00 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.029: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 12:01:20.030: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 12:01:20.030: INFO: node-exporter-cggqd from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 12:01:20.030: INFO: observability-manager-764bb5b6d9-5pw7g from pks-system started at 2022-05-07 10:26:47 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container observability-manager ready: true, restart count 0
May  7 12:01:20.030: INFO: sink-controller-77c8c69d54-vmhrt from pks-system started at 2022-05-07 10:26:54 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container sink-controller ready: true, restart count 0
May  7 12:01:20.030: INFO: telegraf-8f7tz from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container telegraf ready: true, restart count 0
May  7 12:01:20.030: INFO: telemetry-agent-6f6f75b47-qvqbh from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
May  7 12:01:20.030: INFO: wavefront-collector-6b7bf647bb-zvh8w from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container wavefront-collector ready: true, restart count 0
May  7 12:01:20.030: INFO: sonobuoy from sonobuoy started at 2022-05-07 10:16:30 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  7 12:01:20.030: INFO: sonobuoy-e2e-job-fe2325511c6f494a from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container e2e ready: true, restart count 0
May  7 12:01:20.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 12:01:20.030: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-kdbxn from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 12:01:20.030: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 12:01:20.030: INFO: 
Logging pods the apiserver thinks is on node 5615889a-d356-43b5-a818-02fb040c6965 before test
May  7 12:01:20.052: INFO: coredns-787c57488d-2wsp6 from kube-system started at 2022-05-07 10:41:08 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container coredns ready: true, restart count 0
May  7 12:01:20.052: INFO: update-demo-nautilus-h76hf from kubectl-275 started at 2022-05-07 12:01:13 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container update-demo ready: true, restart count 0
May  7 12:01:20.052: INFO: fluent-bit-7vdxl from pks-system started at 2022-05-07 10:40:58 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 12:01:20.052: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 12:01:20.052: INFO: node-exporter-2bp48 from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 12:01:20.052: INFO: telegraf-x4jms from pks-system started at 2022-05-07 10:40:58 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container telegraf ready: true, restart count 0
May  7 12:01:20.052: INFO: pod-qos-class-76c4d847-b19b-44e7-a2a8-ec8051239345 from pods-1758 started at 2022-05-07 12:01:04 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container agnhost ready: false, restart count 0
May  7 12:01:20.052: INFO: tester from prestop-9167 started at 2022-05-07 12:00:55 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container tester ready: true, restart count 0
May  7 12:01:20.052: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-8dzqr from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.052: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 12:01:20.052: INFO: 	Container systemd-logs ready: true, restart count 0
May  7 12:01:20.052: INFO: 
Logging pods the apiserver thinks is on node 99a282ee-1c9e-4ab7-bf0e-2dd1a29793b3 before test
May  7 12:01:20.080: INFO: coredns-787c57488d-mhgg5 from kube-system started at 2022-05-07 09:57:15 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container coredns ready: true, restart count 0
May  7 12:01:20.080: INFO: metrics-server-66c5bff789-gl2ww from kube-system started at 2022-05-07 10:26:48 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container metrics-server ready: true, restart count 0
May  7 12:01:20.080: INFO: update-demo-nautilus-mm48b from kubectl-275 started at 2022-05-07 12:01:05 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container update-demo ready: true, restart count 0
May  7 12:01:20.080: INFO: fluent-bit-pzwr7 from pks-system started at 2022-05-07 10:27:01 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container fluent-bit ready: true, restart count 0
May  7 12:01:20.080: INFO: 	Container ghostunnel ready: true, restart count 0
May  7 12:01:20.080: INFO: metric-controller-669f9bc57b-tzdqq from pks-system started at 2022-05-07 10:40:32 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container metric-controller ready: true, restart count 0
May  7 12:01:20.080: INFO: node-exporter-bs69m from pks-system started at 2022-05-07 09:57:32 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
May  7 12:01:20.080: INFO: telegraf-dp5wl from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.080: INFO: 	Container telegraf ready: true, restart count 0
May  7 12:01:20.081: INFO: validator-8567d4b66-q8584 from pks-system started at 2022-05-07 10:26:55 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.081: INFO: 	Container validator ready: true, restart count 0
May  7 12:01:20.081: INFO: wavefront-proxy-54bfcd9d6b-4lb72 from pks-system started at 2022-05-07 09:59:30 +0000 UTC (1 container statuses recorded)
May  7 12:01:20.081: INFO: 	Container wavefront-proxy ready: true, restart count 0
May  7 12:01:20.081: INFO: sonobuoy-systemd-logs-daemon-set-136821a1bc7d49a0-qtqsp from sonobuoy started at 2022-05-07 10:16:36 +0000 UTC (2 container statuses recorded)
May  7 12:01:20.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  7 12:01:20.081: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16ecd0ed1bb4527b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:21.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-94" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":332,"skipped":6532,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 12:01:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:22.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2875" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":333,"skipped":6542,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:22.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-bws2
STEP: Creating a pod to test atomic-volume-subpath
May  7 12:01:22.362: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bws2" in namespace "subpath-9405" to be "Succeeded or Failed"
May  7 12:01:22.369: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.670738ms
May  7 12:01:24.379: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017545138s
May  7 12:01:26.385: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023323627s
May  7 12:01:28.394: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 6.032753348s
May  7 12:01:30.405: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 8.043673267s
May  7 12:01:32.415: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 10.053567577s
May  7 12:01:34.437: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 12.075767444s
May  7 12:01:36.458: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 14.096772419s
May  7 12:01:38.504: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 16.142233343s
May  7 12:01:40.528: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 18.166808474s
May  7 12:01:42.542: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 20.180632632s
May  7 12:01:44.558: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Running", Reason="", readiness=true. Elapsed: 22.195905338s
May  7 12:01:46.566: INFO: Pod "pod-subpath-test-projected-bws2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.204581694s
STEP: Saw pod success
May  7 12:01:46.566: INFO: Pod "pod-subpath-test-projected-bws2" satisfied condition "Succeeded or Failed"
May  7 12:01:46.573: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-subpath-test-projected-bws2 container test-container-subpath-projected-bws2: <nil>
STEP: delete the pod
May  7 12:01:46.621: INFO: Waiting for pod pod-subpath-test-projected-bws2 to disappear
May  7 12:01:46.626: INFO: Pod pod-subpath-test-projected-bws2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bws2
May  7 12:01:46.627: INFO: Deleting pod "pod-subpath-test-projected-bws2" in namespace "subpath-9405"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:46.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9405" for this suite.

• [SLOW TEST:24.363 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":334,"skipped":6557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:46.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:01:46.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6473" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":6579,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:01:46.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May  7 12:01:46.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
May  7 12:01:47.370: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  7 12:01:49.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:01:51.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:01:53.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:01:55.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:01:57.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:01:59.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 7, 12, 1, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  7 12:02:03.349: INFO: Waited 1.850743797s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May  7 12:02:03.473: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:02:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9663" for this suite.

• [SLOW TEST:17.262 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":336,"skipped":6600,"failed":0}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:02:04.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
May  7 12:02:04.408: INFO: Major version: 1
STEP: Confirm minor version
May  7 12:02:04.408: INFO: cleanMinorVersion: 23
May  7 12:02:04.409: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:02:04.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8678" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":337,"skipped":6601,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:02:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
May  7 12:04:05.426: INFO: Successfully updated pod "var-expansion-073a25c0-4b58-4b69-87bc-5d4f624ba887"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May  7 12:04:07.452: INFO: Deleting pod "var-expansion-073a25c0-4b58-4b69-87bc-5d4f624ba887" in namespace "var-expansion-5634"
May  7 12:04:07.463: INFO: Wait up to 5m0s for pod "var-expansion-073a25c0-4b58-4b69-87bc-5d4f624ba887" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:04:39.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5634" for this suite.

• [SLOW TEST:155.077 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":338,"skipped":6602,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:04:39.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May  7 12:04:39.625: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:41.638: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:43.861: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 30.1.0.3 on the node which pod1 resides and expect scheduled
May  7 12:04:43.946: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:45.956: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:47.961: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 30.1.0.3 but use UDP protocol on the node which pod2 resides
May  7 12:04:47.982: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:49.991: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:51.989: INFO: The status of Pod pod3 is Running (Ready = true)
May  7 12:04:52.014: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  7 12:04:54.021: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May  7 12:04:54.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 30.1.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-4643 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 12:04:54.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 12:04:54.026: INFO: ExecWithOptions: Clientset creation
May  7 12:04:54.027: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/hostport-4643/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+30.1.0.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.1.0.3, port: 54323
May  7 12:04:54.150: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://30.1.0.3:54323/hostname] Namespace:hostport-4643 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 12:04:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 12:04:54.151: INFO: ExecWithOptions: Clientset creation
May  7 12:04:54.151: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/hostport-4643/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F30.1.0.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 30.1.0.3, port: 54323 UDP
May  7 12:04:54.265: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 30.1.0.3 54323] Namespace:hostport-4643 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  7 12:04:54.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
May  7 12:04:54.266: INFO: ExecWithOptions: Clientset creation
May  7 12:04:54.266: INFO: ExecWithOptions: execute(POST https://10.150.0.1:443/api/v1/namespaces/hostport-4643/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+30.1.0.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:04:59.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4643" for this suite.

• [SLOW TEST:19.890 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":339,"skipped":6614,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:04:59.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5286
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 12:04:59.547: INFO: Found 0 stateful pods, waiting for 1
May  7 12:05:09.556: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
May  7 12:05:09.596: INFO: Found 1 stateful pods, waiting for 2
May  7 12:05:19.612: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  7 12:05:19.613: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May  7 12:05:19.646: INFO: Deleting all statefulset in ns statefulset-5286
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:05:19.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5286" for this suite.

• [SLOW TEST:20.349 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":340,"skipped":6620,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:05:19.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 12:05:19.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0" in namespace "downward-api-5242" to be "Succeeded or Failed"
May  7 12:05:19.843: INFO: Pod "downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.098369ms
May  7 12:05:21.855: INFO: Pod "downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026169067s
May  7 12:05:23.862: INFO: Pod "downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032564121s
STEP: Saw pod success
May  7 12:05:23.862: INFO: Pod "downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0" satisfied condition "Succeeded or Failed"
May  7 12:05:23.867: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0 container client-container: <nil>
STEP: delete the pod
May  7 12:05:23.912: INFO: Waiting for pod downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0 to disappear
May  7 12:05:23.922: INFO: Pod downwardapi-volume-99153755-69cf-48fa-b795-5cb4f30f1be0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:05:23.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5242" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":341,"skipped":6641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:05:23.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May  7 12:05:24.016: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May  7 12:05:24.029: INFO: starting watch
STEP: patching
STEP: updating
May  7 12:05:24.063: INFO: waiting for watch events with expected annotations
May  7 12:05:24.063: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:05:24.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4521" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":342,"skipped":6669,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:05:24.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
May  7 12:05:24.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b" in namespace "downward-api-4656" to be "Succeeded or Failed"
May  7 12:05:24.224: INFO: Pod "downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.086272ms
May  7 12:05:26.241: INFO: Pod "downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033188459s
May  7 12:05:28.250: INFO: Pod "downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042196061s
STEP: Saw pod success
May  7 12:05:28.250: INFO: Pod "downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b" satisfied condition "Succeeded or Failed"
May  7 12:05:28.256: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b container client-container: <nil>
STEP: delete the pod
May  7 12:05:28.292: INFO: Waiting for pod downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b to disappear
May  7 12:05:28.300: INFO: Pod downwardapi-volume-b662a7a7-6cf2-4631-8f50-895c01bf322b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:05:28.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4656" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6669,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:05:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
May  7 12:05:28.397: INFO: created test-pod-1
May  7 12:05:32.429: INFO: running and ready test-pod-1
May  7 12:05:32.438: INFO: created test-pod-2
May  7 12:05:36.468: INFO: running and ready test-pod-2
May  7 12:05:36.480: INFO: created test-pod-3
May  7 12:05:40.518: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
May  7 12:05:40.588: INFO: Pod quantity 3 is different from expected quantity 0
May  7 12:05:41.604: INFO: Pod quantity 3 is different from expected quantity 0
May  7 12:05:42.608: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:05:43.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7040" for this suite.

• [SLOW TEST:15.302 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":344,"skipped":6685,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:05:43.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-s4mp
STEP: Creating a pod to test atomic-volume-subpath
May  7 12:05:43.740: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s4mp" in namespace "subpath-4651" to be "Succeeded or Failed"
May  7 12:05:43.747: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.79877ms
May  7 12:05:45.817: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076942407s
May  7 12:05:47.833: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 4.092741579s
May  7 12:05:49.842: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 6.102063386s
May  7 12:05:51.853: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 8.113499575s
May  7 12:05:53.863: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 10.122822927s
May  7 12:05:55.873: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 12.132875182s
May  7 12:05:57.890: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 14.150398065s
May  7 12:05:59.899: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 16.159019946s
May  7 12:06:01.913: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 18.173078101s
May  7 12:06:03.923: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 20.182639648s
May  7 12:06:05.939: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Running", Reason="", readiness=true. Elapsed: 22.198808782s
May  7 12:06:07.951: INFO: Pod "pod-subpath-test-configmap-s4mp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.211240015s
STEP: Saw pod success
May  7 12:06:07.951: INFO: Pod "pod-subpath-test-configmap-s4mp" satisfied condition "Succeeded or Failed"
May  7 12:06:07.957: INFO: Trying to get logs from node 5615889a-d356-43b5-a818-02fb040c6965 pod pod-subpath-test-configmap-s4mp container test-container-subpath-configmap-s4mp: <nil>
STEP: delete the pod
May  7 12:06:07.994: INFO: Waiting for pod pod-subpath-test-configmap-s4mp to disappear
May  7 12:06:08.003: INFO: Pod pod-subpath-test-configmap-s4mp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s4mp
May  7 12:06:08.003: INFO: Deleting pod "pod-subpath-test-configmap-s4mp" in namespace "subpath-4651"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:06:08.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4651" for this suite.

• [SLOW TEST:24.403 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":345,"skipped":6693,"failed":0}
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May  7 12:06:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1543283775
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
May  7 12:06:08.087: INFO: Got root ca configmap in namespace "svcaccounts-1518"
May  7 12:06:08.096: INFO: Deleted root ca configmap in namespace "svcaccounts-1518"
STEP: waiting for a new root ca configmap created
May  7 12:06:08.604: INFO: Recreated root ca configmap in namespace "svcaccounts-1518"
May  7 12:06:08.614: INFO: Updated root ca configmap in namespace "svcaccounts-1518"
STEP: waiting for the root ca configmap reconciled
May  7 12:06:09.124: INFO: Reconciled root ca configmap in namespace "svcaccounts-1518"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May  7 12:06:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1518" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":346,"skipped":6693,"failed":0}
SSSMay  7 12:06:09.153: INFO: Running AfterSuite actions on all nodes
May  7 12:06:09.153: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May  7 12:06:09.154: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
May  7 12:06:09.154: INFO: Running AfterSuite actions on node 1
May  7 12:06:09.154: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 6557.805 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 1h49m21.117379653s
Test Suite Passed
