Conformance test: not doing test setup.
I0705 15:56:45.638126    6089 e2e.go:129] Starting e2e run "395cf3f5-c566-4140-8576-603604fae17c" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1657036605 - Will randomize all specs
Will run 356 of 6971 specs

Jul  5 15:56:47.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 15:56:47.688: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  5 15:56:47.749: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Jul  5 15:56:47.819: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  5 15:56:47.819: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Jul  5 15:56:47.819: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-disk-plugin-alicloud' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'egress-filter-applier' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker-1-v1.24.2' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jul  5 15:56:47.836: INFO: e2e test version: v1.24.2
Jul  5 15:56:47.847: INFO: kube-apiserver version: v1.24.2
Jul  5 15:56:47.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 15:56:47.860: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:56:47.860: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
W0705 15:56:47.909000    6089 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul  5 15:56:47.909: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jul  5 15:56:47.927: INFO: PSP annotation exists on dry run pod: "extensions.gardener.cloud.kube-system.csi-disk-plugin-alicloud"; assuming PodSecurityPolicy is enabled
W0705 15:56:47.938584    6089 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0705 15:56:47.950579    6089 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul  5 15:56:47.965: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3018
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jul  5 15:56:48.164: INFO: Waiting up to 5m0s for pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f" in namespace "downward-api-3018" to be "Succeeded or Failed"
Jul  5 15:56:48.175: INFO: Pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.299502ms
Jul  5 15:56:50.187: INFO: Pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02333755s
Jul  5 15:56:52.199: INFO: Pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035441608s
Jul  5 15:56:54.212: INFO: Pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048201775s
STEP: Saw pod success
Jul  5 15:56:54.212: INFO: Pod "downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f" satisfied condition "Succeeded or Failed"
Jul  5 15:56:54.224: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f container dapi-container: <nil>
STEP: delete the pod
Jul  5 15:56:54.269: INFO: Waiting for pod downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f to disappear
Jul  5 15:56:54.280: INFO: Pod downward-api-f09547e6-7464-46cb-a8c8-c870a5e4e95f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jul  5 15:56:54.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3018" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":1,"skipped":4,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:56:54.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2199
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul  5 15:56:55.143: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
W0705 15:56:55.143615    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jul  5 15:56:55.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2199" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":2,"skipped":12,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:56:55.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4482
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 15:56:56.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 15:56:58.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 15:57:00.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 15, 56, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 15:57:03.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 15:57:03.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5565-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 15:57:05.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4482" for this suite.
STEP: Destroying namespace "webhook-4482-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":3,"skipped":14,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:57:05.582: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4405
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 15:57:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4405" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":4,"skipped":26,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:57:05.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-844
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul  5 15:57:08.174: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-844 PodName:var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 15:57:08.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 15:57:08.175: INFO: ExecWithOptions: Clientset creation
Jul  5 15:57:08.175: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-844/pods/var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jul  5 15:57:08.426: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-844 PodName:var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 15:57:08.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 15:57:08.426: INFO: ExecWithOptions: Clientset creation
Jul  5 15:57:08.426: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/var-expansion-844/pods/var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jul  5 15:57:09.248: INFO: Successfully updated pod "var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul  5 15:57:09.260: INFO: Deleting pod "var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9" in namespace "var-expansion-844"
Jul  5 15:57:09.272: INFO: Wait up to 5m0s for pod "var-expansion-9addeecd-4a64-4f3c-8832-632d8640c4e9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 15:57:43.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-844" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":5,"skipped":43,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:57:43.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1043
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jul  5 15:57:43.563: INFO: The status of Pod annotationupdate5d0acbec-5962-4c11-97e9-0ed13c4efac1 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 15:57:45.576: INFO: The status of Pod annotationupdate5d0acbec-5962-4c11-97e9-0ed13c4efac1 is Running (Ready = true)
Jul  5 15:57:46.177: INFO: Successfully updated pod "annotationupdate5d0acbec-5962-4c11-97e9-0ed13c4efac1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 15:57:50.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1043" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":6,"skipped":66,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:57:50.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2477
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-12fc56da-4391-4bc8-90b8-ddbea4c68c85
STEP: Creating a pod to test consume secrets
Jul  5 15:57:50.505: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18" in namespace "projected-2477" to be "Succeeded or Failed"
Jul  5 15:57:50.516: INFO: Pod "pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18": Phase="Pending", Reason="", readiness=false. Elapsed: 10.979249ms
Jul  5 15:57:52.528: INFO: Pod "pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023711782s
Jul  5 15:57:54.541: INFO: Pod "pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036346793s
STEP: Saw pod success
Jul  5 15:57:54.541: INFO: Pod "pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18" satisfied condition "Succeeded or Failed"
Jul  5 15:57:54.552: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 15:57:54.587: INFO: Waiting for pod pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18 to disappear
Jul  5 15:57:54.599: INFO: Pod pod-projected-secrets-0b8d4bb1-911e-4099-a0a7-a7fbfdf18f18 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 15:57:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2477" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":7,"skipped":75,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:57:54.632: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3552
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jul  5 15:57:54.873: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  5 15:57:56.886: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jul  5 15:57:56.925: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  5 15:57:58.938: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul  5 15:57:58.962: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 15:57:58.974: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 15:58:00.974: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 15:58:00.987: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jul  5 15:58:01.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3552" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":8,"skipped":84,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:01.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6320
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jul  5 15:58:01.244: INFO: Creating e2e-svc-a-8p4wk
Jul  5 15:58:01.260: INFO: Creating e2e-svc-b-vx5bf
Jul  5 15:58:01.275: INFO: Creating e2e-svc-c-lwcwn
STEP: deleting service collection
Jul  5 15:58:01.336: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 15:58:01.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6320" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":9,"skipped":106,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:01.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1336
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 15:58:01.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 15:58:02.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1336" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":10,"skipped":124,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:02.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5897
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 15:58:02.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f" in namespace "downward-api-5897" to be "Succeeded or Failed"
Jul  5 15:58:02.388: INFO: Pod "downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.450016ms
Jul  5 15:58:04.401: INFO: Pod "downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023833546s
Jul  5 15:58:06.413: INFO: Pod "downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036077451s
STEP: Saw pod success
Jul  5 15:58:06.413: INFO: Pod "downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f" satisfied condition "Succeeded or Failed"
Jul  5 15:58:06.424: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f container client-container: <nil>
STEP: delete the pod
Jul  5 15:58:06.458: INFO: Waiting for pod downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f to disappear
Jul  5 15:58:06.469: INFO: Pod downwardapi-volume-f6376065-ee16-4681-a5ce-a6cd817a586f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 15:58:06.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5897" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":11,"skipped":124,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:06.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3980
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 15:58:16.886: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:16.905: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:16.925: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:16.971: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:16.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:17.004: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:17.021: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:17.038: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:17.038: INFO: Lookups using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local]

Jul  5 15:58:22.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.106: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.122: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.138: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.155: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.171: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.187: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.203: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:22.203: INFO: Lookups using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local]

Jul  5 15:58:27.055: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.106: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.123: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.155: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.171: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.187: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.203: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:27.203: INFO: Lookups using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local]

Jul  5 15:58:32.054: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.102: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.118: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.134: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.150: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.167: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.183: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.198: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:32.198: INFO: Lookups using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local]

Jul  5 15:58:37.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.103: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.119: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.136: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.152: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.169: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.185: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.202: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local from pod dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9: the server could not find the requested resource (get pods dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9)
Jul  5 15:58:37.202: INFO: Lookups using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3980.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3980.svc.cluster.local jessie_udp@dns-test-service-2.dns-3980.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3980.svc.cluster.local]

Jul  5 15:58:42.205: INFO: DNS probes using dns-3980/dns-test-9a206e56-4514-41bc-b7a0-a46560eaa1a9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 15:58:42.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3980" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":12,"skipped":135,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:42.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7923
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jul  5 15:58:42.468: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 15:58:46.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7923" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":13,"skipped":139,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:46.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5367
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 15:58:47.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:47.082: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:48.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:48.116: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:49.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:49.118: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:50.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:50.116: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:51.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:51.115: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:52.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:52.115: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 15:58:53.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 15:58:53.115: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Jul  5 15:58:53.141: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jul  5 15:58:53.166: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jul  5 15:58:53.177: INFO: Observed &DaemonSet event: ADDED
Jul  5 15:58:53.177: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.177: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.177: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.177: INFO: Found daemon set daemon-set in namespace daemonsets-5367 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul  5 15:58:53.178: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jul  5 15:58:53.208: INFO: Observed &DaemonSet event: ADDED
Jul  5 15:58:53.208: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.208: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.208: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.208: INFO: Observed daemon set daemon-set in namespace daemonsets-5367 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul  5 15:58:53.209: INFO: Observed &DaemonSet event: MODIFIED
Jul  5 15:58:53.209: INFO: Found daemon set daemon-set in namespace daemonsets-5367 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jul  5 15:58:53.209: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5367, will wait for the garbage collector to delete the pods
Jul  5 15:58:53.296: INFO: Deleting DaemonSet.extensions daemon-set took: 12.752623ms
Jul  5 15:58:53.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.164965ms
Jul  5 15:58:56.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 15:58:56.909: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 15:58:56.921: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5546"},"items":null}

Jul  5 15:58:56.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5546"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 15:58:56.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5367" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":14,"skipped":142,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:57.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5606
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jul  5 15:58:57.204: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5606 cluster-info'
Jul  5 15:58:57.286: INFO: stderr: ""
Jul  5 15:58:57.286: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 15:58:57.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5606" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":15,"skipped":159,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:58:57.311: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1731
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul  5 15:58:57.513: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 15:58:57.537: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 15:58:57.549: INFO: 
Logging pods the apiserver thinks is on node izgw85sex2ooqi4ztetrj0z before test
Jul  5 15:58:57.574: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul  5 15:58:57.574: INFO: apiserver-proxy-pdxqb from kube-system started at 2022-07-05 15:47:30 +0000 UTC (2 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container proxy ready: true, restart count 0
Jul  5 15:58:57.574: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 15:58:57.574: INFO: blackbox-exporter-ccdc4b99c-5trtq from kube-system started at 2022-07-05 15:55:27 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 15:58:57.574: INFO: calico-kube-controllers-6959b48bb7-bjpsr from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul  5 15:58:57.574: INFO: calico-node-gpt27 from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 15:58:57.574: INFO: calico-node-vertical-autoscaler-5b74b8f994-rthwq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 15:58:57.574: INFO: calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 15:58:57.574: INFO: calico-typha-vertical-autoscaler-78b946fc85-59kkq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 15:58:57.574: INFO: coredns-7f49f7db48-x8h24 from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container coredns ready: true, restart count 0
Jul  5 15:58:57.574: INFO: coredns-7f49f7db48-zglbj from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container coredns ready: true, restart count 0
Jul  5 15:58:57.574: INFO: csi-disk-plugin-alicloud-8pszw from kube-system started at 2022-07-05 15:47:30 +0000 UTC (3 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 15:58:57.574: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 15:58:57.574: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 15:58:57.574: INFO: egress-filter-applier-p45dc from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 15:58:57.574: INFO: kube-proxy-worker-1-v1.24.2-pjhw2 from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 15:58:57.574: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 15:58:57.574: INFO: metrics-server-788cb89-tfl5k from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 15:58:57.574: INFO: node-exporter-p2ztl from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 15:58:57.574: INFO: node-problem-detector-fkknm from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul  5 15:58:57.574: INFO: vpn-shoot-5fcf58b56b-k9pwx from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul  5 15:58:57.574: INFO: dashboard-metrics-scraper-9c4f98cd5-sthht from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul  5 15:58:57.574: INFO: kubernetes-dashboard-55d4694cd7-fscg2 from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.574: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul  5 15:58:57.574: INFO: 
Logging pods the apiserver thinks is on node izgw8bazids4c4cxzuus22z before test
Jul  5 15:58:57.589: INFO: addons-nginx-ingress-controller-696bcbf64f-t4lqp from kube-system started at 2022-07-05 15:54:27 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul  5 15:58:57.589: INFO: apiserver-proxy-nxtmb from kube-system started at 2022-07-05 15:47:45 +0000 UTC (2 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container proxy ready: true, restart count 0
Jul  5 15:58:57.589: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 15:58:57.589: INFO: blackbox-exporter-ccdc4b99c-m6ktb from kube-system started at 2022-07-05 15:54:27 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 15:58:57.589: INFO: calico-node-4fk5s from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 15:58:57.589: INFO: calico-typha-deploy-7f646d87dc-6rg66 from kube-system started at 2022-07-05 15:48:39 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container calico-typha ready: true, restart count 0
Jul  5 15:58:57.589: INFO: csi-disk-plugin-alicloud-7czsh from kube-system started at 2022-07-05 15:47:45 +0000 UTC (3 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 15:58:57.589: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 15:58:57.589: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 15:58:57.589: INFO: egress-filter-applier-p65qk from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 15:58:57.589: INFO: kube-proxy-worker-1-v1.24.2-mxz5v from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 15:58:57.589: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 15:58:57.589: INFO: node-exporter-gjd9k from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 15:58:57.589: INFO: node-problem-detector-mg54t from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 15:58:57.589: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8c9aa7c9-1584-4614-a7b9-016ab5a31921 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8c9aa7c9-1584-4614-a7b9-016ab5a31921 off the node izgw8bazids4c4cxzuus22z
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8c9aa7c9-1584-4614-a7b9-016ab5a31921
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jul  5 15:59:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1731" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":16,"skipped":167,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:59:03.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1200
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul  5 15:59:08.575: INFO: Successfully updated pod "adopt-release-58z6r"
STEP: Checking that the Job readopts the Pod
Jul  5 15:59:08.575: INFO: Waiting up to 15m0s for pod "adopt-release-58z6r" in namespace "job-1200" to be "adopted"
Jul  5 15:59:08.587: INFO: Pod "adopt-release-58z6r": Phase="Running", Reason="", readiness=true. Elapsed: 11.279541ms
Jul  5 15:59:10.600: INFO: Pod "adopt-release-58z6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.02429274s
Jul  5 15:59:10.600: INFO: Pod "adopt-release-58z6r" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul  5 15:59:11.127: INFO: Successfully updated pod "adopt-release-58z6r"
STEP: Checking that the Job releases the Pod
Jul  5 15:59:11.128: INFO: Waiting up to 15m0s for pod "adopt-release-58z6r" in namespace "job-1200" to be "released"
Jul  5 15:59:11.139: INFO: Pod "adopt-release-58z6r": Phase="Running", Reason="", readiness=true. Elapsed: 11.370516ms
Jul  5 15:59:13.152: INFO: Pod "adopt-release-58z6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.024157579s
Jul  5 15:59:13.152: INFO: Pod "adopt-release-58z6r" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jul  5 15:59:13.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1200" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":17,"skipped":178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:59:13.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-192
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 15:59:13.423: INFO: Endpoints addresses: [10.243.17.119] , ports: [443]
Jul  5 15:59:13.423: INFO: EndpointSlices addresses: [10.243.17.119] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jul  5 15:59:13.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-192" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":18,"skipped":202,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:59:13.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2497
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jul  5 15:59:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2497" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":19,"skipped":208,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:59:23.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8512
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8512
STEP: creating service affinity-nodeport-transition in namespace services-8512
STEP: creating replication controller affinity-nodeport-transition in namespace services-8512
I0705 15:59:23.942055    6089 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8512, replica count: 3
I0705 15:59:26.993648    6089 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 15:59:29.994714    6089 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 15:59:30.038: INFO: Creating new exec pod
Jul  5 15:59:33.100: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul  5 15:59:33.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul  5 15:59:33.458: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 15:59:33.458: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.126.226 80'
Jul  5 15:59:33.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.126.226 80\nConnection to 172.28.126.226 80 port [tcp/http] succeeded!\n"
Jul  5 15:59:33.793: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 15:59:33.793: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 30060'
Jul  5 15:59:34.119: INFO: stderr: "+ nc -v -t -w 2 10.250.25.206 30060\n+ Connection to 10.250.25.206 30060 port [tcp/*] succeeded!\necho hostName\n"
Jul  5 15:59:34.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 15:59:34.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.207 30060'
Jul  5 15:59:34.418: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.207 30060\nConnection to 10.250.25.207 30060 port [tcp/*] succeeded!\n"
Jul  5 15:59:34.418: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 15:59:34.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.25.206:30060/ ; done'
Jul  5 15:59:34.891: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n"
Jul  5 15:59:34.892: INFO: stdout: "\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-hmjxw\naffinity-nodeport-transition-gdtlh"
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-hmjxw
Jul  5 15:59:34.892: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:34.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8512 exec execpod-affinity5fl25 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.25.206:30060/ ; done'
Jul  5 15:59:35.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30060/\n"
Jul  5 15:59:35.301: INFO: stdout: "\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh\naffinity-nodeport-transition-gdtlh"
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.301: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Received response from host: affinity-nodeport-transition-gdtlh
Jul  5 15:59:35.302: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8512, will wait for the garbage collector to delete the pods
Jul  5 15:59:35.390: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.442625ms
Jul  5 15:59:35.492: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.096995ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 15:59:37.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8512" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":20,"skipped":211,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 15:59:37.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5670
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul  5 16:00:17.647: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0705 16:00:17.646931    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jul  5 16:00:17.647: INFO: Deleting pod "simpletest.rc-2qjfz" in namespace "gc-5670"
Jul  5 16:00:17.661: INFO: Deleting pod "simpletest.rc-2wxr2" in namespace "gc-5670"
Jul  5 16:00:17.675: INFO: Deleting pod "simpletest.rc-48t24" in namespace "gc-5670"
Jul  5 16:00:17.689: INFO: Deleting pod "simpletest.rc-4gtxp" in namespace "gc-5670"
Jul  5 16:00:17.703: INFO: Deleting pod "simpletest.rc-4rzfg" in namespace "gc-5670"
Jul  5 16:00:17.717: INFO: Deleting pod "simpletest.rc-4s7nh" in namespace "gc-5670"
Jul  5 16:00:17.731: INFO: Deleting pod "simpletest.rc-54gdr" in namespace "gc-5670"
Jul  5 16:00:17.744: INFO: Deleting pod "simpletest.rc-5fp9k" in namespace "gc-5670"
Jul  5 16:00:17.759: INFO: Deleting pod "simpletest.rc-5hptj" in namespace "gc-5670"
Jul  5 16:00:17.772: INFO: Deleting pod "simpletest.rc-5rt5l" in namespace "gc-5670"
Jul  5 16:00:17.786: INFO: Deleting pod "simpletest.rc-5t7hw" in namespace "gc-5670"
Jul  5 16:00:17.800: INFO: Deleting pod "simpletest.rc-5x2lq" in namespace "gc-5670"
Jul  5 16:00:17.814: INFO: Deleting pod "simpletest.rc-5xrcv" in namespace "gc-5670"
Jul  5 16:00:17.828: INFO: Deleting pod "simpletest.rc-68vmg" in namespace "gc-5670"
Jul  5 16:00:17.842: INFO: Deleting pod "simpletest.rc-6b8fm" in namespace "gc-5670"
Jul  5 16:00:17.856: INFO: Deleting pod "simpletest.rc-6j45l" in namespace "gc-5670"
Jul  5 16:00:17.871: INFO: Deleting pod "simpletest.rc-6mnlt" in namespace "gc-5670"
Jul  5 16:00:17.885: INFO: Deleting pod "simpletest.rc-79n2v" in namespace "gc-5670"
Jul  5 16:00:17.900: INFO: Deleting pod "simpletest.rc-7dfp5" in namespace "gc-5670"
Jul  5 16:00:17.913: INFO: Deleting pod "simpletest.rc-7g78q" in namespace "gc-5670"
Jul  5 16:00:17.927: INFO: Deleting pod "simpletest.rc-7q65q" in namespace "gc-5670"
Jul  5 16:00:17.940: INFO: Deleting pod "simpletest.rc-7tc9v" in namespace "gc-5670"
Jul  5 16:00:17.954: INFO: Deleting pod "simpletest.rc-7x45b" in namespace "gc-5670"
Jul  5 16:00:17.967: INFO: Deleting pod "simpletest.rc-8cd88" in namespace "gc-5670"
Jul  5 16:00:17.981: INFO: Deleting pod "simpletest.rc-8f9w7" in namespace "gc-5670"
Jul  5 16:00:17.996: INFO: Deleting pod "simpletest.rc-8j8qj" in namespace "gc-5670"
Jul  5 16:00:18.010: INFO: Deleting pod "simpletest.rc-8k5rr" in namespace "gc-5670"
Jul  5 16:00:18.024: INFO: Deleting pod "simpletest.rc-8n5ch" in namespace "gc-5670"
Jul  5 16:00:18.038: INFO: Deleting pod "simpletest.rc-8vpmr" in namespace "gc-5670"
Jul  5 16:00:18.054: INFO: Deleting pod "simpletest.rc-8zc6n" in namespace "gc-5670"
Jul  5 16:00:18.068: INFO: Deleting pod "simpletest.rc-959ss" in namespace "gc-5670"
Jul  5 16:00:18.082: INFO: Deleting pod "simpletest.rc-9jxwf" in namespace "gc-5670"
Jul  5 16:00:18.099: INFO: Deleting pod "simpletest.rc-9llrh" in namespace "gc-5670"
Jul  5 16:00:18.114: INFO: Deleting pod "simpletest.rc-9pcfz" in namespace "gc-5670"
Jul  5 16:00:18.128: INFO: Deleting pod "simpletest.rc-9qs8z" in namespace "gc-5670"
Jul  5 16:00:18.142: INFO: Deleting pod "simpletest.rc-b95nd" in namespace "gc-5670"
Jul  5 16:00:18.157: INFO: Deleting pod "simpletest.rc-bbszx" in namespace "gc-5670"
Jul  5 16:00:18.170: INFO: Deleting pod "simpletest.rc-bdfkm" in namespace "gc-5670"
Jul  5 16:00:18.186: INFO: Deleting pod "simpletest.rc-bwvlw" in namespace "gc-5670"
Jul  5 16:00:18.200: INFO: Deleting pod "simpletest.rc-cr8lf" in namespace "gc-5670"
Jul  5 16:00:18.214: INFO: Deleting pod "simpletest.rc-csxp2" in namespace "gc-5670"
Jul  5 16:00:18.229: INFO: Deleting pod "simpletest.rc-cv5gx" in namespace "gc-5670"
Jul  5 16:00:18.243: INFO: Deleting pod "simpletest.rc-dnknf" in namespace "gc-5670"
Jul  5 16:00:18.259: INFO: Deleting pod "simpletest.rc-dpfrk" in namespace "gc-5670"
Jul  5 16:00:18.273: INFO: Deleting pod "simpletest.rc-f66xp" in namespace "gc-5670"
Jul  5 16:00:18.288: INFO: Deleting pod "simpletest.rc-fdqxh" in namespace "gc-5670"
Jul  5 16:00:18.302: INFO: Deleting pod "simpletest.rc-gmvv2" in namespace "gc-5670"
Jul  5 16:00:18.316: INFO: Deleting pod "simpletest.rc-h94f9" in namespace "gc-5670"
Jul  5 16:00:18.330: INFO: Deleting pod "simpletest.rc-hvb2k" in namespace "gc-5670"
Jul  5 16:00:18.344: INFO: Deleting pod "simpletest.rc-hvtlv" in namespace "gc-5670"
Jul  5 16:00:18.358: INFO: Deleting pod "simpletest.rc-hzcsh" in namespace "gc-5670"
Jul  5 16:00:18.372: INFO: Deleting pod "simpletest.rc-hzx6b" in namespace "gc-5670"
Jul  5 16:00:18.387: INFO: Deleting pod "simpletest.rc-j7llx" in namespace "gc-5670"
Jul  5 16:00:18.400: INFO: Deleting pod "simpletest.rc-j7mcc" in namespace "gc-5670"
Jul  5 16:00:18.415: INFO: Deleting pod "simpletest.rc-jqcwf" in namespace "gc-5670"
Jul  5 16:00:18.429: INFO: Deleting pod "simpletest.rc-jxkrl" in namespace "gc-5670"
Jul  5 16:00:18.443: INFO: Deleting pod "simpletest.rc-k8gdk" in namespace "gc-5670"
Jul  5 16:00:18.457: INFO: Deleting pod "simpletest.rc-k9hw6" in namespace "gc-5670"
Jul  5 16:00:18.470: INFO: Deleting pod "simpletest.rc-k9wsc" in namespace "gc-5670"
Jul  5 16:00:18.484: INFO: Deleting pod "simpletest.rc-khvtm" in namespace "gc-5670"
Jul  5 16:00:18.497: INFO: Deleting pod "simpletest.rc-kxfx4" in namespace "gc-5670"
Jul  5 16:00:18.511: INFO: Deleting pod "simpletest.rc-l9b6h" in namespace "gc-5670"
Jul  5 16:00:18.524: INFO: Deleting pod "simpletest.rc-lhvgh" in namespace "gc-5670"
Jul  5 16:00:18.538: INFO: Deleting pod "simpletest.rc-lnf2l" in namespace "gc-5670"
Jul  5 16:00:18.552: INFO: Deleting pod "simpletest.rc-lw4gq" in namespace "gc-5670"
Jul  5 16:00:18.566: INFO: Deleting pod "simpletest.rc-lztwq" in namespace "gc-5670"
Jul  5 16:00:18.580: INFO: Deleting pod "simpletest.rc-m74vl" in namespace "gc-5670"
Jul  5 16:00:18.593: INFO: Deleting pod "simpletest.rc-m79sl" in namespace "gc-5670"
Jul  5 16:00:18.614: INFO: Deleting pod "simpletest.rc-m94rj" in namespace "gc-5670"
Jul  5 16:00:18.666: INFO: Deleting pod "simpletest.rc-n86m2" in namespace "gc-5670"
Jul  5 16:00:18.713: INFO: Deleting pod "simpletest.rc-njctw" in namespace "gc-5670"
Jul  5 16:00:18.763: INFO: Deleting pod "simpletest.rc-nqqvg" in namespace "gc-5670"
Jul  5 16:00:18.813: INFO: Deleting pod "simpletest.rc-pbv2b" in namespace "gc-5670"
Jul  5 16:00:18.866: INFO: Deleting pod "simpletest.rc-pzv9m" in namespace "gc-5670"
Jul  5 16:00:18.913: INFO: Deleting pod "simpletest.rc-qlx56" in namespace "gc-5670"
Jul  5 16:00:18.963: INFO: Deleting pod "simpletest.rc-qwgc8" in namespace "gc-5670"
Jul  5 16:00:19.012: INFO: Deleting pod "simpletest.rc-rftrp" in namespace "gc-5670"
Jul  5 16:00:19.064: INFO: Deleting pod "simpletest.rc-s2f68" in namespace "gc-5670"
Jul  5 16:00:19.114: INFO: Deleting pod "simpletest.rc-s5j5k" in namespace "gc-5670"
Jul  5 16:00:19.163: INFO: Deleting pod "simpletest.rc-sg78h" in namespace "gc-5670"
Jul  5 16:00:19.214: INFO: Deleting pod "simpletest.rc-sknd7" in namespace "gc-5670"
Jul  5 16:00:19.263: INFO: Deleting pod "simpletest.rc-spxm5" in namespace "gc-5670"
Jul  5 16:00:19.313: INFO: Deleting pod "simpletest.rc-stsn7" in namespace "gc-5670"
Jul  5 16:00:19.363: INFO: Deleting pod "simpletest.rc-tc8wg" in namespace "gc-5670"
Jul  5 16:00:19.414: INFO: Deleting pod "simpletest.rc-tw4qk" in namespace "gc-5670"
Jul  5 16:00:19.463: INFO: Deleting pod "simpletest.rc-twccx" in namespace "gc-5670"
Jul  5 16:00:19.513: INFO: Deleting pod "simpletest.rc-tzclm" in namespace "gc-5670"
Jul  5 16:00:19.564: INFO: Deleting pod "simpletest.rc-v6kbh" in namespace "gc-5670"
Jul  5 16:00:19.614: INFO: Deleting pod "simpletest.rc-vcvw6" in namespace "gc-5670"
Jul  5 16:00:19.664: INFO: Deleting pod "simpletest.rc-vhqbz" in namespace "gc-5670"
Jul  5 16:00:19.714: INFO: Deleting pod "simpletest.rc-vn8r4" in namespace "gc-5670"
Jul  5 16:00:19.764: INFO: Deleting pod "simpletest.rc-wdrfb" in namespace "gc-5670"
Jul  5 16:00:19.813: INFO: Deleting pod "simpletest.rc-wfq9t" in namespace "gc-5670"
Jul  5 16:00:19.864: INFO: Deleting pod "simpletest.rc-x7hh7" in namespace "gc-5670"
Jul  5 16:00:19.913: INFO: Deleting pod "simpletest.rc-xbvv7" in namespace "gc-5670"
Jul  5 16:00:19.962: INFO: Deleting pod "simpletest.rc-zfcsr" in namespace "gc-5670"
Jul  5 16:00:20.013: INFO: Deleting pod "simpletest.rc-zljr5" in namespace "gc-5670"
Jul  5 16:00:20.064: INFO: Deleting pod "simpletest.rc-zn5dt" in namespace "gc-5670"
Jul  5 16:00:20.114: INFO: Deleting pod "simpletest.rc-zw9rd" in namespace "gc-5670"
Jul  5 16:00:20.169: INFO: Deleting pod "simpletest.rc-zzfmw" in namespace "gc-5670"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:00:20.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5670" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":21,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:20.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3875
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-341308de-cfd4-4246-9f65-9e90f5666118
STEP: Creating a pod to test consume secrets
Jul  5 16:00:20.543: INFO: Waiting up to 5m0s for pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06" in namespace "secrets-3875" to be "Succeeded or Failed"
Jul  5 16:00:20.554: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 11.249015ms
Jul  5 16:00:22.567: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024189649s
Jul  5 16:00:24.580: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036615682s
Jul  5 16:00:26.592: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048936159s
Jul  5 16:00:28.605: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.062257654s
STEP: Saw pod success
Jul  5 16:00:28.605: INFO: Pod "pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06" satisfied condition "Succeeded or Failed"
Jul  5 16:00:28.617: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 16:00:28.692: INFO: Waiting for pod pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06 to disappear
Jul  5 16:00:28.703: INFO: Pod pod-secrets-c9e4cac1-ee6e-442c-90c8-365c7327ac06 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:00:28.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3875" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":22,"skipped":259,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:28.737: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-611
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-611
STEP: creating service affinity-clusterip-transition in namespace services-611
STEP: creating replication controller affinity-clusterip-transition in namespace services-611
I0705 16:00:28.967097    6089 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-611, replica count: 3
I0705 16:00:32.019397    6089 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:00:32.041: INFO: Creating new exec pod
Jul  5 16:00:35.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-611 exec execpod-affinityd8n4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul  5 16:00:35.444: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul  5 16:00:35.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:00:35.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-611 exec execpod-affinityd8n4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.107.232 80'
Jul  5 16:00:35.775: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.26.107.232 80\nConnection to 172.26.107.232 80 port [tcp/http] succeeded!\n"
Jul  5 16:00:35.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:00:35.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-611 exec execpod-affinityd8n4b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.26.107.232:80/ ; done'
Jul  5 16:00:36.195: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n"
Jul  5 16:00:36.195: INFO: stdout: "\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-rdwl8\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-rdwl8\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-rdwl8\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-rdwl8\naffinity-clusterip-transition-n95wk\naffinity-clusterip-transition-n95wk"
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-rdwl8
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-rdwl8
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.195: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-rdwl8
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-rdwl8
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.196: INFO: Received response from host: affinity-clusterip-transition-n95wk
Jul  5 16:00:36.219: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-611 exec execpod-affinityd8n4b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.26.107.232:80/ ; done'
Jul  5 16:00:36.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.107.232:80/\n"
Jul  5 16:00:36.621: INFO: stdout: "\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g\naffinity-clusterip-transition-94r5g"
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Received response from host: affinity-clusterip-transition-94r5g
Jul  5 16:00:36.621: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-611, will wait for the garbage collector to delete the pods
Jul  5 16:00:36.713: INFO: Deleting ReplicationController affinity-clusterip-transition took: 15.682199ms
Jul  5 16:00:36.814: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.911284ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:00:38.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-611" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":23,"skipped":263,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:38.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-678
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-3a2a4073-7c01-4c6b-badb-4704620db5c3-3854
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:00:39.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-678" for this suite.
STEP: Destroying namespace "nspatchtest-3a2a4073-7c01-4c6b-badb-4704620db5c3-3854" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":24,"skipped":278,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:39.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7117
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul  5 16:00:39.484: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 16:00:39.509: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 16:00:39.520: INFO: 
Logging pods the apiserver thinks is on node izgw85sex2ooqi4ztetrj0z before test
Jul  5 16:00:39.546: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul  5 16:00:39.546: INFO: apiserver-proxy-pdxqb from kube-system started at 2022-07-05 15:47:30 +0000 UTC (2 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container proxy ready: true, restart count 0
Jul  5 16:00:39.546: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 16:00:39.546: INFO: blackbox-exporter-ccdc4b99c-5trtq from kube-system started at 2022-07-05 15:55:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 16:00:39.546: INFO: calico-kube-controllers-6959b48bb7-bjpsr from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul  5 16:00:39.546: INFO: calico-node-gpt27 from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 16:00:39.546: INFO: calico-node-vertical-autoscaler-5b74b8f994-rthwq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:00:39.546: INFO: calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:00:39.546: INFO: calico-typha-vertical-autoscaler-78b946fc85-59kkq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:00:39.546: INFO: coredns-7f49f7db48-x8h24 from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container coredns ready: true, restart count 0
Jul  5 16:00:39.546: INFO: coredns-7f49f7db48-zglbj from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container coredns ready: true, restart count 0
Jul  5 16:00:39.546: INFO: csi-disk-plugin-alicloud-8pszw from kube-system started at 2022-07-05 15:47:30 +0000 UTC (3 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 16:00:39.546: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 16:00:39.546: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 16:00:39.546: INFO: egress-filter-applier-p45dc from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 16:00:39.546: INFO: kube-proxy-worker-1-v1.24.2-pjhw2 from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 16:00:39.546: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 16:00:39.546: INFO: metrics-server-788cb89-tfl5k from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 16:00:39.546: INFO: node-exporter-p2ztl from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 16:00:39.546: INFO: node-problem-detector-fkknm from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul  5 16:00:39.546: INFO: vpn-shoot-5fcf58b56b-k9pwx from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul  5 16:00:39.546: INFO: dashboard-metrics-scraper-9c4f98cd5-sthht from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul  5 16:00:39.546: INFO: kubernetes-dashboard-55d4694cd7-fscg2 from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.546: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul  5 16:00:39.546: INFO: 
Logging pods the apiserver thinks is on node izgw8bazids4c4cxzuus22z before test
Jul  5 16:00:39.562: INFO: addons-nginx-ingress-controller-696bcbf64f-t4lqp from kube-system started at 2022-07-05 15:54:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul  5 16:00:39.562: INFO: apiserver-proxy-nxtmb from kube-system started at 2022-07-05 15:47:45 +0000 UTC (2 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container proxy ready: true, restart count 0
Jul  5 16:00:39.562: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 16:00:39.562: INFO: blackbox-exporter-ccdc4b99c-m6ktb from kube-system started at 2022-07-05 15:54:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 16:00:39.562: INFO: calico-node-4fk5s from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 16:00:39.562: INFO: calico-typha-deploy-7f646d87dc-6rg66 from kube-system started at 2022-07-05 15:48:39 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container calico-typha ready: true, restart count 0
Jul  5 16:00:39.562: INFO: csi-disk-plugin-alicloud-7czsh from kube-system started at 2022-07-05 15:47:45 +0000 UTC (3 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 16:00:39.562: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 16:00:39.562: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 16:00:39.562: INFO: egress-filter-applier-p65qk from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 16:00:39.562: INFO: kube-proxy-worker-1-v1.24.2-mxz5v from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 16:00:39.562: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 16:00:39.562: INFO: node-exporter-gjd9k from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 16:00:39.562: INFO: node-problem-detector-mg54t from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:00:39.562: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node izgw85sex2ooqi4ztetrj0z
STEP: verifying the node has the label node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod addons-nginx-ingress-controller-696bcbf64f-t4lqp requesting resource cpu=100m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j requesting resource cpu=0m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod apiserver-proxy-nxtmb requesting resource cpu=40m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod apiserver-proxy-pdxqb requesting resource cpu=40m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod blackbox-exporter-ccdc4b99c-5trtq requesting resource cpu=11m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod blackbox-exporter-ccdc4b99c-m6ktb requesting resource cpu=11m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod calico-kube-controllers-6959b48bb7-bjpsr requesting resource cpu=10m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod calico-node-4fk5s requesting resource cpu=250m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod calico-node-gpt27 requesting resource cpu=250m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod calico-node-vertical-autoscaler-5b74b8f994-rthwq requesting resource cpu=10m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod calico-typha-deploy-7f646d87dc-6rg66 requesting resource cpu=320m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc requesting resource cpu=10m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod calico-typha-vertical-autoscaler-78b946fc85-59kkq requesting resource cpu=10m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod coredns-7f49f7db48-x8h24 requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod coredns-7f49f7db48-zglbj requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod csi-disk-plugin-alicloud-7czsh requesting resource cpu=34m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod csi-disk-plugin-alicloud-8pszw requesting resource cpu=34m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod egress-filter-applier-p45dc requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod egress-filter-applier-p65qk requesting resource cpu=50m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod kube-proxy-worker-1-v1.24.2-mxz5v requesting resource cpu=34m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod kube-proxy-worker-1-v1.24.2-pjhw2 requesting resource cpu=34m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod metrics-server-788cb89-tfl5k requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod node-exporter-gjd9k requesting resource cpu=50m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod node-exporter-p2ztl requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod node-problem-detector-fkknm requesting resource cpu=20m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod node-problem-detector-mg54t requesting resource cpu=20m on Node izgw8bazids4c4cxzuus22z
Jul  5 16:00:39.655: INFO: Pod vpn-shoot-5fcf58b56b-k9pwx requesting resource cpu=100m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod dashboard-metrics-scraper-9c4f98cd5-sthht requesting resource cpu=0m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.655: INFO: Pod kubernetes-dashboard-55d4694cd7-fscg2 requesting resource cpu=50m on Node izgw85sex2ooqi4ztetrj0z
STEP: Starting Pods to consume most of the cluster CPU.
Jul  5 16:00:39.655: INFO: Creating a pod which consumes cpu=763m on Node izgw85sex2ooqi4ztetrj0z
Jul  5 16:00:39.673: INFO: Creating a pod which consumes cpu=707m on Node izgw8bazids4c4cxzuus22z
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6.16fefa39efd5395d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7117/filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6 to izgw85sex2ooqi4ztetrj0z]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6.16fefa3a11818072], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.7"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6.16fefa3a3fb3d9e0], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.7" in 775.033978ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6.16fefa3a414cd690], Reason = [Created], Message = [Created container filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6.16fefa3a44e067ed], Reason = [Started], Message = [Started container filler-pod-f48a91de-8fae-44de-ad03-bca3799dd1f6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4.16fefa39f0a8d665], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7117/filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4 to izgw8bazids4c4cxzuus22z]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4.16fefa3a1165f530], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4.16fefa3a12dcaeda], Reason = [Created], Message = [Created container filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4.16fefa3a158e6c15], Reason = [Started], Message = [Started container filler-pod-fe48b926-47b5-491a-a032-79fe87d526f4]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16fefa3a6c381895], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.]
STEP: removing the label node off the node izgw85sex2ooqi4ztetrj0z
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node izgw8bazids4c4cxzuus22z
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:00:42.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7117" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":25,"skipped":278,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:42.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4498
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-bdd70da9-f064-456a-bd6f-8f4e8499a7b1
STEP: Creating a pod to test consume configMaps
Jul  5 16:00:43.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80" in namespace "configmap-4498" to be "Succeeded or Failed"
Jul  5 16:00:43.133: INFO: Pod "pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.173972ms
Jul  5 16:00:45.145: INFO: Pod "pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80": Phase="Running", Reason="", readiness=false. Elapsed: 2.023221308s
Jul  5 16:00:47.158: INFO: Pod "pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036281547s
STEP: Saw pod success
Jul  5 16:00:47.158: INFO: Pod "pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80" satisfied condition "Succeeded or Failed"
Jul  5 16:00:47.170: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:00:47.248: INFO: Waiting for pod pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80 to disappear
Jul  5 16:00:47.259: INFO: Pod pod-configmaps-74252f15-94b9-48d1-919c-4268be05df80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:00:47.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4498" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":26,"skipped":289,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:47.297: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-288
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jul  5 16:00:47.510: INFO: created test-podtemplate-1
Jul  5 16:00:47.522: INFO: created test-podtemplate-2
Jul  5 16:00:47.534: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul  5 16:00:47.546: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul  5 16:00:47.562: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jul  5 16:00:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-288" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":27,"skipped":289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:47.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9350
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d
Jul  5 16:00:47.830: INFO: Pod name my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d: Found 1 pods out of 1
Jul  5 16:00:47.830: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d" are running
Jul  5 16:00:49.855: INFO: Pod "my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d-mng72" is running (conditions: [])
Jul  5 16:00:49.855: INFO: Trying to dial the pod
Jul  5 16:00:54.954: INFO: Controller my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d: Got expected result from replica 1 [my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d-mng72]: "my-hostname-basic-25617a3a-2a01-4f39-ace5-eb454711321d-mng72", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jul  5 16:00:54.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9350" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":28,"skipped":371,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:00:54.988: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9093
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:01:06.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9093" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":29,"skipped":385,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:06.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2373
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:01:06.543: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 16:01:08.567: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  5 16:01:10.580: INFO: Creating deployment "test-rollover-deployment"
Jul  5 16:01:10.603: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  5 16:01:12.627: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  5 16:01:12.651: INFO: Ensure that both replica sets have 1 created replica
Jul  5 16:01:12.674: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  5 16:01:12.698: INFO: Updating deployment test-rollover-deployment
Jul  5 16:01:12.698: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  5 16:01:14.724: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  5 16:01:14.747: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  5 16:01:14.770: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 16:01:14.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:01:16.794: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 16:01:16.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:01:18.796: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 16:01:18.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:01:20.794: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 16:01:20.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:01:22.795: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 16:01:22.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:01:24.795: INFO: 
Jul  5 16:01:24.795: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 16:01:24.831: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2373  3f497233-6f15-4b5f-8d59-8f94b31f48fb 7618 2 2022-07-05 16:01:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-05 16:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:01:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c52498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-05 16:01:10 +0000 UTC,LastTransitionTime:2022-07-05 16:01:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2022-07-05 16:01:23 +0000 UTC,LastTransitionTime:2022-07-05 16:01:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 16:01:24.844: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-2373  1cf1ba82-5ac5-44fc-a31e-8edf61046306 7611 2 2022-07-05 16:01:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3f497233-6f15-4b5f-8d59-8f94b31f48fb 0xc003a92c67 0xc003a92c68}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f497233-6f15-4b5f-8d59-8f94b31f48fb\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:01:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:01:24.844: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  5 16:01:24.844: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2373  51f2dc1a-db1d-4ac1-9b45-91c00039b1cc 7617 2 2022-07-05 16:01:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3f497233-6f15-4b5f-8d59-8f94b31f48fb 0xc003a92b47 0xc003a92b48}] []  [{e2e.test Update apps/v1 2022-07-05 16:01:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:01:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f497233-6f15-4b5f-8d59-8f94b31f48fb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:01:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a92c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:01:24.845: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-2373  84498107-48aa-4ee1-9ecd-f2c73c936d2b 7553 2 2022-07-05 16:01:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3f497233-6f15-4b5f-8d59-8f94b31f48fb 0xc003a92d70 0xc003a92d71}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:01:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f497233-6f15-4b5f-8d59-8f94b31f48fb\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:01:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:01:24.857: INFO: Pod "test-rollover-deployment-779c67f4f8-7cg9v" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-7cg9v test-rollover-deployment-779c67f4f8- deployment-2373  80b5a2af-3e46-4cb6-8f24-2c70e661ea00 7565 0 2022-07-05 16:01:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:1ad7627512d60aeaf665a1d75a8ab0ab9dde9ab57afd7ab74d4f3f31da5c6998 cni.projectcalico.org/podIP:172.16.1.83/32 cni.projectcalico.org/podIPs:172.16.1.83/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 1cf1ba82-5ac5-44fc-a31e-8edf61046306 0xc003c52847 0xc003c52848}] []  [{kube-controller-manager Update v1 2022-07-05 16:01:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf1ba82-5ac5-44fc-a31e-8edf61046306\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:01:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dldwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dldwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:01:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:01:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:01:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:01:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.83,StartTime:2022-07-05 16:01:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:01:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://b2f0cc1aa56ee84c890571f93baf905124e7b9418b48ad04fa8b6be25bf4b686,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 16:01:24.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2373" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":30,"skipped":405,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:24.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4700
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jul  5 16:01:27.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4700" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":31,"skipped":408,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:27.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7994
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-8c0c1c15-ee47-49bb-aea9-a708fe245b8f
STEP: Creating a pod to test consume configMaps
Jul  5 16:01:28.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d" in namespace "configmap-7994" to be "Succeeded or Failed"
Jul  5 16:01:28.151: INFO: Pod "pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.537978ms
Jul  5 16:01:30.163: INFO: Pod "pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023317721s
Jul  5 16:01:32.176: INFO: Pod "pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036401243s
STEP: Saw pod success
Jul  5 16:01:32.176: INFO: Pod "pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d" satisfied condition "Succeeded or Failed"
Jul  5 16:01:32.187: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:01:32.265: INFO: Waiting for pod pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d to disappear
Jul  5 16:01:32.276: INFO: Pod pod-configmaps-3bbe4cd5-b986-4036-bf79-013b01bd808d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:01:32.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7994" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":32,"skipped":419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:32.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6945
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:01:32.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 1, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 1, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:01:35.873: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:01:35.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:01:38.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6945" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":33,"skipped":478,"failed":0}
SSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:38.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6764
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jul  5 16:01:39.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6764" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":34,"skipped":484,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:39.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in csistoragecapacity-8128
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jul  5 16:01:39.644: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jul  5 16:01:39.735: INFO: waiting for watch events with expected annotations in namespace
Jul  5 16:01:39.735: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jul  5 16:01:39.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-8128" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":35,"skipped":500,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:39.821: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8350
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-9dde7341-3606-4b35-ad12-66a7e009addf
STEP: Creating a pod to test consume configMaps
Jul  5 16:01:40.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234" in namespace "projected-8350" to be "Succeeded or Failed"
Jul  5 16:01:40.066: INFO: Pod "pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234": Phase="Pending", Reason="", readiness=false. Elapsed: 11.380681ms
Jul  5 16:01:42.079: INFO: Pod "pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024500497s
Jul  5 16:01:44.092: INFO: Pod "pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037355344s
STEP: Saw pod success
Jul  5 16:01:44.092: INFO: Pod "pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234" satisfied condition "Succeeded or Failed"
Jul  5 16:01:44.104: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:01:44.167: INFO: Waiting for pod pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234 to disappear
Jul  5 16:01:44.179: INFO: Pod pod-projected-configmaps-1c35efd5-bfd9-4509-8ac3-950d21b86234 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:01:44.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8350" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":512,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:44.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3277
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 16:01:44.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 16:01:44.510: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:01:45.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:01:45.544: INFO: Node izgw8bazids4c4cxzuus22z is running 0 daemon pod, expected 1
Jul  5 16:01:46.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 16:01:46.545: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  5 16:01:46.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:01:46.606: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:01:47.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 16:01:47.639: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3277, will wait for the garbage collector to delete the pods
Jul  5 16:01:47.737: INFO: Deleting DaemonSet.extensions daemon-set took: 12.817162ms
Jul  5 16:01:47.838: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.667762ms
Jul  5 16:01:50.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 16:01:50.651: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 16:01:50.662: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8067"},"items":null}

Jul  5 16:01:50.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8067"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:01:50.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3277" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":37,"skipped":523,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:50.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1744
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jul  5 16:01:50.998: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:01:53.011: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jul  5 16:01:53.051: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:01:55.064: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul  5 16:01:55.089: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 16:01:55.101: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 16:01:57.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 16:01:57.114: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jul  5 16:01:57.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1744" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:01:57.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8176
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jul  5 16:06:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8176" for this suite.

• [SLOW TEST:300.294 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":39,"skipped":586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:06:57.469: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2111
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:06:57.671: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 create -f -'
Jul  5 16:06:58.557: INFO: stderr: ""
Jul  5 16:06:58.557: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul  5 16:06:58.557: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 create -f -'
Jul  5 16:06:58.770: INFO: stderr: ""
Jul  5 16:06:58.770: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  5 16:06:59.783: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:06:59.783: INFO: Found 1 / 1
Jul  5 16:06:59.783: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  5 16:06:59.795: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:06:59.795: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 16:06:59.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 describe pod agnhost-primary-7j5qj'
Jul  5 16:06:59.925: INFO: stderr: ""
Jul  5 16:06:59.925: INFO: stdout: "Name:         agnhost-primary-7j5qj\nNamespace:    kubectl-2111\nPriority:     0\nNode:         izgw8bazids4c4cxzuus22z/10.250.25.207\nStart Time:   Tue, 05 Jul 2022 16:06:58 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: b8c68fd1d3cb49fc8efb1cb60b5371175a8f8d40493518773b4a2dc69423d1da\n              cni.projectcalico.org/podIP: 172.16.1.90/32\n              cni.projectcalico.org/podIPs: 172.16.1.90/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.16.1.90\nIPs:\n  IP:           172.16.1.90\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://41688f3c6822af1a24ad4c9f9aa27239c5a4c74753f92460972f7513dcf80734\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 05 Jul 2022 16:06:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tms5g-6sg.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2zjsl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2zjsl:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2111/agnhost-primary-7j5qj to izgw8bazids4c4cxzuus22z\n  Normal  Pulled     0s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Jul  5 16:06:59.926: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 describe rc agnhost-primary'
Jul  5 16:07:00.068: INFO: stderr: ""
Jul  5 16:07:00.068: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2111\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-7j5qj\n"
Jul  5 16:07:00.068: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 describe service agnhost-primary'
Jul  5 16:07:00.167: INFO: stderr: ""
Jul  5 16:07:00.167: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2111\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.28.135.89\nIPs:               172.28.135.89\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.1.90:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  5 16:07:00.189: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 describe node izgw85sex2ooqi4ztetrj0z'
Jul  5 16:07:00.388: INFO: stderr: ""
Jul  5 16:07:00.388: INFO: stdout: "Name:               izgw85sex2ooqi4ztetrj0z\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.t6-c1m2.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=izgw85sex2ooqi4ztetrj0z\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=false\n                    node.kubernetes.io/instance-type=ecs.t6-c1m2.large\n                    node.kubernetes.io/role=node\n                    topology.diskplugin.csi.alibabacloud.com/zone=eu-central-1b\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1b\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.24.2\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 3d50222d762a435ea95f46953b747a609b4ced2963d5d558a7c1a702a367b145\n                    csi.volume.kubernetes.io/nodeid: {\"diskplugin.csi.alibabacloud.com\":\"i-gw85sex2ooqi4ztetrj0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"networking.gardener.cloud/node-local-dns-enabled\":\"false\",\"node.kubernetes.io/role\":\"node...\n                    projectcalico.org/IPv4Address: 10.250.25.206/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 05 Jul 2022 15:47:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  izgw85sex2ooqi4ztetrj0z\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 05 Jul 2022 16:06:57 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentUnregisterNetDevice   False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  CorruptDockerOverlay2         False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  KernelDeadlock                False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 05 Jul 2022 16:03:54 +0000   Tue, 05 Jul 2022 15:48:51 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  NetworkUnavailable            False   Tue, 05 Jul 2022 15:48:20 +0000   Tue, 05 Jul 2022 15:48:20 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Tue, 05 Jul 2022 16:06:50 +0000   Tue, 05 Jul 2022 15:47:25 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 05 Jul 2022 16:06:50 +0000   Tue, 05 Jul 2022 15:47:25 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 05 Jul 2022 16:06:50 +0000   Tue, 05 Jul 2022 15:47:25 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 05 Jul 2022 16:06:50 +0000   Tue, 05 Jul 2022 15:48:06 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.25.206\n  Hostname:    izgw85sex2ooqi4ztetrj0z\nCapacity:\n  cpu:                2\n  ephemeral-storage:  34941568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3836140Ki\n  pods:               110\nAllocatable:\n  cpu:                1920m\n  ephemeral-storage:  33991157324\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2685164Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c4746f1e4c33424d977ef2cb0b428f95\n  System UUID:                c4746f1e-4c33-424d-977e-f2cb0b428f95\n  Boot ID:                    f618ad66-2172-4dab-89d2-555ccdbac605\n  Kernel Version:             5.10.123-garden-cloud-amd64\n  OS Image:                   Garden Linux 576.10\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.11\n  Kubelet Version:            v1.24.2\n  Kube-Proxy Version:         v1.24.2\nPodCIDR:                      172.16.0.0/24\nPodCIDRs:                     172.16.0.0/24\nProviderID:                   eu-central-1.i-gw85sex2ooqi4ztetrj0\nNon-terminated Pods:          (19 in total)\n  Namespace                   Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 apiserver-proxy-pdxqb                                             40m (2%)      0 (0%)      40Mi (1%)        1114Mi (42%)   19m\n  kube-system                 blackbox-exporter-ccdc4b99c-5trtq                                 11m (0%)      0 (0%)      11500k (0%)      150Mi (5%)     11m\n  kube-system                 calico-kube-controllers-6959b48bb7-bjpsr                          10m (0%)      0 (0%)      50Mi (1%)        2Gi (78%)      21m\n  kube-system                 calico-node-gpt27                                                 250m (13%)    0 (0%)      100Mi (3%)       2800Mi (106%)  19m\n  kube-system                 calico-node-vertical-autoscaler-5b74b8f994-rthwq                  10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)     21m\n  kube-system                 calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc               10m (0%)      0 (0%)      50Mi (1%)        100Mi (3%)     21m\n  kube-system                 calico-typha-vertical-autoscaler-78b946fc85-59kkq                 10m (0%)      0 (0%)      50Mi (1%)        130Mi (4%)     21m\n  kube-system                 coredns-7f49f7db48-x8h24                                          50m (2%)      0 (0%)      15Mi (0%)        1500Mi (57%)   21m\n  kube-system                 coredns-7f49f7db48-zglbj                                          50m (2%)      0 (0%)      15Mi (0%)        1500Mi (57%)   20m\n  kube-system                 csi-disk-plugin-alicloud-8pszw                                    34m (1%)      0 (0%)      104Mi (3%)       1580Mi (60%)   19m\n  kube-system                 egress-filter-applier-p45dc                                       50m (2%)      100m (5%)   64Mi (2%)        256Mi (9%)     19m\n  kube-system                 kube-proxy-worker-1-v1.24.2-pjhw2                                 34m (1%)      0 (0%)      47753748 (1%)    2Gi (78%)      12m\n  kube-system                 metrics-server-788cb89-tfl5k                                      50m (2%)      0 (0%)      150Mi (5%)       1Gi (39%)      21m\n  kube-system                 node-exporter-p2ztl                                               50m (2%)      0 (0%)      50Mi (1%)        250Mi (9%)     19m\n  kube-system                 node-problem-detector-fkknm                                       20m (1%)      0 (0%)      20Mi (0%)        120Mi (4%)     19m\n  kube-system                 vpn-shoot-5fcf58b56b-k9pwx                                        100m (5%)     0 (0%)      100Mi (3%)       100Mi (3%)     21m\n  kubernetes-dashboard        dashboard-metrics-scraper-9c4f98cd5-sthht                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kubernetes-dashboard        kubernetes-dashboard-55d4694cd7-fscg2                             50m (2%)      0 (0%)      50Mi (1%)        256Mi (9%)     21m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                829m (43%)        100m (5%)\n  memory             1011360756 (36%)  15106Mi (576%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 19m                kube-proxy       \n  Normal  Starting                 12m                kube-proxy       \n  Normal  NodeHasSufficientMemory  19m (x8 over 19m)  kubelet          Node izgw85sex2ooqi4ztetrj0z status is now: NodeHasSufficientMemory\n  Normal  InitializedNode          19m                node-controller  Initialize node successfully\n  Normal  RegisteredNode           19m                node-controller  Node izgw85sex2ooqi4ztetrj0z event: Registered Node izgw85sex2ooqi4ztetrj0z in Controller\n"
Jul  5 16:07:00.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2111 describe namespace kubectl-2111'
Jul  5 16:07:00.494: INFO: stderr: ""
Jul  5 16:07:00.494: INFO: stdout: "Name:         kubectl-2111\nLabels:       e2e-framework=kubectl\n              e2e-run=395cf3f5-c566-4140-8576-603604fae17c\n              kubernetes.io/metadata.name=kubectl-2111\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:07:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2111" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":40,"skipped":638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:07:00.529: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-943
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:07:00.760: INFO: The status of Pod busybox-scheduling-e841993a-981e-452b-a1bb-68ecfa596c57 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:07:02.773: INFO: The status of Pod busybox-scheduling-e841993a-981e-452b-a1bb-68ecfa596c57 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jul  5 16:07:02.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-943" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":41,"skipped":701,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:07:02.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1335
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1335
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jul  5 16:07:03.088: INFO: Found 0 stateful pods, waiting for 3
Jul  5 16:07:13.102: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:07:13.102: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:07:13.102: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:07:13.138: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1335 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:07:13.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:07:13.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:07:13.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jul  5 16:07:13.606: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  5 16:07:13.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1335 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 16:07:14.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 16:07:14.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 16:07:14.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jul  5 16:07:34.091: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1335 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:07:34.415: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:07:34.415: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:07:34.415: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 16:07:44.501: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  5 16:07:44.536: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-1335 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 16:07:44.861: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 16:07:44.861: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 16:07:44.861: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 16:07:44.918: INFO: Waiting for StatefulSet statefulset-1335/ss2 to complete update
Jul  5 16:07:44.918: INFO: Waiting for Pod statefulset-1335/ss2-0 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
Jul  5 16:07:44.918: INFO: Waiting for Pod statefulset-1335/ss2-1 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
Jul  5 16:07:44.918: INFO: Waiting for Pod statefulset-1335/ss2-2 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 16:07:54.947: INFO: Deleting all statefulset in ns statefulset-1335
Jul  5 16:07:54.958: INFO: Scaling statefulset ss2 to 0
Jul  5 16:08:05.007: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 16:08:05.019: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 16:08:05.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1335" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":42,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:05.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-1836
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 16:08:05.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1836" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":43,"skipped":737,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:05.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-719
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:08:05.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 8, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 8, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 8, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 8, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:08:08.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:08:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-719" for this suite.
STEP: Destroying namespace "webhook-719-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":44,"skipped":737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:09.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3000
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  5 16:08:09.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3000  1a6be6fb-503e-49f7-a35f-3a3173f10751 10250 0 2022-07-05 16:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-05 16:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:08:09.610: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3000  1a6be6fb-503e-49f7-a35f-3a3173f10751 10251 0 2022-07-05 16:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-05 16:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  5 16:08:09.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3000  1a6be6fb-503e-49f7-a35f-3a3173f10751 10252 0 2022-07-05 16:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-05 16:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:08:09.657: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3000  1a6be6fb-503e-49f7-a35f-3a3173f10751 10253 0 2022-07-05 16:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-05 16:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jul  5 16:08:09.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3000" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":45,"skipped":800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:09.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4873
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:08:09.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66" in namespace "projected-4873" to be "Succeeded or Failed"
Jul  5 16:08:09.912: INFO: Pod "downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66": Phase="Pending", Reason="", readiness=false. Elapsed: 11.287029ms
Jul  5 16:08:11.925: INFO: Pod "downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66": Phase="Running", Reason="", readiness=false. Elapsed: 2.023742487s
Jul  5 16:08:13.937: INFO: Pod "downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035922105s
STEP: Saw pod success
Jul  5 16:08:13.937: INFO: Pod "downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66" satisfied condition "Succeeded or Failed"
Jul  5 16:08:13.949: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66 container client-container: <nil>
STEP: delete the pod
Jul  5 16:08:13.983: INFO: Waiting for pod downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66 to disappear
Jul  5 16:08:13.994: INFO: Pod downwardapi-volume-56f5e69e-6744-4ae3-a5be-1b68f268ec66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:08:13.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4873" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":834,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:14.028: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9025
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:08:14.258: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:08:16.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:18.272: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:20.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:22.272: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:24.272: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:26.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:28.272: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:30.270: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:32.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:34.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = false)
Jul  5 16:08:36.271: INFO: The status of Pod test-webserver-e67e8de9-ba0e-4e6b-a335-1cdd9dfab4e0 is Running (Ready = true)
Jul  5 16:08:36.282: INFO: Container started at 2022-07-05 16:08:14 +0000 UTC, pod became ready at 2022-07-05 16:08:34 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 16:08:36.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9025" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":845,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:36.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-924
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  5 16:08:36.535: INFO: Waiting up to 5m0s for pod "pod-23267f0b-e5cd-4347-ac76-2f4005e05651" in namespace "emptydir-924" to be "Succeeded or Failed"
Jul  5 16:08:36.546: INFO: Pod "pod-23267f0b-e5cd-4347-ac76-2f4005e05651": Phase="Pending", Reason="", readiness=false. Elapsed: 11.125191ms
Jul  5 16:08:38.559: INFO: Pod "pod-23267f0b-e5cd-4347-ac76-2f4005e05651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024425022s
Jul  5 16:08:40.573: INFO: Pod "pod-23267f0b-e5cd-4347-ac76-2f4005e05651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037796206s
STEP: Saw pod success
Jul  5 16:08:40.573: INFO: Pod "pod-23267f0b-e5cd-4347-ac76-2f4005e05651" satisfied condition "Succeeded or Failed"
Jul  5 16:08:40.585: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-23267f0b-e5cd-4347-ac76-2f4005e05651 container test-container: <nil>
STEP: delete the pod
Jul  5 16:08:40.630: INFO: Waiting for pod pod-23267f0b-e5cd-4347-ac76-2f4005e05651 to disappear
Jul  5 16:08:40.641: INFO: Pod pod-23267f0b-e5cd-4347-ac76-2f4005e05651 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:08:40.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-924" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":847,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:40.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1226
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-9fad3ca6-6c44-4ca8-9ce9-72e7776cd42d
STEP: Creating secret with name s-test-opt-upd-765ba72f-b692-4da1-b7d7-6d0ba1041851
STEP: Creating the pod
Jul  5 16:08:40.944: INFO: The status of Pod pod-secrets-5cb8f0ff-0199-4cc5-bc05-f41cfbf6e4a7 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:08:42.956: INFO: The status of Pod pod-secrets-5cb8f0ff-0199-4cc5-bc05-f41cfbf6e4a7 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-9fad3ca6-6c44-4ca8-9ce9-72e7776cd42d
STEP: Updating secret s-test-opt-upd-765ba72f-b692-4da1-b7d7-6d0ba1041851
STEP: Creating secret with name s-test-opt-create-b784a436-220a-4fcc-b87f-c6d52e169c4c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:08:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1226" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":851,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:08:47.232: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7758
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jul  5 16:08:47.433: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:09:47.536: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:09:47.548: INFO: Starting informer...
STEP: Starting pods...
Jul  5 16:09:47.589: INFO: Pod1 is running on izgw8bazids4c4cxzuus22z. Tainting Node
Jul  5 16:09:49.649: INFO: Pod2 is running on izgw8bazids4c4cxzuus22z. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul  5 16:09:55.852: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul  5 16:10:15.889: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:10:15.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7758" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":50,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:10:15.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7759
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 16:10:19.234: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jul  5 16:10:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7759" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":51,"skipped":892,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:10:19.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3664
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  5 16:10:19.576: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3664  2c815f06-21b5-4dcd-9578-a386418c7aca 11045 0 2022-07-05 16:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-07-05 16:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:10:19.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3664  2c815f06-21b5-4dcd-9578-a386418c7aca 11046 0 2022-07-05 16:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-07-05 16:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jul  5 16:10:19.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3664" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":52,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:10:19.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9092
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul  5 16:10:19.839: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:11:19.944: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jul  5 16:11:19.992: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul  5 16:11:20.008: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul  5 16:11:20.044: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul  5 16:11:20.061: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:11:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9092" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":53,"skipped":921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:11:32.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1977
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:11:32.568: INFO: Creating deployment "webserver-deployment"
Jul  5 16:11:32.581: INFO: Waiting for observed generation 1
Jul  5 16:11:34.604: INFO: Waiting for all required pods to come up
Jul  5 16:11:34.626: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  5 16:11:36.650: INFO: Waiting for deployment "webserver-deployment" to complete
Jul  5 16:11:36.673: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul  5 16:11:36.697: INFO: Updating deployment webserver-deployment
Jul  5 16:11:36.697: INFO: Waiting for observed generation 2
Jul  5 16:11:38.720: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  5 16:11:38.731: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  5 16:11:38.743: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul  5 16:11:38.777: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  5 16:11:38.777: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  5 16:11:38.788: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul  5 16:11:38.811: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul  5 16:11:38.811: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul  5 16:11:38.835: INFO: Updating deployment webserver-deployment
Jul  5 16:11:38.835: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul  5 16:11:38.858: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  5 16:11:40.883: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 16:11:40.907: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1977  7a13d5ae-d796-4480-94d1-23d60740bfcb 11757 3 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030c57e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:11,UnavailableReplicas:22,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-07-05 16:11:38 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-07-05 16:11:40 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,},},ReadyReplicas:11,CollisionCount:nil,},}

Jul  5 16:11:40.919: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-1977  70d48fd5-3dab-425b-852a-a8497c25d7c4 11716 3 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7a13d5ae-d796-4480-94d1-23d60740bfcb 0xc0039116c7 0xc0039116c8}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a13d5ae-d796-4480-94d1-23d60740bfcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003911768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:11:40.919: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul  5 16:11:40.919: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-1977  99331129-271a-4494-aee3-016a1d5a2c64 11759 3 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7a13d5ae-d796-4480-94d1-23d60740bfcb 0xc0039115d7 0xc0039115d8}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a13d5ae-d796-4480-94d1-23d60740bfcb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003911668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:12,AvailableReplicas:12,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:11:40.962: INFO: Pod "webserver-deployment-55df494869-2vcjd" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2vcjd webserver-deployment-55df494869- deployment-1977  ed06ef75-f119-4748-a48c-6a370af96124 11752 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:b5f784f3373f6673eaf02ad7f42a65fb7f0e84676873af2e1d5de73eabb2bb0c cni.projectcalico.org/podIP:172.16.0.104/32 cni.projectcalico.org/podIPs:172.16.0.104/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc0030c5c17 0xc0030c5c18}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hjhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hjhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.104,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://675a29d39048835bbf1715413e6638016eb8054bfee8dad41a03778e281f42cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.962: INFO: Pod "webserver-deployment-55df494869-59lkt" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-59lkt webserver-deployment-55df494869- deployment-1977  89cc804c-ff58-4331-9d0a-2b7185e096a2 11556 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e19c6742caa852151800b93e3e415fce73f8feacfd4380da43af10f601223000 cni.projectcalico.org/podIP:172.16.1.110/32 cni.projectcalico.org/podIPs:172.16.1.110/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc0030c5e37 0xc0030c5e38}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7q5fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7q5fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.110,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://72d9e5cc194ab065eef0c61d6e730de4875e759847ff2963428aae355c911038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.962: INFO: Pod "webserver-deployment-55df494869-5b5md" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-5b5md webserver-deployment-55df494869- deployment-1977  ad7c8dd6-9052-48e1-b162-4dee0d807a95 11755 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:428ccbacbea958e4c402eda4c7aa9a81cf31b04690abcc8a80d28fc88fcab0af cni.projectcalico.org/podIP:172.16.0.101/32 cni.projectcalico.org/podIPs:172.16.0.101/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385a057 0xc00385a058}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66xf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66xf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.101,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://76109d44071353e3308068d83de73bc1c39170c14ba90fe8e63c34ffb3339370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-654rf" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-654rf webserver-deployment-55df494869- deployment-1977  13c5e645-8715-43a9-8cec-b88f5c549509 11758 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:9f02a320765724632c8e75ce4ad4d863eba4ba697e319d74fbc5c2ff39672fd3 cni.projectcalico.org/podIP:172.16.0.97/32 cni.projectcalico.org/podIPs:172.16.0.97/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385a277 0xc00385a278}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxwfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxwfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.97,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6d2c9623b307e30a453b0d87a383434b539c5fefe43b3f3a2eaaa89ad1af1e92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-7c4ld" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7c4ld webserver-deployment-55df494869- deployment-1977  65e3e251-866f-4e0a-abfe-1dcbe5551c60 11719 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:dbb6c96eb2f9cbb28a367bacfdc669ed9a7b42dd0ecb2843ff4b66e516275894 cni.projectcalico.org/podIP:172.16.1.115/32 cni.projectcalico.org/podIPs:172.16.1.115/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385a490 0xc00385a491}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vg8x4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vg8x4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-866lx" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-866lx webserver-deployment-55df494869- deployment-1977  abe3b8aa-9ee8-40fc-a21d-2ab813fe1b4f 11740 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f5aa2e4986a4b69281675c5dc095befba2a72a9e83cf074d9f36542520a19ad1 cni.projectcalico.org/podIP:172.16.1.124/32 cni.projectcalico.org/podIPs:172.16.1.124/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385a687 0xc00385a688}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvlfn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvlfn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-88vs6" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-88vs6 webserver-deployment-55df494869- deployment-1977  342da502-d375-454c-a411-1fa24b14459d 11749 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:5d935b22136905379cbdb5accdc5c34f6abb4d4cf8ca449cee06fabf3dfc7c99 cni.projectcalico.org/podIP:172.16.0.99/32 cni.projectcalico.org/podIPs:172.16.0.99/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385a887 0xc00385a888}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mw2mp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw2mp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.99,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1ce987ce093f88c9e4811ada5c69ff0d9051b6eb50c663ea993612084dd56449,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-8lgdn" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8lgdn webserver-deployment-55df494869- deployment-1977  2e1a5796-dbc2-4439-892a-4027728daf27 11537 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:80d8c75bda0272d97815ca54b6df347fcf02af69ac54f186745539eb72db7595 cni.projectcalico.org/podIP:172.16.0.88/32 cni.projectcalico.org/podIPs:172.16.0.88/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385aaa0 0xc00385aaa1}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8q2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8q2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.88,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d5fb892beece03671b1936de2ae80fb19e59b5f6f99b820d6e36f18ed496e95b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.963: INFO: Pod "webserver-deployment-55df494869-b7hj2" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-b7hj2 webserver-deployment-55df494869- deployment-1977  2edfbff9-eed8-4908-aae8-d99c398d9830 11725 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:d130a4e911e38fd6a65a12a8e472a24931027eb77a28a61a23de711c05b2e006 cni.projectcalico.org/podIP:172.16.1.117/32 cni.projectcalico.org/podIPs:172.16.1.117/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385acb0 0xc00385acb1}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86nqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86nqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-dbmxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dbmxb webserver-deployment-55df494869- deployment-1977  088b1a95-2715-44e9-a3c1-9cacfd9b7589 11731 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e69c7abd2ef776c2aaf7465614d63769e461b4d584592bb61fa417b735175b5b cni.projectcalico.org/podIP:172.16.1.121/32 cni.projectcalico.org/podIPs:172.16.1.121/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385aea7 0xc00385aea8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctn7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctn7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-dnsfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dnsfs webserver-deployment-55df494869- deployment-1977  a00c96f8-2d15-41cf-bc10-14467852b9c4 11733 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c5ef7eb16b3fb6b090d87d8843277cd667df89952c28b2847a9fcc101c698ba2 cni.projectcalico.org/podIP:172.16.1.119/32 cni.projectcalico.org/podIPs:172.16.1.119/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385b0a7 0xc00385b0a8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxllh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxllh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-fc9rt" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-fc9rt webserver-deployment-55df494869- deployment-1977  c32fc31b-3eab-4ff0-a7c4-0c5494ec60f4 11540 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c822c07fa48a43b7403ae6dc075eb81ce8eb21540aa2f3d90d1f14280d569886 cni.projectcalico.org/podIP:172.16.0.92/32 cni.projectcalico.org/podIPs:172.16.0.92/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385b2a7 0xc00385b2a8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bg56w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bg56w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.92,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9eebecfa9fe8d00d331517fdf275024555bb925bb6247510577238bbc29f9be3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-lst7m" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-lst7m webserver-deployment-55df494869- deployment-1977  481d0c6e-4f8c-4aad-bfd1-a8f4d0bc8ec4 11544 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:2ed1dfeaa2fa8af2bd23d271219556f42a9fedabd1a0e439eb7b09f76a4c11ae cni.projectcalico.org/podIP:172.16.1.108/32 cni.projectcalico.org/podIPs:172.16.1.108/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385b4c0 0xc00385b4c1}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6xmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6xmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.108,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f9af4808318a935fe806b8d43e16d5387307de8467f3b373cdd401da7e8597e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-m5j9g" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-m5j9g webserver-deployment-55df494869- deployment-1977  58a4cdd8-9b2b-4130-b2e6-7a0e31a2ffca 11561 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:6b0dedea6604261a86993a657b0ec6b54edc197c09e4a21da6a3bc13d3e53abf cni.projectcalico.org/podIP:172.16.0.89/32 cni.projectcalico.org/podIPs:172.16.0.89/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385b6d7 0xc00385b6d8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dx6nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dx6nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.89,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://856e0bd8c9c913bc72d4a1b09cba91fb477bdc1d28dab43d1e08ac3c1cdc6c41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.964: INFO: Pod "webserver-deployment-55df494869-n5hgb" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-n5hgb webserver-deployment-55df494869- deployment-1977  d8bc43ed-8675-4a60-9a9f-0a746e952567 11564 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:8f3cc3971cafd13774e479010b2f93f46e082f15afe3d147206b98901cd70f45 cni.projectcalico.org/podIP:172.16.0.91/32 cni.projectcalico.org/podIPs:172.16.0.91/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385b8f0 0xc00385b8f1}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mpb5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mpb5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.91,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3108c74afea94eea3b494dbc04c0a7288e157736d68bc616750f51a94e67dc9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.965: INFO: Pod "webserver-deployment-55df494869-rk422" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rk422 webserver-deployment-55df494869- deployment-1977  31201b42-0a56-4d64-a8e3-a5742ff6f32c 11738 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:622b23888c4c7413fe274992c1db932325462f116a442ccca2dd2ad6b16f8ea1 cni.projectcalico.org/podIP:172.16.1.122/32 cni.projectcalico.org/podIPs:172.16.1.122/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385bb00 0xc00385bb01}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chdxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chdxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.965: INFO: Pod "webserver-deployment-55df494869-rwgk6" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rwgk6 webserver-deployment-55df494869- deployment-1977  38d97a03-83b4-4260-b6f7-c8584d2dbdd3 11534 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:a2639528345843c0333b8fd568cb0569b392d0ae2f5d9899c4ab8f1b6dc03025 cni.projectcalico.org/podIP:172.16.0.90/32 cni.projectcalico.org/podIPs:172.16.0.90/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385bcf7 0xc00385bcf8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dx9x9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dx9x9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.90,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://014baa0bfbd9aa8ccf172445d544bcfd21adc8459b9719685e33f8051adb0d8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.970: INFO: Pod "webserver-deployment-55df494869-x24np" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-x24np webserver-deployment-55df494869- deployment-1977  7cb5f4d9-c1ea-4a09-95e7-5c69a708e62a 11553 0 2022-07-05 16:11:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:0d945aa6927b052898b454a1cc3652e944198e7c82d657b3a66b8a57c20e150e cni.projectcalico.org/podIP:172.16.1.107/32 cni.projectcalico.org/podIPs:172.16.1.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc00385bf10 0xc00385bf11}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmdrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmdrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.107,StartTime:2022-07-05 16:11:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://446a5bd07a41aee56baa2fcce17759a90e54cd2a9e4ee5e72c3d267bc43d1807,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.970: INFO: Pod "webserver-deployment-55df494869-z2w8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-z2w8p webserver-deployment-55df494869- deployment-1977  a7ca7a0e-727c-46de-9c60-a3ec8e7bca05 11727 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e58861cd0e7ac595e934f858a2266999f00b8c5c48d3fcf7019334e38d34df14 cni.projectcalico.org/podIP:172.16.1.118/32 cni.projectcalico.org/podIPs:172.16.1.118/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc003a34127 0xc003a34128}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m84lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m84lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.970: INFO: Pod "webserver-deployment-55df494869-zx9xm" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-zx9xm webserver-deployment-55df494869- deployment-1977  640e1231-f99c-4b8c-b790-0fc590f9d6d4 11761 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c6b36600863b2a93ba5efc66f7d81acbb99e0b96a1dd9c49b73b9a437125fa53 cni.projectcalico.org/podIP:172.16.0.98/32 cni.projectcalico.org/podIPs:172.16.0.98/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 99331129-271a-4494-aee3-016a1d5a2c64 0xc003a34327 0xc003a34328}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99331129-271a-4494-aee3-016a1d5a2c64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ww2lm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ww2lm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.98,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:11:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c07cde05a1e18efd7f6a3dc4419b7f065d989826f9ebc659a7c250bb90e321f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.971: INFO: Pod "webserver-deployment-57ccb67bb8-2wsfk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-2wsfk webserver-deployment-57ccb67bb8- deployment-1977  f2491205-28c3-4eb8-9323-a20c06a1f247 11732 0 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:e8c16937fcafd01210ff2c3207f7efd4ec5161267f03b44c31095673e035fe10 cni.projectcalico.org/podIP:172.16.0.93/32 cni.projectcalico.org/podIPs:172.16.0.93/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a34540 0xc003a34541}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lh6rq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lh6rq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.93,StartTime:2022-07-05 16:11:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.971: INFO: Pod "webserver-deployment-57ccb67bb8-6qp4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-6qp4x webserver-deployment-57ccb67bb8- deployment-1977  469fba57-bab1-4c49-940c-e41f2c65e772 11736 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:a7794ac78d0cf020eed6b8f2a73900a8b43e3f8943257e9dd7950dd2c7b124c4 cni.projectcalico.org/podIP:172.16.1.123/32 cni.projectcalico.org/podIPs:172.16.1.123/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a34780 0xc003a34781}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjldb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjldb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.972: INFO: Pod "webserver-deployment-57ccb67bb8-74z5p" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-74z5p webserver-deployment-57ccb67bb8- deployment-1977  6d796dcf-1efa-4d37-a687-6edaacb1f9aa 11742 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:a61f705662702a1dbee19310af5f09b8611fdf845e19d03e7122de123b00e9f3 cni.projectcalico.org/podIP:172.16.0.103/32 cni.projectcalico.org/podIPs:172.16.0.103/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a34997 0xc003a34998}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cxsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cxsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.972: INFO: Pod "webserver-deployment-57ccb67bb8-92gmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-92gmt webserver-deployment-57ccb67bb8- deployment-1977  22dd97e0-61ee-4462-9028-411864222a8f 11715 0 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:2cc69930c4835c3307a2b19cfbd403114c19dae96b0bb2d2460a510cdd1b0864 cni.projectcalico.org/podIP:172.16.1.112/32 cni.projectcalico.org/podIPs:172.16.1.112/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a34bb7 0xc003a34bb8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5zsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5zsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.112,StartTime:2022-07-05 16:11:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.972: INFO: Pod "webserver-deployment-57ccb67bb8-f8968" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-f8968 webserver-deployment-57ccb67bb8- deployment-1977  a1f369c2-9727-41df-bd2d-cdd8f5c947a8 11730 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:e9df8321bb8f3e4626087a1e181a1f105ab81feeabc11c7283c467478b6e6176 cni.projectcalico.org/podIP:172.16.1.120/32 cni.projectcalico.org/podIPs:172.16.1.120/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a34e07 0xc003a34e08}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q68bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q68bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.972: INFO: Pod "webserver-deployment-57ccb67bb8-fn4xp" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-fn4xp webserver-deployment-57ccb67bb8- deployment-1977  2de5fb9d-2559-49a8-9352-eb77acf34a93 11718 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:9ddc90a13d2c6be87a2914460528301252c381cbd96545808398531c8af8e940 cni.projectcalico.org/podIP:172.16.0.95/32 cni.projectcalico.org/podIPs:172.16.0.95/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35027 0xc003a35028}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jjm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jjm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.972: INFO: Pod "webserver-deployment-57ccb67bb8-lrz7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-lrz7z webserver-deployment-57ccb67bb8- deployment-1977  8c92d837-bc75-4e06-9992-e2ce3b8d884b 11645 0 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:870eed543e0360a66cb4a0b9e5ef49dd18e89db09a398226115791a629dad6dc cni.projectcalico.org/podIP:172.16.0.94/32 cni.projectcalico.org/podIPs:172.16.0.94/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35247 0xc003a35248}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrpw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrpw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:172.16.0.94,StartTime:2022-07-05 16:11:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.0.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-q8pdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-q8pdq webserver-deployment-57ccb67bb8- deployment-1977  080d948f-a694-416d-b816-daedcceba72a 11616 0 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:547e004b87554b2456449e505a4528bfa9af8fc2cceacd2f889524aa2f0de27f cni.projectcalico.org/podIP:172.16.1.114/32 cni.projectcalico.org/podIPs:172.16.1.114/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35490 0xc003a35491}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5xc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5xc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-qsp92" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-qsp92 webserver-deployment-57ccb67bb8- deployment-1977  31ba7f93-2c03-49b0-b03a-9f39852d623d 11741 0 2022-07-05 16:11:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:1ab5f6bae6ea56f5b34e4605c5dab4a064021ee2c8584e0d4517224c98d3db64 cni.projectcalico.org/podIP:172.16.1.113/32 cni.projectcalico.org/podIPs:172.16.1.113/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a356a7 0xc003a356a8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 16:11:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 16:11:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-29qhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-29qhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.113,StartTime:2022-07-05 16:11:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-s5tlt" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-s5tlt webserver-deployment-57ccb67bb8- deployment-1977  31b1569e-e1cb-4c8d-9848-002875025acc 11723 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:19612a6f00218eec42c8a318aac58ba9f27123fc0b118a3244e3a754ceaa800f cni.projectcalico.org/podIP:172.16.0.96/32 cni.projectcalico.org/podIPs:172.16.0.96/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a358f7 0xc003a358f8}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsvkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsvkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-sdk9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-sdk9v webserver-deployment-57ccb67bb8- deployment-1977  14d1943b-03da-4e91-b6e1-3a129e6f52c6 11734 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:b2c6a5f3a152ed5bd455dcd3ae34cabdc351eaaf52f792e36d4a324964a0438b cni.projectcalico.org/podIP:172.16.0.102/32 cni.projectcalico.org/podIPs:172.16.0.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35b17 0xc003a35b18}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgv6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgv6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-vgj2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-vgj2g webserver-deployment-57ccb67bb8- deployment-1977  11a7d64b-4f2e-40d6-9fcc-d9b45bc76924 11735 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f111bea9035fdb39418aecad31bd7e718c4a84272f421a3c85638bc5d19182fb cni.projectcalico.org/podIP:172.16.0.100/32 cni.projectcalico.org/podIPs:172.16.0.100/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35d37 0xc003a35d38}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5vp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5vp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:11:40.973: INFO: Pod "webserver-deployment-57ccb67bb8-wx2gj" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wx2gj webserver-deployment-57ccb67bb8- deployment-1977  4da1d354-72f0-44c5-a201-a452b0cc6513 11724 0 2022-07-05 16:11:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:086dcb5811249200994430cf374211474aa1752df23f3856e75e4a51b172837d cni.projectcalico.org/podIP:172.16.1.116/32 cni.projectcalico.org/podIPs:172.16.1.116/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 70d48fd5-3dab-425b-852a-a8497c25d7c4 0xc003a35f57 0xc003a35f58}] []  [{kube-controller-manager Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70d48fd5-3dab-425b-852a-a8497c25d7c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:11:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-07-05 16:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74rtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74rtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:11:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:11:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 16:11:40.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1977" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":54,"skipped":943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:11:41.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6465
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:11:41.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  5 16:11:41.235: INFO: The status of Pod pod-logs-websocket-9ce38193-7f14-44e8-a7a7-c878dde754de is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:11:43.249: INFO: The status of Pod pod-logs-websocket-9ce38193-7f14-44e8-a7a7-c878dde754de is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 16:11:43.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6465" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":974,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:11:43.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5004
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-c26a3f57-5ec4-4654-95b0-7fc2121d40ce
STEP: Creating a pod to test consume configMaps
Jul  5 16:11:43.567: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94" in namespace "projected-5004" to be "Succeeded or Failed"
Jul  5 16:11:43.578: INFO: Pod "pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94": Phase="Pending", Reason="", readiness=false. Elapsed: 11.171219ms
Jul  5 16:11:45.590: INFO: Pod "pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023609032s
Jul  5 16:11:47.603: INFO: Pod "pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036702363s
STEP: Saw pod success
Jul  5 16:11:47.603: INFO: Pod "pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94" satisfied condition "Succeeded or Failed"
Jul  5 16:11:47.615: INFO: Trying to get logs from node izgw85sex2ooqi4ztetrj0z pod pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:11:47.696: INFO: Waiting for pod pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94 to disappear
Jul  5 16:11:47.707: INFO: Pod pod-projected-configmaps-e40efb1c-33e8-47a7-a267-a655d2adea94 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:11:47.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5004" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":56,"skipped":988,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:11:47.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1018
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jul  5 16:11:47.961: INFO: Waiting up to 5m0s for pod "downward-api-af69a06f-e7cf-4889-924d-136371683181" in namespace "downward-api-1018" to be "Succeeded or Failed"
Jul  5 16:11:47.972: INFO: Pod "downward-api-af69a06f-e7cf-4889-924d-136371683181": Phase="Pending", Reason="", readiness=false. Elapsed: 11.375816ms
Jul  5 16:11:49.985: INFO: Pod "downward-api-af69a06f-e7cf-4889-924d-136371683181": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02420066s
Jul  5 16:11:51.997: INFO: Pod "downward-api-af69a06f-e7cf-4889-924d-136371683181": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036120753s
STEP: Saw pod success
Jul  5 16:11:51.997: INFO: Pod "downward-api-af69a06f-e7cf-4889-924d-136371683181" satisfied condition "Succeeded or Failed"
Jul  5 16:11:52.008: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downward-api-af69a06f-e7cf-4889-924d-136371683181 container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:11:52.051: INFO: Waiting for pod downward-api-af69a06f-e7cf-4889-924d-136371683181 to disappear
Jul  5 16:11:52.062: INFO: Pod downward-api-af69a06f-e7cf-4889-924d-136371683181 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jul  5 16:11:52.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1018" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":57,"skipped":1003,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:11:52.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5936
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:12:20.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5936" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":58,"skipped":1017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:20.431: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6863
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:12:20.633: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:12:22.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6863" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":59,"skipped":1054,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:22.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6546
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:12:22.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a" in namespace "projected-6546" to be "Succeeded or Failed"
Jul  5 16:12:22.634: INFO: Pod "downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.59328ms
Jul  5 16:12:24.647: INFO: Pod "downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024223384s
Jul  5 16:12:26.659: INFO: Pod "downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036642224s
STEP: Saw pod success
Jul  5 16:12:26.660: INFO: Pod "downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a" satisfied condition "Succeeded or Failed"
Jul  5 16:12:26.671: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a container client-container: <nil>
STEP: delete the pod
Jul  5 16:12:26.714: INFO: Waiting for pod downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a to disappear
Jul  5 16:12:26.725: INFO: Pod downwardapi-volume-446551ff-37c5-411f-b5f9-e6a9ee829e3a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:12:26.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6546" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":1055,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:26.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4889
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-4889/secret-test-2edda91f-a042-4200-8e61-019949b672e0
STEP: Creating a pod to test consume secrets
Jul  5 16:12:26.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81" in namespace "secrets-4889" to be "Succeeded or Failed"
Jul  5 16:12:27.001: INFO: Pod "pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 11.223138ms
Jul  5 16:12:29.014: INFO: Pod "pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024265341s
Jul  5 16:12:31.027: INFO: Pod "pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037186888s
STEP: Saw pod success
Jul  5 16:12:31.027: INFO: Pod "pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81" satisfied condition "Succeeded or Failed"
Jul  5 16:12:31.039: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81 container env-test: <nil>
STEP: delete the pod
Jul  5 16:12:31.073: INFO: Waiting for pod pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81 to disappear
Jul  5 16:12:31.084: INFO: Pod pod-configmaps-50fb5cdf-51ef-4d20-8773-1d8890f1eb81 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:12:31.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4889" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:31.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6673
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  5 16:12:31.338: INFO: Waiting up to 5m0s for pod "pod-b4424cee-566f-494e-89cc-67d382ae6127" in namespace "emptydir-6673" to be "Succeeded or Failed"
Jul  5 16:12:31.349: INFO: Pod "pod-b4424cee-566f-494e-89cc-67d382ae6127": Phase="Pending", Reason="", readiness=false. Elapsed: 11.353945ms
Jul  5 16:12:33.362: INFO: Pod "pod-b4424cee-566f-494e-89cc-67d382ae6127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024062561s
Jul  5 16:12:35.374: INFO: Pod "pod-b4424cee-566f-494e-89cc-67d382ae6127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036366655s
STEP: Saw pod success
Jul  5 16:12:35.374: INFO: Pod "pod-b4424cee-566f-494e-89cc-67d382ae6127" satisfied condition "Succeeded or Failed"
Jul  5 16:12:35.386: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-b4424cee-566f-494e-89cc-67d382ae6127 container test-container: <nil>
STEP: delete the pod
Jul  5 16:12:35.422: INFO: Waiting for pod pod-b4424cee-566f-494e-89cc-67d382ae6127 to disappear
Jul  5 16:12:35.433: INFO: Pod pod-b4424cee-566f-494e-89cc-67d382ae6127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:12:35.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6673" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":62,"skipped":1156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:35.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9278
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9f4ac90e-79e4-4452-9194-c662838ae63e
STEP: Creating the pod
Jul  5 16:12:35.721: INFO: The status of Pod pod-projected-configmaps-1c102e21-6e76-4d43-88cc-9ad73c56a747 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:12:37.734: INFO: The status of Pod pod-projected-configmaps-1c102e21-6e76-4d43-88cc-9ad73c56a747 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-9f4ac90e-79e4-4452-9194-c662838ae63e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:12:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9278" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:39.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5487
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:12:40.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392" in namespace "projected-5487" to be "Succeeded or Failed"
Jul  5 16:12:40.084: INFO: Pod "downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392": Phase="Pending", Reason="", readiness=false. Elapsed: 11.560768ms
Jul  5 16:12:42.097: INFO: Pod "downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024330591s
Jul  5 16:12:44.109: INFO: Pod "downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036801784s
STEP: Saw pod success
Jul  5 16:12:44.109: INFO: Pod "downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392" satisfied condition "Succeeded or Failed"
Jul  5 16:12:44.121: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392 container client-container: <nil>
STEP: delete the pod
Jul  5 16:12:44.155: INFO: Waiting for pod downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392 to disappear
Jul  5 16:12:44.166: INFO: Pod downwardapi-volume-94737cb7-2978-49b8-80be-e7df85cfd392 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:12:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5487" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":1218,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:44.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2038
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:12:44.436: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jul  5 16:12:46.483: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jul  5 16:12:46.506: INFO: observed ReplicaSet test-rs in namespace replicaset-2038 with ReadyReplicas 1, AvailableReplicas 1
Jul  5 16:12:46.506: INFO: observed ReplicaSet test-rs in namespace replicaset-2038 with ReadyReplicas 1, AvailableReplicas 1
Jul  5 16:12:46.511: INFO: observed ReplicaSet test-rs in namespace replicaset-2038 with ReadyReplicas 1, AvailableReplicas 1
Jul  5 16:12:46.513: INFO: observed ReplicaSet test-rs in namespace replicaset-2038 with ReadyReplicas 1, AvailableReplicas 1
Jul  5 16:12:47.275: INFO: observed ReplicaSet test-rs in namespace replicaset-2038 with ReadyReplicas 2, AvailableReplicas 2
Jul  5 16:12:48.096: INFO: observed Replicaset test-rs in namespace replicaset-2038 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 16:12:48.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2038" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":65,"skipped":1230,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:48.130: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5514
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jul  5 16:12:52.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5514" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":66,"skipped":1235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:52.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3705
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:12:52.606: INFO: Creating deployment "test-recreate-deployment"
Jul  5 16:12:52.624: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  5 16:12:52.648: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  5 16:12:52.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 12, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 12, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 12, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 12, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-845d658455\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:12:54.672: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  5 16:12:54.696: INFO: Updating deployment test-recreate-deployment
Jul  5 16:12:54.696: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 16:12:54.737: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3705  b760f28c-eaa2-4d4a-b199-fda136c79c3e 12530 2 2022-07-05 16:12:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a30d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-07-05 16:12:54 +0000 UTC,LastTransitionTime:2022-07-05 16:12:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-07-05 16:12:54 +0000 UTC,LastTransitionTime:2022-07-05 16:12:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul  5 16:12:54.749: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-3705  35894a2d-f64d-4242-bea9-e66c282dba8b 12529 1 2022-07-05 16:12:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b760f28c-eaa2-4d4a-b199-fda136c79c3e 0xc001a31220 0xc001a31221}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b760f28c-eaa2-4d4a-b199-fda136c79c3e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a312b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:12:54.749: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  5 16:12:54.749: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-3705  85581225-adea-4b9b-806b-f458814d2339 12522 2 2022-07-05 16:12:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b760f28c-eaa2-4d4a-b199-fda136c79c3e 0xc001a31107 0xc001a31108}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b760f28c-eaa2-4d4a-b199-fda136c79c3e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a311b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:12:54.761: INFO: Pod "test-recreate-deployment-cd8586fc7-8s2k9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-8s2k9 test-recreate-deployment-cd8586fc7- deployment-3705  42929d35-0aae-4c3e-b1f3-549e4a9bcda5 12531 0 2022-07-05 16:12:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 35894a2d-f64d-4242-bea9-e66c282dba8b 0xc002f12f40 0xc002f12f41}] []  [{kube-controller-manager Update v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35894a2d-f64d-4242-bea9-e66c282dba8b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:12:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr97c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr97c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:12:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:12:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:12:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:12:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:,StartTime:2022-07-05 16:12:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 16:12:54.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3705" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":67,"skipped":1273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:54.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6677
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jul  5 16:12:55.013: INFO: Waiting up to 5m0s for pod "var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864" in namespace "var-expansion-6677" to be "Succeeded or Failed"
Jul  5 16:12:55.024: INFO: Pod "var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864": Phase="Pending", Reason="", readiness=false. Elapsed: 11.239132ms
Jul  5 16:12:57.037: INFO: Pod "var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024236644s
Jul  5 16:12:59.050: INFO: Pod "var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036795197s
STEP: Saw pod success
Jul  5 16:12:59.050: INFO: Pod "var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864" satisfied condition "Succeeded or Failed"
Jul  5 16:12:59.071: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864 container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:12:59.106: INFO: Waiting for pod var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864 to disappear
Jul  5 16:12:59.118: INFO: Pod var-expansion-4772ff2d-a8cc-4538-beb2-48cea2058864 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 16:12:59.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6677" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":68,"skipped":1317,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:12:59.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2525
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-2525
STEP: creating replication controller nodeport-test in namespace services-2525
I0705 16:12:59.388894    6089 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2525, replica count: 2
I0705 16:13:02.441626    6089 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:13:02.441: INFO: Creating new exec pod
Jul  5 16:13:05.503: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  5 16:13:05.894: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  5 16:13:05.894: INFO: stdout: ""
Jul  5 16:13:06.895: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  5 16:13:07.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  5 16:13:07.206: INFO: stdout: ""
Jul  5 16:13:07.895: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  5 16:13:08.224: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  5 16:13:08.224: INFO: stdout: ""
Jul  5 16:13:08.895: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul  5 16:13:09.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul  5 16:13:09.217: INFO: stdout: "nodeport-test-qn7j9"
Jul  5 16:13:09.217: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.206.124 80'
Jul  5 16:13:09.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.206.124 80\nConnection to 172.31.206.124 80 port [tcp/http] succeeded!\n"
Jul  5 16:13:09.505: INFO: stdout: "nodeport-test-qn7j9"
Jul  5 16:13:09.505: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 32300'
Jul  5 16:13:09.858: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 32300\nConnection to 10.250.25.206 32300 port [tcp/*] succeeded!\n"
Jul  5 16:13:09.858: INFO: stdout: "nodeport-test-qn7j9"
Jul  5 16:13:09.858: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2525 exec execpodbfzcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.207 32300'
Jul  5 16:13:10.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.207 32300\nConnection to 10.250.25.207 32300 port [tcp/*] succeeded!\n"
Jul  5 16:13:10.208: INFO: stdout: "nodeport-test-qn7j9"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:13:10.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2525" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":69,"skipped":1335,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:10.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9326
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:13:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9326" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":70,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:26.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5362
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:13:28.916: INFO: Deleting pod "var-expansion-92e59c27-ba4e-484a-8338-d9c3324bbbb1" in namespace "var-expansion-5362"
Jul  5 16:13:28.929: INFO: Wait up to 5m0s for pod "var-expansion-92e59c27-ba4e-484a-8338-d9c3324bbbb1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 16:13:32.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5362" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":71,"skipped":1424,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:32.988: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-1056
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1056
STEP: Deleting pre-stop pod
Jul  5 16:13:42.406: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jul  5 16:13:42.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1056" for this suite.
•{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":72,"skipped":1436,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:42.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9117
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:13:42.676: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3cf99b38-62d4-446a-99cc-c8815fd8a228" in namespace "security-context-test-9117" to be "Succeeded or Failed"
Jul  5 16:13:42.687: INFO: Pod "busybox-user-65534-3cf99b38-62d4-446a-99cc-c8815fd8a228": Phase="Pending", Reason="", readiness=false. Elapsed: 11.027617ms
Jul  5 16:13:44.699: INFO: Pod "busybox-user-65534-3cf99b38-62d4-446a-99cc-c8815fd8a228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023264047s
Jul  5 16:13:46.711: INFO: Pod "busybox-user-65534-3cf99b38-62d4-446a-99cc-c8815fd8a228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03557378s
Jul  5 16:13:46.712: INFO: Pod "busybox-user-65534-3cf99b38-62d4-446a-99cc-c8815fd8a228" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 16:13:46.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9117" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":73,"skipped":1438,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:46.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:13:46.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22" in namespace "downward-api-4399" to be "Succeeded or Failed"
Jul  5 16:13:46.984: INFO: Pod "downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22": Phase="Pending", Reason="", readiness=false. Elapsed: 18.357534ms
Jul  5 16:13:48.997: INFO: Pod "downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031009383s
Jul  5 16:13:51.008: INFO: Pod "downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042764494s
STEP: Saw pod success
Jul  5 16:13:51.009: INFO: Pod "downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22" satisfied condition "Succeeded or Failed"
Jul  5 16:13:51.020: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22 container client-container: <nil>
STEP: delete the pod
Jul  5 16:13:51.055: INFO: Waiting for pod downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22 to disappear
Jul  5 16:13:51.066: INFO: Pod downwardapi-volume-46ff26c6-08a8-4cf4-8929-c5ec94eece22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:13:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4399" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":74,"skipped":1453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:13:51.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6838
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6838
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-6838
Jul  5 16:13:51.338: INFO: Found 0 stateful pods, waiting for 1
Jul  5 16:14:01.352: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 16:14:01.438: INFO: Deleting all statefulset in ns statefulset-6838
Jul  5 16:14:01.450: INFO: Scaling statefulset ss to 0
Jul  5 16:14:11.498: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 16:14:11.510: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 16:14:11.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6838" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":75,"skipped":1503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:11.584: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9529
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-4d08d569-4b1b-45bb-8044-7cf2450a5766
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:14:11.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9529" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":76,"skipped":1525,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:11.821: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6694
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:14:12.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 14, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 14, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 14, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:14:15.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:14:15.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:14:18.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6694" for this suite.
STEP: Destroying namespace "webhook-6694-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":77,"skipped":1529,"failed":0}
S
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:18.522: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9803
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul  5 16:14:18.743: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9803  f7a16802-a277-4cd5-bac6-d87359f2ad11 13267 0 2022-07-05 16:14:18 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-07-05 16:14:18 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4s2lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4s2lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 16:14:18.754: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:14:20.766: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul  5 16:14:20.766: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9803 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:14:20.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:14:20.767: INFO: ExecWithOptions: Clientset creation
Jul  5 16:14:20.767: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-9803/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jul  5 16:14:21.080: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9803 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:14:21.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:14:21.081: INFO: ExecWithOptions: Clientset creation
Jul  5 16:14:21.081: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/dns-9803/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul  5 16:14:21.389: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 16:14:21.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9803" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":78,"skipped":1530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:21.438: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3850
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 16:14:21.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 16:14:21.720: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:14:22.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:14:22.754: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:14:23.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 16:14:23.753: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  5 16:14:23.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:14:23.813: INFO: Node izgw8bazids4c4cxzuus22z is running 0 daemon pod, expected 1
Jul  5 16:14:24.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:14:24.846: INFO: Node izgw8bazids4c4cxzuus22z is running 0 daemon pod, expected 1
Jul  5 16:14:25.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:14:25.848: INFO: Node izgw8bazids4c4cxzuus22z is running 0 daemon pod, expected 1
Jul  5 16:14:26.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 16:14:26.848: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3850, will wait for the garbage collector to delete the pods
Jul  5 16:14:26.934: INFO: Deleting DaemonSet.extensions daemon-set took: 12.818208ms
Jul  5 16:14:27.035: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.690674ms
Jul  5 16:14:29.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 16:14:29.547: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 16:14:29.558: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13405"},"items":null}

Jul  5 16:14:29.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13405"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:14:29.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3850" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":79,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:29.639: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4497
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:14:30.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 14, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 14, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 14, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 14, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:14:33.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:14:33.284: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3036-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:14:36.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4497" for this suite.
STEP: Destroying namespace "webhook-4497-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":80,"skipped":1591,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:14:36.369: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7766
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul  5 16:14:36.603: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:15:36.706: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jul  5 16:15:36.755: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul  5 16:15:36.771: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul  5 16:15:36.805: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul  5 16:15:36.823: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:15:44.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7766" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":81,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:15:45.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8968
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jul  5 16:15:45.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8968" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":82,"skipped":1662,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:15:45.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3319
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:15:45.604: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca" in namespace "security-context-test-3319" to be "Succeeded or Failed"
Jul  5 16:15:45.615: INFO: Pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca": Phase="Pending", Reason="", readiness=false. Elapsed: 11.041697ms
Jul  5 16:15:47.628: INFO: Pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023807338s
Jul  5 16:15:49.641: INFO: Pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036244178s
Jul  5 16:15:49.641: INFO: Pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca" satisfied condition "Succeeded or Failed"
Jul  5 16:15:49.661: INFO: Got logs for pod "busybox-privileged-false-bae33428-b119-4b2d-bd11-bc6a35214dca": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 16:15:49.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3319" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:15:49.694: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-624
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:15:50.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 15, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 15, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 15, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:15:53.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul  5 16:15:55.516: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-624 attach --namespace=webhook-624 to-be-attached-pod -i -c=container1'
Jul  5 16:15:55.724: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:15:55.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-624" for this suite.
STEP: Destroying namespace "webhook-624-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":84,"skipped":1706,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:15:55.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1269
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jul  5 16:15:56.055: INFO: Waiting up to 5m0s for pod "test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c" in namespace "svcaccounts-1269" to be "Succeeded or Failed"
Jul  5 16:15:56.066: INFO: Pod "test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893181ms
Jul  5 16:15:58.078: INFO: Pod "test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c": Phase="Running", Reason="", readiness=false. Elapsed: 2.023044752s
Jul  5 16:16:00.091: INFO: Pod "test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0364577s
STEP: Saw pod success
Jul  5 16:16:00.092: INFO: Pod "test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c" satisfied condition "Succeeded or Failed"
Jul  5 16:16:00.103: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:16:00.138: INFO: Waiting for pod test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c to disappear
Jul  5 16:16:00.150: INFO: Pod test-pod-db6ad115-6f00-438b-ab10-cbe6dc67722c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 16:16:00.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1269" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":85,"skipped":1723,"failed":0}

------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:16:00.184: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3956
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 16:16:04.472: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jul  5 16:16:04.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3956" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":86,"skipped":1723,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:16:04.531: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6311
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:16:15.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6311" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":87,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:16:15.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8076
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jul  5 16:22:00.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8076" for this suite.

• [SLOW TEST:344.317 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":88,"skipped":1770,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:00.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1544
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-63d30681-f769-4e05-b090-5e76cc91db31
STEP: Creating a pod to test consume configMaps
Jul  5 16:22:00.476: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237" in namespace "projected-1544" to be "Succeeded or Failed"
Jul  5 16:22:00.487: INFO: Pod "pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237": Phase="Pending", Reason="", readiness=false. Elapsed: 11.205368ms
Jul  5 16:22:02.499: INFO: Pod "pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023407219s
Jul  5 16:22:04.513: INFO: Pod "pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036970322s
STEP: Saw pod success
Jul  5 16:22:04.513: INFO: Pod "pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237" satisfied condition "Succeeded or Failed"
Jul  5 16:22:04.528: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:22:04.572: INFO: Waiting for pod pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237 to disappear
Jul  5 16:22:04.585: INFO: Pod pod-projected-configmaps-3863b8dd-037f-4c1f-8061-9a40b1885237 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:22:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1544" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1773,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:04.618: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-169
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-4d6613d3-d31a-4c11-98ea-41e17ba73553
STEP: Creating a pod to test consume configMaps
Jul  5 16:22:04.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7" in namespace "configmap-169" to be "Succeeded or Failed"
Jul  5 16:22:04.860: INFO: Pod "pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.05706ms
Jul  5 16:22:06.873: INFO: Pod "pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023570166s
Jul  5 16:22:08.885: INFO: Pod "pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035567704s
STEP: Saw pod success
Jul  5 16:22:08.885: INFO: Pod "pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7" satisfied condition "Succeeded or Failed"
Jul  5 16:22:08.901: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:22:08.936: INFO: Waiting for pod pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7 to disappear
Jul  5 16:22:08.947: INFO: Pod pod-configmaps-51d3c8a5-d942-41ee-9f99-3df347f100e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:22:08.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-169" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:08.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6045
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-e6483c01-74e0-4623-bf5b-46db2eed4dba
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:22:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6045" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":91,"skipped":1824,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:09.216: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8368
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  5 16:22:09.436: INFO: Waiting up to 5m0s for pod "pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97" in namespace "emptydir-8368" to be "Succeeded or Failed"
Jul  5 16:22:09.450: INFO: Pod "pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97": Phase="Pending", Reason="", readiness=false. Elapsed: 13.462623ms
Jul  5 16:22:11.528: INFO: Pod "pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091100804s
Jul  5 16:22:13.541: INFO: Pod "pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104207642s
STEP: Saw pod success
Jul  5 16:22:13.541: INFO: Pod "pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97" satisfied condition "Succeeded or Failed"
Jul  5 16:22:13.553: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97 container test-container: <nil>
STEP: delete the pod
Jul  5 16:22:13.589: INFO: Waiting for pod pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97 to disappear
Jul  5 16:22:13.601: INFO: Pod pod-ee78368e-de56-4241-bb0a-c20d0a6b5f97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:22:13.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8368" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:13.636: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4409
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4409
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 16:22:13.839: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  5 16:22:13.909: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:22:15.921: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:22:17.921: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:22:19.921: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:22:21.922: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:22:23.922: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:22:25.922: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  5 16:22:25.944: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul  5 16:22:28.008: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul  5 16:22:28.008: INFO: Breadth first check of 172.16.0.115 on host 10.250.25.206...
Jul  5 16:22:28.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.162:9080/dial?request=hostname&protocol=http&host=172.16.0.115&port=8083&tries=1'] Namespace:pod-network-test-4409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:22:28.020: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:22:28.020: INFO: ExecWithOptions: Clientset creation
Jul  5 16:22:28.021: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.1.162%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.0.115%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul  5 16:22:28.280: INFO: Waiting for responses: map[]
Jul  5 16:22:28.280: INFO: reached 172.16.0.115 after 0/1 tries
Jul  5 16:22:28.281: INFO: Breadth first check of 172.16.1.161 on host 10.250.25.207...
Jul  5 16:22:28.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.162:9080/dial?request=hostname&protocol=http&host=172.16.1.161&port=8083&tries=1'] Namespace:pod-network-test-4409 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:22:28.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:22:28.293: INFO: ExecWithOptions: Clientset creation
Jul  5 16:22:28.293: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4409/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.1.162%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.1.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul  5 16:22:28.567: INFO: Waiting for responses: map[]
Jul  5 16:22:28.567: INFO: reached 172.16.1.161 after 0/1 tries
Jul  5 16:22:28.567: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jul  5 16:22:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4409" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":1871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:28.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6764
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jul  5 16:22:28.805: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 create -f -'
Jul  5 16:22:29.670: INFO: stderr: ""
Jul  5 16:22:29.670: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 16:22:29.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:22:29.780: INFO: stderr: ""
Jul  5 16:22:29.780: INFO: stdout: "update-demo-nautilus-dkqw5 update-demo-nautilus-wmwcp "
Jul  5 16:22:29.780: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:29.895: INFO: stderr: ""
Jul  5 16:22:29.895: INFO: stdout: ""
Jul  5 16:22:29.895: INFO: update-demo-nautilus-dkqw5 is created but not running
Jul  5 16:22:34.897: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:22:35.007: INFO: stderr: ""
Jul  5 16:22:35.007: INFO: stdout: "update-demo-nautilus-dkqw5 update-demo-nautilus-wmwcp "
Jul  5 16:22:35.007: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:35.121: INFO: stderr: ""
Jul  5 16:22:35.121: INFO: stdout: ""
Jul  5 16:22:35.122: INFO: update-demo-nautilus-dkqw5 is created but not running
Jul  5 16:22:40.125: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:22:40.255: INFO: stderr: ""
Jul  5 16:22:40.255: INFO: stdout: "update-demo-nautilus-dkqw5 update-demo-nautilus-wmwcp "
Jul  5 16:22:40.255: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:40.373: INFO: stderr: ""
Jul  5 16:22:40.373: INFO: stdout: "true"
Jul  5 16:22:40.373: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:22:40.479: INFO: stderr: ""
Jul  5 16:22:40.479: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:22:40.479: INFO: validating pod update-demo-nautilus-dkqw5
Jul  5 16:22:40.507: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:22:40.507: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:22:40.507: INFO: update-demo-nautilus-dkqw5 is verified up and running
Jul  5 16:22:40.507: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-wmwcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:40.627: INFO: stderr: ""
Jul  5 16:22:40.627: INFO: stdout: "true"
Jul  5 16:22:40.627: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-wmwcp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:22:40.742: INFO: stderr: ""
Jul  5 16:22:40.742: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:22:40.742: INFO: validating pod update-demo-nautilus-wmwcp
Jul  5 16:22:40.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:22:40.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:22:40.854: INFO: update-demo-nautilus-wmwcp is verified up and running
STEP: scaling down the replication controller
Jul  5 16:22:40.856: INFO: scanned /root for discovery docs: <nil>
Jul  5 16:22:40.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul  5 16:22:41.967: INFO: stderr: ""
Jul  5 16:22:41.967: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 16:22:41.967: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:22:42.066: INFO: stderr: ""
Jul  5 16:22:42.066: INFO: stdout: "update-demo-nautilus-dkqw5 "
Jul  5 16:22:42.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:42.180: INFO: stderr: ""
Jul  5 16:22:42.180: INFO: stdout: "true"
Jul  5 16:22:42.180: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:22:42.295: INFO: stderr: ""
Jul  5 16:22:42.295: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:22:42.295: INFO: validating pod update-demo-nautilus-dkqw5
Jul  5 16:22:42.362: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:22:42.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:22:42.362: INFO: update-demo-nautilus-dkqw5 is verified up and running
STEP: scaling up the replication controller
Jul  5 16:22:42.364: INFO: scanned /root for discovery docs: <nil>
Jul  5 16:22:42.364: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul  5 16:22:43.520: INFO: stderr: ""
Jul  5 16:22:43.520: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 16:22:43.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:22:43.642: INFO: stderr: ""
Jul  5 16:22:43.642: INFO: stdout: "update-demo-nautilus-dkqw5 update-demo-nautilus-gw2fq "
Jul  5 16:22:43.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:43.735: INFO: stderr: ""
Jul  5 16:22:43.736: INFO: stdout: "true"
Jul  5 16:22:43.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-dkqw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:22:43.847: INFO: stderr: ""
Jul  5 16:22:43.847: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:22:43.847: INFO: validating pod update-demo-nautilus-dkqw5
Jul  5 16:22:43.910: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:22:43.910: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:22:43.910: INFO: update-demo-nautilus-dkqw5 is verified up and running
Jul  5 16:22:43.910: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-gw2fq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:22:44.000: INFO: stderr: ""
Jul  5 16:22:44.000: INFO: stdout: "true"
Jul  5 16:22:44.000: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods update-demo-nautilus-gw2fq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:22:44.098: INFO: stderr: ""
Jul  5 16:22:44.098: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:22:44.098: INFO: validating pod update-demo-nautilus-gw2fq
Jul  5 16:22:44.126: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:22:44.126: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:22:44.126: INFO: update-demo-nautilus-gw2fq is verified up and running
STEP: using delete to clean up resources
Jul  5 16:22:44.126: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 delete --grace-period=0 --force -f -'
Jul  5 16:22:44.246: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:22:44.246: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  5 16:22:44.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get rc,svc -l name=update-demo --no-headers'
Jul  5 16:22:44.345: INFO: stderr: "No resources found in kubectl-6764 namespace.\n"
Jul  5 16:22:44.345: INFO: stdout: ""
Jul  5 16:22:44.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6764 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 16:22:44.459: INFO: stderr: ""
Jul  5 16:22:44.459: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:22:44.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6764" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":94,"skipped":1896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:44.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  5 16:22:44.794: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  5 16:22:44.816: INFO: starting watch
STEP: patching
STEP: updating
Jul  5 16:22:44.863: INFO: waiting for watch events with expected annotations
Jul  5 16:22:44.863: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jul  5 16:22:44.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9056" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":95,"skipped":1922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:44.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6252
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:22:45.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109" in namespace "downward-api-6252" to be "Succeeded or Failed"
Jul  5 16:22:45.228: INFO: Pod "downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109": Phase="Pending", Reason="", readiness=false. Elapsed: 11.330295ms
Jul  5 16:22:47.241: INFO: Pod "downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023939193s
Jul  5 16:22:49.253: INFO: Pod "downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035815858s
STEP: Saw pod success
Jul  5 16:22:49.253: INFO: Pod "downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109" satisfied condition "Succeeded or Failed"
Jul  5 16:22:49.264: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109 container client-container: <nil>
STEP: delete the pod
Jul  5 16:22:49.298: INFO: Waiting for pod downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109 to disappear
Jul  5 16:22:49.309: INFO: Pod downwardapi-volume-88ad74f5-714c-47e8-af90-596c4872f109 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:22:49.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6252" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:49.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2148
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 16:22:49.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2148" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":97,"skipped":1981,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:49.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9179
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul  5 16:22:49.880: INFO: observed Pod pod-test in namespace pods-9179 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul  5 16:22:49.880: INFO: observed Pod pod-test in namespace pods-9179 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  }]
Jul  5 16:22:49.890: INFO: observed Pod pod-test in namespace pods-9179 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  }]
Jul  5 16:22:50.318: INFO: observed Pod pod-test in namespace pods-9179 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  }]
Jul  5 16:22:51.435: INFO: Found Pod pod-test in namespace pods-9179 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:22:49 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul  5 16:22:51.532: INFO: observed event type MODIFIED
Jul  5 16:22:53.441: INFO: observed event type MODIFIED
Jul  5 16:22:53.592: INFO: observed event type MODIFIED
Jul  5 16:22:54.443: INFO: observed event type MODIFIED
Jul  5 16:22:54.451: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 16:22:54.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9179" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":98,"skipped":1988,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:54.497: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8054
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:22:54.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08" in namespace "projected-8054" to be "Succeeded or Failed"
Jul  5 16:22:54.724: INFO: Pod "downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.997058ms
Jul  5 16:22:56.736: INFO: Pod "downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023163981s
Jul  5 16:22:58.749: INFO: Pod "downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035715684s
STEP: Saw pod success
Jul  5 16:22:58.749: INFO: Pod "downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08" satisfied condition "Succeeded or Failed"
Jul  5 16:22:58.761: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08 container client-container: <nil>
STEP: delete the pod
Jul  5 16:22:58.795: INFO: Waiting for pod downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08 to disappear
Jul  5 16:22:58.807: INFO: Pod downwardapi-volume-83db531f-10e0-4779-963a-0e9ee9862d08 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:22:58.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8054" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":99,"skipped":1989,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:22:58.842: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9424
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jul  5 16:22:59.045: INFO: namespace kubectl-9424
Jul  5 16:22:59.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9424 create -f -'
Jul  5 16:22:59.264: INFO: stderr: ""
Jul  5 16:22:59.264: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  5 16:23:00.276: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:23:00.276: INFO: Found 0 / 1
Jul  5 16:23:01.277: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:23:01.278: INFO: Found 1 / 1
Jul  5 16:23:01.278: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  5 16:23:01.289: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:23:01.289: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 16:23:01.289: INFO: wait on agnhost-primary startup in kubectl-9424 
Jul  5 16:23:01.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9424 logs agnhost-primary-mct5r agnhost-primary'
Jul  5 16:23:01.392: INFO: stderr: ""
Jul  5 16:23:01.392: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul  5 16:23:01.392: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9424 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul  5 16:23:01.522: INFO: stderr: ""
Jul  5 16:23:01.522: INFO: stdout: "service/rm2 exposed\n"
Jul  5 16:23:01.533: INFO: Service rm2 in namespace kubectl-9424 found.
STEP: exposing service
Jul  5 16:23:03.558: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9424 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul  5 16:23:03.648: INFO: stderr: ""
Jul  5 16:23:03.648: INFO: stdout: "service/rm3 exposed\n"
Jul  5 16:23:03.659: INFO: Service rm3 in namespace kubectl-9424 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:23:05.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9424" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":100,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:23:05.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3620
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jul  5 16:23:05.916: INFO: PodSpec: initContainers in spec.initContainers
Jul  5 16:23:53.556: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-383c01f1-d7f3-420b-b3b2-cd23aa8a0cf7", GenerateName:"", Namespace:"init-container-3620", SelfLink:"", UID:"f93b8522-4fcc-4e0f-bcef-2040db6bae21", ResourceVersion:"16732", Generation:0, CreationTimestamp:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"916283800"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"630542daf74cba7b811ecb3b1229841f74e207bf9436d925782a294d3313a717", "cni.projectcalico.org/podIP":"172.16.1.169/32", "cni.projectcalico.org/podIPs":"172.16.1.169/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002eb31e8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.July, 5, 16, 23, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002eb3218), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.July, 5, 16, 23, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002eb3248), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n97tm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003447cc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n97tm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n97tm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n97tm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a8d980), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"izgw8bazids4c4cxzuus22z", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026992d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a8da00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a8da20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a8da28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a8da2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002d368c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.25.207", PodIP:"172.16.1.169", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.1.169"}}, StartTime:time.Date(2022, time.July, 5, 16, 23, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e5c000)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000e5c070)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://3a7f3939026da9df8b46e39aed099ac5fe37b5447196eda84c5343aa570fcbd1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ee8020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ee8000), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc00155806f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 16:23:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3620" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":101,"skipped":2024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:23:53.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3946
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:23:53.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5" in namespace "downward-api-3946" to be "Succeeded or Failed"
Jul  5 16:23:53.825: INFO: Pod "downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.358356ms
Jul  5 16:23:55.838: INFO: Pod "downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024108761s
Jul  5 16:23:57.850: INFO: Pod "downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036451317s
STEP: Saw pod success
Jul  5 16:23:57.850: INFO: Pod "downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5" satisfied condition "Succeeded or Failed"
Jul  5 16:23:57.861: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5 container client-container: <nil>
STEP: delete the pod
Jul  5 16:23:57.897: INFO: Waiting for pod downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5 to disappear
Jul  5 16:23:57.908: INFO: Pod downwardapi-volume-256149bd-79b1-4b1a-b9c1-73a8001f81f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:23:57.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3946" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":102,"skipped":2061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:23:57.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4017
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4017
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 16:23:58.145: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  5 16:23:58.216: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:24:00.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:24:02.228: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:24:04.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:24:06.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:24:08.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:24:10.229: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  5 16:24:10.252: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul  5 16:24:12.352: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul  5 16:24:12.352: INFO: Going to poll 172.16.0.117 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul  5 16:24:12.363: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.117 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4017 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:24:12.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:24:12.364: INFO: ExecWithOptions: Clientset creation
Jul  5 16:24:12.364: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4017/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.0.117+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul  5 16:24:13.581: INFO: Found all 1 expected endpoints: [netserver-0]
Jul  5 16:24:13.581: INFO: Going to poll 172.16.1.171 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul  5 16:24:13.593: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.1.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4017 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:24:13.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:24:13.593: INFO: ExecWithOptions: Clientset creation
Jul  5 16:24:13.593: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-4017/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.1.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul  5 16:24:14.869: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jul  5 16:24:14.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4017" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":103,"skipped":2098,"failed":0}
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:24:14.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4443
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jul  5 16:24:15.106: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:25:15.205: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:25:15.217: INFO: Starting informer...
STEP: Starting pod...
Jul  5 16:25:15.247: INFO: Pod is running on izgw8bazids4c4cxzuus22z. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul  5 16:25:15.287: INFO: Pod wasn't evicted. Proceeding
Jul  5 16:25:15.287: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul  5 16:26:30.329: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:26:30.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4443" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":104,"skipped":2105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:30.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7958
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7958
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7958
STEP: creating replication controller externalsvc in namespace services-7958
I0705 16:26:30.607568    6089 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7958, replica count: 2
I0705 16:26:33.658780    6089 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul  5 16:26:33.700: INFO: Creating new exec pod
Jul  5 16:26:35.740: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7958 exec execpodgvx4d -- /bin/sh -x -c nslookup clusterip-service.services-7958.svc.cluster.local'
Jul  5 16:26:36.117: INFO: stderr: "+ nslookup clusterip-service.services-7958.svc.cluster.local\n"
Jul  5 16:26:36.117: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nclusterip-service.services-7958.svc.cluster.local\tcanonical name = externalsvc.services-7958.svc.cluster.local.\nName:\texternalsvc.services-7958.svc.cluster.local\nAddress: 172.31.21.254\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7958, will wait for the garbage collector to delete the pods
Jul  5 16:26:36.192: INFO: Deleting ReplicationController externalsvc took: 12.504419ms
Jul  5 16:26:36.292: INFO: Terminating ReplicationController externalsvc pods took: 100.666524ms
Jul  5 16:26:37.909: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:26:37.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7958" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":105,"skipped":2145,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:37.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3975
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:26:38.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jul  5 16:26:41.980: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3975 --namespace=crd-publish-openapi-3975 create -f -'
Jul  5 16:26:43.167: INFO: stderr: ""
Jul  5 16:26:43.167: INFO: stdout: "e2e-test-crd-publish-openapi-7326-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul  5 16:26:43.167: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3975 --namespace=crd-publish-openapi-3975 delete e2e-test-crd-publish-openapi-7326-crds test-cr'
Jul  5 16:26:43.304: INFO: stderr: ""
Jul  5 16:26:43.304: INFO: stdout: "e2e-test-crd-publish-openapi-7326-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul  5 16:26:43.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3975 --namespace=crd-publish-openapi-3975 apply -f -'
Jul  5 16:26:43.580: INFO: stderr: ""
Jul  5 16:26:43.580: INFO: stdout: "e2e-test-crd-publish-openapi-7326-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul  5 16:26:43.580: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3975 --namespace=crd-publish-openapi-3975 delete e2e-test-crd-publish-openapi-7326-crds test-cr'
Jul  5 16:26:43.715: INFO: stderr: ""
Jul  5 16:26:43.715: INFO: stdout: "e2e-test-crd-publish-openapi-7326-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul  5 16:26:43.715: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3975 explain e2e-test-crd-publish-openapi-7326-crds'
Jul  5 16:26:43.969: INFO: stderr: ""
Jul  5 16:26:43.969: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7326-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:26:47.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3975" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":106,"skipped":2146,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:47.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6351
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jul  5 16:26:49.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6351" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":107,"skipped":2157,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:49.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-173
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:26:50.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-173 version'
Jul  5 16:26:50.240: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jul  5 16:26:50.240: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.2\", GitCommit:\"f66044f4361b9f1f96f0053dd46cb7dce5e990a8\", GitTreeState:\"clean\", BuildDate:\"2022-06-15T14:22:29Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.2\", GitCommit:\"f66044f4361b9f1f96f0053dd46cb7dce5e990a8\", GitTreeState:\"clean\", BuildDate:\"2022-06-15T14:15:38Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:26:50.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-173" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":108,"skipped":2177,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:50.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2081
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-76af9c2d-f6ea-453c-a8cd-83ae1d3274aa
STEP: Creating configMap with name cm-test-opt-upd-636b5d58-d94f-4a9a-b93a-e9ed92b8d4e8
STEP: Creating the pod
Jul  5 16:26:50.457: INFO: The status of Pod pod-configmaps-37377116-f69b-446f-89ed-59c2f8cfd3dc is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:26:52.465: INFO: The status of Pod pod-configmaps-37377116-f69b-446f-89ed-59c2f8cfd3dc is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-76af9c2d-f6ea-453c-a8cd-83ae1d3274aa
STEP: Updating configmap cm-test-opt-upd-636b5d58-d94f-4a9a-b93a-e9ed92b8d4e8
STEP: Creating configMap with name cm-test-opt-create-5e015c66-58b9-4cf1-8df1-8517dcdc9ead
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:26:54.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2081" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":2180,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:26:54.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6607
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9981
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1028
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:27:08.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6607" for this suite.
STEP: Destroying namespace "nsdeletetest-9981" for this suite.
Jul  5 16:27:08.242: INFO: Namespace nsdeletetest-9981 was already deleted
STEP: Destroying namespace "nsdeletetest-1028" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":110,"skipped":2194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:08.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6778
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:27:09.124: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:27:12.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:27:12.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6778" for this suite.
STEP: Destroying namespace "webhook-6778-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":111,"skipped":2217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:12.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6256
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:27:12.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6256" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":112,"skipped":2254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:12.639: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7263
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jul  5 16:27:12.811: INFO: Waiting up to 5m0s for pod "downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f" in namespace "downward-api-7263" to be "Succeeded or Failed"
Jul  5 16:27:12.817: INFO: Pod "downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654022ms
Jul  5 16:27:14.825: INFO: Pod "downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014804945s
Jul  5 16:27:16.834: INFO: Pod "downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02324225s
STEP: Saw pod success
Jul  5 16:27:16.834: INFO: Pod "downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f" satisfied condition "Succeeded or Failed"
Jul  5 16:27:16.841: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:27:16.875: INFO: Waiting for pod downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f to disappear
Jul  5 16:27:16.882: INFO: Pod downward-api-20071b2b-3af1-412c-8ebd-cd445bf7fb6f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jul  5 16:27:16.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7263" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":113,"skipped":2287,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:16.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8017
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  5 16:27:17.081: INFO: Waiting up to 5m0s for pod "pod-abe03920-4521-4e16-bb4f-67cff08c3441" in namespace "emptydir-8017" to be "Succeeded or Failed"
Jul  5 16:27:17.088: INFO: Pod "pod-abe03920-4521-4e16-bb4f-67cff08c3441": Phase="Pending", Reason="", readiness=false. Elapsed: 6.621134ms
Jul  5 16:27:19.096: INFO: Pod "pod-abe03920-4521-4e16-bb4f-67cff08c3441": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014878445s
Jul  5 16:27:21.104: INFO: Pod "pod-abe03920-4521-4e16-bb4f-67cff08c3441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023175945s
STEP: Saw pod success
Jul  5 16:27:21.104: INFO: Pod "pod-abe03920-4521-4e16-bb4f-67cff08c3441" satisfied condition "Succeeded or Failed"
Jul  5 16:27:21.111: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-abe03920-4521-4e16-bb4f-67cff08c3441 container test-container: <nil>
STEP: delete the pod
Jul  5 16:27:21.138: INFO: Waiting for pod pod-abe03920-4521-4e16-bb4f-67cff08c3441 to disappear
Jul  5 16:27:21.145: INFO: Pod pod-abe03920-4521-4e16-bb4f-67cff08c3441 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:27:21.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8017" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":2289,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:21.164: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6111
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  5 16:27:21.334: INFO: Waiting up to 5m0s for pod "pod-d48323d1-c18f-42c6-a3f2-64546fea0541" in namespace "emptydir-6111" to be "Succeeded or Failed"
Jul  5 16:27:21.340: INFO: Pod "pod-d48323d1-c18f-42c6-a3f2-64546fea0541": Phase="Pending", Reason="", readiness=false. Elapsed: 6.473463ms
Jul  5 16:27:23.348: INFO: Pod "pod-d48323d1-c18f-42c6-a3f2-64546fea0541": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014264682s
Jul  5 16:27:25.356: INFO: Pod "pod-d48323d1-c18f-42c6-a3f2-64546fea0541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022140228s
STEP: Saw pod success
Jul  5 16:27:25.356: INFO: Pod "pod-d48323d1-c18f-42c6-a3f2-64546fea0541" satisfied condition "Succeeded or Failed"
Jul  5 16:27:25.363: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-d48323d1-c18f-42c6-a3f2-64546fea0541 container test-container: <nil>
STEP: delete the pod
Jul  5 16:27:25.411: INFO: Waiting for pod pod-d48323d1-c18f-42c6-a3f2-64546fea0541 to disappear
Jul  5 16:27:25.421: INFO: Pod pod-d48323d1-c18f-42c6-a3f2-64546fea0541 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:27:25.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6111" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":115,"skipped":2304,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:25.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4103
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  5 16:27:25.960: INFO: Pod name wrapped-volume-race-1e964090-282d-41d3-8a99-6e0874e8f6f6: Found 0 pods out of 5
Jul  5 16:27:30.984: INFO: Pod name wrapped-volume-race-1e964090-282d-41d3-8a99-6e0874e8f6f6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1e964090-282d-41d3-8a99-6e0874e8f6f6 in namespace emptydir-wrapper-4103, will wait for the garbage collector to delete the pods
Jul  5 16:27:31.086: INFO: Deleting ReplicationController wrapped-volume-race-1e964090-282d-41d3-8a99-6e0874e8f6f6 took: 8.297243ms
Jul  5 16:27:31.187: INFO: Terminating ReplicationController wrapped-volume-race-1e964090-282d-41d3-8a99-6e0874e8f6f6 pods took: 100.826496ms
STEP: Creating RC which spawns configmap-volume pods
Jul  5 16:27:32.312: INFO: Pod name wrapped-volume-race-248d1741-397a-4a77-925f-d327fdde5277: Found 0 pods out of 5
Jul  5 16:27:37.336: INFO: Pod name wrapped-volume-race-248d1741-397a-4a77-925f-d327fdde5277: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-248d1741-397a-4a77-925f-d327fdde5277 in namespace emptydir-wrapper-4103, will wait for the garbage collector to delete the pods
Jul  5 16:27:37.437: INFO: Deleting ReplicationController wrapped-volume-race-248d1741-397a-4a77-925f-d327fdde5277 took: 7.867878ms
Jul  5 16:27:37.538: INFO: Terminating ReplicationController wrapped-volume-race-248d1741-397a-4a77-925f-d327fdde5277 pods took: 100.744808ms
STEP: Creating RC which spawns configmap-volume pods
Jul  5 16:27:39.263: INFO: Pod name wrapped-volume-race-997ad3c4-32ca-4736-bc62-b80561d69143: Found 0 pods out of 5
Jul  5 16:27:44.288: INFO: Pod name wrapped-volume-race-997ad3c4-32ca-4736-bc62-b80561d69143: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-997ad3c4-32ca-4736-bc62-b80561d69143 in namespace emptydir-wrapper-4103, will wait for the garbage collector to delete the pods
Jul  5 16:27:44.390: INFO: Deleting ReplicationController wrapped-volume-race-997ad3c4-32ca-4736-bc62-b80561d69143 took: 8.249177ms
Jul  5 16:27:44.491: INFO: Terminating ReplicationController wrapped-volume-race-997ad3c4-32ca-4736-bc62-b80561d69143 pods took: 100.569553ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jul  5 16:27:46.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4103" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":116,"skipped":2318,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:27:46.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jul  5 16:27:57.244: INFO: 72 pods remaining
Jul  5 16:27:57.244: INFO: 72 pods has nil DeletionTimestamp
Jul  5 16:27:57.244: INFO: 
STEP: Gathering metrics
Jul  5 16:28:02.255: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0705 16:28:02.255162    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jul  5 16:28:02.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-27vd2" in namespace "gc-9180"
Jul  5 16:28:02.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m9tp" in namespace "gc-9180"
Jul  5 16:28:02.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k88t" in namespace "gc-9180"
Jul  5 16:28:02.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h6m2" in namespace "gc-9180"
Jul  5 16:28:02.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k84d" in namespace "gc-9180"
Jul  5 16:28:02.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tzsj" in namespace "gc-9180"
Jul  5 16:28:02.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xxdl" in namespace "gc-9180"
Jul  5 16:28:02.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lnf2" in namespace "gc-9180"
Jul  5 16:28:02.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pj2h" in namespace "gc-9180"
Jul  5 16:28:02.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rkrq" in namespace "gc-9180"
Jul  5 16:28:02.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-75g7w" in namespace "gc-9180"
Jul  5 16:28:02.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-77tmm" in namespace "gc-9180"
Jul  5 16:28:02.365: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hznl" in namespace "gc-9180"
Jul  5 16:28:02.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-7k449" in namespace "gc-9180"
Jul  5 16:28:02.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ql2t" in namespace "gc-9180"
Jul  5 16:28:02.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s5hz" in namespace "gc-9180"
Jul  5 16:28:02.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vcpb" in namespace "gc-9180"
Jul  5 16:28:02.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-829lt" in namespace "gc-9180"
Jul  5 16:28:02.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bs9l" in namespace "gc-9180"
Jul  5 16:28:02.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zzc2" in namespace "gc-9180"
Jul  5 16:28:02.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bwq8" in namespace "gc-9180"
Jul  5 16:28:02.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f8v8" in namespace "gc-9180"
Jul  5 16:28:02.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hqdz" in namespace "gc-9180"
Jul  5 16:28:02.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p2wp" in namespace "gc-9180"
Jul  5 16:28:02.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qvbn" in namespace "gc-9180"
Jul  5 16:28:02.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbf56" in namespace "gc-9180"
Jul  5 16:28:02.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsclr" in namespace "gc-9180"
Jul  5 16:28:02.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwxlw" in namespace "gc-9180"
Jul  5 16:28:02.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb7h5" in namespace "gc-9180"
Jul  5 16:28:02.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccgx8" in namespace "gc-9180"
Jul  5 16:28:02.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmbs8" in namespace "gc-9180"
Jul  5 16:28:02.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmh2z" in namespace "gc-9180"
Jul  5 16:28:02.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmplj" in namespace "gc-9180"
Jul  5 16:28:02.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpthr" in namespace "gc-9180"
Jul  5 16:28:02.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9c8w" in namespace "gc-9180"
Jul  5 16:28:02.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-dczhd" in namespace "gc-9180"
Jul  5 16:28:02.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfwpl" in namespace "gc-9180"
Jul  5 16:28:02.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqstw" in namespace "gc-9180"
Jul  5 16:28:02.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-f58vm" in namespace "gc-9180"
Jul  5 16:28:02.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-f74r4" in namespace "gc-9180"
Jul  5 16:28:02.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftpn4" in namespace "gc-9180"
Jul  5 16:28:02.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5qgp" in namespace "gc-9180"
Jul  5 16:28:02.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb7w6" in namespace "gc-9180"
Jul  5 16:28:02.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-gthdp" in namespace "gc-9180"
Jul  5 16:28:02.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-h59j5" in namespace "gc-9180"
Jul  5 16:28:02.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6wz8" in namespace "gc-9180"
Jul  5 16:28:02.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-h89st" in namespace "gc-9180"
Jul  5 16:28:02.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd2v5" in namespace "gc-9180"
Jul  5 16:28:02.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjwrw" in namespace "gc-9180"
Jul  5 16:28:02.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlzjf" in namespace "gc-9180"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:28:02.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9180" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":117,"skipped":2322,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:02.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3726
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7ggd2 in namespace proxy-3726
I0705 16:28:02.911181    6089 runners.go:193] Created replication controller with name: proxy-service-7ggd2, namespace: proxy-3726, replica count: 1
I0705 16:28:03.963179    6089 runners.go:193] proxy-service-7ggd2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 16:28:04.964013    6089 runners.go:193] proxy-service-7ggd2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 16:28:05.964223    6089 runners.go:193] proxy-service-7ggd2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0705 16:28:06.964770    6089 runners.go:193] proxy-service-7ggd2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:28:06.971: INFO: setup took 4.07922404s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  5 16:28:06.997: INFO: (0) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 26.355627ms)
Jul  5 16:28:06.997: INFO: (0) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 26.263731ms)
Jul  5 16:28:06.997: INFO: (0) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 26.190735ms)
Jul  5 16:28:07.000: INFO: (0) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 28.89367ms)
Jul  5 16:28:07.002: INFO: (0) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 30.595204ms)
Jul  5 16:28:07.002: INFO: (0) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 30.45519ms)
Jul  5 16:28:07.002: INFO: (0) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 30.409362ms)
Jul  5 16:28:07.002: INFO: (0) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 30.462024ms)
Jul  5 16:28:07.002: INFO: (0) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 30.508628ms)
Jul  5 16:28:07.003: INFO: (0) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 32.050613ms)
Jul  5 16:28:07.003: INFO: (0) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 32.078592ms)
Jul  5 16:28:07.003: INFO: (0) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 32.374147ms)
Jul  5 16:28:07.005: INFO: (0) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 33.535366ms)
Jul  5 16:28:07.006: INFO: (0) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 35.457703ms)
Jul  5 16:28:07.006: INFO: (0) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 35.309847ms)
Jul  5 16:28:07.006: INFO: (0) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 35.236991ms)
Jul  5 16:28:07.019: INFO: (1) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 11.872814ms)
Jul  5 16:28:07.019: INFO: (1) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 11.90699ms)
Jul  5 16:28:07.019: INFO: (1) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 11.860235ms)
Jul  5 16:28:07.019: INFO: (1) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 11.967902ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.989867ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.903594ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 15.142676ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 15.03103ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 15.056251ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.918077ms)
Jul  5 16:28:07.022: INFO: (1) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.988242ms)
Jul  5 16:28:07.023: INFO: (1) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 16.808972ms)
Jul  5 16:28:07.023: INFO: (1) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 16.968589ms)
Jul  5 16:28:07.023: INFO: (1) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 16.978042ms)
Jul  5 16:28:07.023: INFO: (1) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 16.967295ms)
Jul  5 16:28:07.025: INFO: (1) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 18.296425ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.616121ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 13.78789ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 13.652721ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.849688ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.166719ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.197658ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.063766ms)
Jul  5 16:28:07.039: INFO: (2) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.125557ms)
Jul  5 16:28:07.043: INFO: (2) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 17.795239ms)
Jul  5 16:28:07.043: INFO: (2) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 17.679277ms)
Jul  5 16:28:07.043: INFO: (2) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 17.840248ms)
Jul  5 16:28:07.043: INFO: (2) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 17.707244ms)
Jul  5 16:28:07.047: INFO: (2) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 21.500235ms)
Jul  5 16:28:07.047: INFO: (2) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 21.516581ms)
Jul  5 16:28:07.047: INFO: (2) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 21.497043ms)
Jul  5 16:28:07.050: INFO: (2) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 25.250777ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.312727ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.3833ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.324768ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.460933ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.400189ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.519737ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.702957ms)
Jul  5 16:28:07.065: INFO: (3) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.891963ms)
Jul  5 16:28:07.066: INFO: (3) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 15.129107ms)
Jul  5 16:28:07.066: INFO: (3) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 15.061563ms)
Jul  5 16:28:07.069: INFO: (3) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 18.258182ms)
Jul  5 16:28:07.069: INFO: (3) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.334045ms)
Jul  5 16:28:07.073: INFO: (3) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 22.195609ms)
Jul  5 16:28:07.073: INFO: (3) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 22.130313ms)
Jul  5 16:28:07.073: INFO: (3) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 22.249879ms)
Jul  5 16:28:07.073: INFO: (3) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 22.428334ms)
Jul  5 16:28:07.085: INFO: (4) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.276162ms)
Jul  5 16:28:07.085: INFO: (4) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 12.467769ms)
Jul  5 16:28:07.085: INFO: (4) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 12.287513ms)
Jul  5 16:28:07.085: INFO: (4) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.387888ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.40366ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.487481ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.841056ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.841177ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.995942ms)
Jul  5 16:28:07.088: INFO: (4) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 15.173742ms)
Jul  5 16:28:07.092: INFO: (4) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 18.277339ms)
Jul  5 16:28:07.092: INFO: (4) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.309082ms)
Jul  5 16:28:07.092: INFO: (4) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 18.386237ms)
Jul  5 16:28:07.092: INFO: (4) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 18.579637ms)
Jul  5 16:28:07.099: INFO: (4) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 26.209197ms)
Jul  5 16:28:07.099: INFO: (4) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 26.156811ms)
Jul  5 16:28:07.112: INFO: (5) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 11.914021ms)
Jul  5 16:28:07.112: INFO: (5) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.078449ms)
Jul  5 16:28:07.112: INFO: (5) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.005469ms)
Jul  5 16:28:07.112: INFO: (5) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 12.003285ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.963718ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.856146ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.991802ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.848235ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.961608ms)
Jul  5 16:28:07.114: INFO: (5) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.805732ms)
Jul  5 16:28:07.118: INFO: (5) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.533808ms)
Jul  5 16:28:07.118: INFO: (5) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 18.435355ms)
Jul  5 16:28:07.118: INFO: (5) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 18.537878ms)
Jul  5 16:28:07.118: INFO: (5) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 18.450786ms)
Jul  5 16:28:07.125: INFO: (5) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 25.60669ms)
Jul  5 16:28:07.125: INFO: (5) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 25.493724ms)
Jul  5 16:28:07.141: INFO: (6) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.706563ms)
Jul  5 16:28:07.141: INFO: (6) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 12.671351ms)
Jul  5 16:28:07.141: INFO: (6) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.785789ms)
Jul  5 16:28:07.141: INFO: (6) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 12.693997ms)
Jul  5 16:28:07.142: INFO: (6) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 13.481823ms)
Jul  5 16:28:07.142: INFO: (6) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 13.404587ms)
Jul  5 16:28:07.143: INFO: (6) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.914787ms)
Jul  5 16:28:07.143: INFO: (6) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.933621ms)
Jul  5 16:28:07.143: INFO: (6) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 15.072717ms)
Jul  5 16:28:07.143: INFO: (6) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.989826ms)
Jul  5 16:28:07.147: INFO: (6) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.481027ms)
Jul  5 16:28:07.147: INFO: (6) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 18.537063ms)
Jul  5 16:28:07.150: INFO: (6) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 22.154048ms)
Jul  5 16:28:07.150: INFO: (6) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 22.311949ms)
Jul  5 16:28:07.154: INFO: (6) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 25.574092ms)
Jul  5 16:28:07.154: INFO: (6) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 25.735382ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 13.502562ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 13.296138ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 13.427795ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 13.245784ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 13.401942ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.248897ms)
Jul  5 16:28:07.168: INFO: (7) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.20555ms)
Jul  5 16:28:07.169: INFO: (7) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.488154ms)
Jul  5 16:28:07.169: INFO: (7) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.395713ms)
Jul  5 16:28:07.169: INFO: (7) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.363704ms)
Jul  5 16:28:07.169: INFO: (7) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.3191ms)
Jul  5 16:28:07.172: INFO: (7) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 17.869955ms)
Jul  5 16:28:07.176: INFO: (7) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 21.454539ms)
Jul  5 16:28:07.176: INFO: (7) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 21.554895ms)
Jul  5 16:28:07.176: INFO: (7) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 21.49464ms)
Jul  5 16:28:07.176: INFO: (7) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 21.654283ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.68601ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.842829ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.753994ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.897796ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 14.837778ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.796141ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.926931ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.98097ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.880356ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 14.862484ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.790253ms)
Jul  5 16:28:07.191: INFO: (8) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.796111ms)
Jul  5 16:28:07.198: INFO: (8) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 22.480164ms)
Jul  5 16:28:07.198: INFO: (8) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 22.41661ms)
Jul  5 16:28:07.198: INFO: (8) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 22.423569ms)
Jul  5 16:28:07.202: INFO: (8) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 26.024369ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 15.372844ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 15.468032ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 15.505288ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 15.261948ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 15.524434ms)
Jul  5 16:28:07.218: INFO: (9) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 15.398998ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 19.7153ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 19.798437ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 19.594343ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 19.672848ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 19.825112ms)
Jul  5 16:28:07.222: INFO: (9) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 19.75758ms)
Jul  5 16:28:07.229: INFO: (9) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 26.755872ms)
Jul  5 16:28:07.229: INFO: (9) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 26.728485ms)
Jul  5 16:28:07.229: INFO: (9) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 26.708199ms)
Jul  5 16:28:07.229: INFO: (9) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 26.86184ms)
Jul  5 16:28:07.242: INFO: (10) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.084019ms)
Jul  5 16:28:07.242: INFO: (10) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.136359ms)
Jul  5 16:28:07.242: INFO: (10) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 13.156576ms)
Jul  5 16:28:07.242: INFO: (10) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 13.218675ms)
Jul  5 16:28:07.244: INFO: (10) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.624144ms)
Jul  5 16:28:07.244: INFO: (10) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.857697ms)
Jul  5 16:28:07.244: INFO: (10) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.79709ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 17.634656ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 17.670747ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 17.634999ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 17.493588ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 17.616958ms)
Jul  5 16:28:07.247: INFO: (10) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 17.676854ms)
Jul  5 16:28:07.248: INFO: (10) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 19.170488ms)
Jul  5 16:28:07.248: INFO: (10) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 19.368224ms)
Jul  5 16:28:07.249: INFO: (10) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 19.34416ms)
Jul  5 16:28:07.262: INFO: (11) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 13.882965ms)
Jul  5 16:28:07.262: INFO: (11) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 13.778458ms)
Jul  5 16:28:07.262: INFO: (11) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.779892ms)
Jul  5 16:28:07.263: INFO: (11) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 13.770752ms)
Jul  5 16:28:07.263: INFO: (11) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 13.938042ms)
Jul  5 16:28:07.266: INFO: (11) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 16.913285ms)
Jul  5 16:28:07.268: INFO: (11) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 19.091038ms)
Jul  5 16:28:07.269: INFO: (11) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 20.37706ms)
Jul  5 16:28:07.269: INFO: (11) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 20.50037ms)
Jul  5 16:28:07.269: INFO: (11) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 20.37065ms)
Jul  5 16:28:07.269: INFO: (11) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 20.415175ms)
Jul  5 16:28:07.269: INFO: (11) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 20.497408ms)
Jul  5 16:28:07.273: INFO: (11) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 23.924728ms)
Jul  5 16:28:07.274: INFO: (11) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 24.936895ms)
Jul  5 16:28:07.278: INFO: (11) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 29.406526ms)
Jul  5 16:28:07.278: INFO: (11) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 29.529631ms)
Jul  5 16:28:07.291: INFO: (12) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 13.200493ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 13.126772ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 13.002362ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 13.249285ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 13.079501ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 12.947143ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 13.099405ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 13.190825ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 13.069484ms)
Jul  5 16:28:07.292: INFO: (12) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 13.278887ms)
Jul  5 16:28:07.295: INFO: (12) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 16.7112ms)
Jul  5 16:28:07.295: INFO: (12) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 16.769365ms)
Jul  5 16:28:07.299: INFO: (12) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 20.102964ms)
Jul  5 16:28:07.299: INFO: (12) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 20.198856ms)
Jul  5 16:28:07.299: INFO: (12) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 20.284963ms)
Jul  5 16:28:07.303: INFO: (12) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 24.160801ms)
Jul  5 16:28:07.317: INFO: (13) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.048807ms)
Jul  5 16:28:07.317: INFO: (13) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.074514ms)
Jul  5 16:28:07.317: INFO: (13) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 13.983321ms)
Jul  5 16:28:07.317: INFO: (13) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.114839ms)
Jul  5 16:28:07.317: INFO: (13) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.058448ms)
Jul  5 16:28:07.319: INFO: (13) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 15.85091ms)
Jul  5 16:28:07.319: INFO: (13) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 16.202845ms)
Jul  5 16:28:07.319: INFO: (13) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 16.200983ms)
Jul  5 16:28:07.319: INFO: (13) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 15.996229ms)
Jul  5 16:28:07.319: INFO: (13) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 16.014092ms)
Jul  5 16:28:07.323: INFO: (13) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 20.004468ms)
Jul  5 16:28:07.323: INFO: (13) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 20.077042ms)
Jul  5 16:28:07.323: INFO: (13) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 20.311042ms)
Jul  5 16:28:07.323: INFO: (13) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 20.105481ms)
Jul  5 16:28:07.331: INFO: (13) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 27.633376ms)
Jul  5 16:28:07.331: INFO: (13) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 27.741793ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.625229ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 12.702041ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 12.622051ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.781741ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 12.66559ms)
Jul  5 16:28:07.344: INFO: (14) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.713263ms)
Jul  5 16:28:07.345: INFO: (14) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.319468ms)
Jul  5 16:28:07.345: INFO: (14) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.388886ms)
Jul  5 16:28:07.345: INFO: (14) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.363373ms)
Jul  5 16:28:07.345: INFO: (14) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 14.581474ms)
Jul  5 16:28:07.345: INFO: (14) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.598586ms)
Jul  5 16:28:07.349: INFO: (14) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.188465ms)
Jul  5 16:28:07.349: INFO: (14) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 18.157713ms)
Jul  5 16:28:07.349: INFO: (14) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 18.307762ms)
Jul  5 16:28:07.349: INFO: (14) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 18.277739ms)
Jul  5 16:28:07.357: INFO: (14) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 25.880773ms)
Jul  5 16:28:07.371: INFO: (15) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.284535ms)
Jul  5 16:28:07.371: INFO: (15) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.33617ms)
Jul  5 16:28:07.371: INFO: (15) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.493197ms)
Jul  5 16:28:07.371: INFO: (15) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.288486ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 14.515938ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.628017ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.536434ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.520376ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.664192ms)
Jul  5 16:28:07.372: INFO: (15) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 14.531235ms)
Jul  5 16:28:07.375: INFO: (15) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 17.909898ms)
Jul  5 16:28:07.375: INFO: (15) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 18.009325ms)
Jul  5 16:28:07.379: INFO: (15) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 21.913687ms)
Jul  5 16:28:07.379: INFO: (15) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 21.729314ms)
Jul  5 16:28:07.379: INFO: (15) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 21.753221ms)
Jul  5 16:28:07.379: INFO: (15) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 21.772417ms)
Jul  5 16:28:07.391: INFO: (16) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 12.383748ms)
Jul  5 16:28:07.391: INFO: (16) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.313529ms)
Jul  5 16:28:07.391: INFO: (16) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.341017ms)
Jul  5 16:28:07.391: INFO: (16) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.511334ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.847467ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 14.814162ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 14.74712ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.932977ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.831646ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 14.793582ms)
Jul  5 16:28:07.394: INFO: (16) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 14.972883ms)
Jul  5 16:28:07.398: INFO: (16) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 18.772727ms)
Jul  5 16:28:07.398: INFO: (16) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 18.609683ms)
Jul  5 16:28:07.398: INFO: (16) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 18.664712ms)
Jul  5 16:28:07.398: INFO: (16) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 18.593075ms)
Jul  5 16:28:07.405: INFO: (16) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 25.955058ms)
Jul  5 16:28:07.417: INFO: (17) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 12.351309ms)
Jul  5 16:28:07.418: INFO: (17) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.205413ms)
Jul  5 16:28:07.418: INFO: (17) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.387984ms)
Jul  5 16:28:07.417: INFO: (17) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 12.154702ms)
Jul  5 16:28:07.418: INFO: (17) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 12.267132ms)
Jul  5 16:28:07.417: INFO: (17) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.223175ms)
Jul  5 16:28:07.420: INFO: (17) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 14.36805ms)
Jul  5 16:28:07.420: INFO: (17) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 14.392937ms)
Jul  5 16:28:07.420: INFO: (17) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.453267ms)
Jul  5 16:28:07.420: INFO: (17) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 14.530038ms)
Jul  5 16:28:07.420: INFO: (17) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 14.463217ms)
Jul  5 16:28:07.423: INFO: (17) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 17.79943ms)
Jul  5 16:28:07.423: INFO: (17) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 17.943477ms)
Jul  5 16:28:07.423: INFO: (17) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 18.04455ms)
Jul  5 16:28:07.423: INFO: (17) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 17.869507ms)
Jul  5 16:28:07.431: INFO: (17) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 25.777439ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 11.771081ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 11.757246ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 11.796338ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 11.738651ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 11.85356ms)
Jul  5 16:28:07.443: INFO: (18) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 11.630308ms)
Jul  5 16:28:07.445: INFO: (18) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 14.065467ms)
Jul  5 16:28:07.445: INFO: (18) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 13.967358ms)
Jul  5 16:28:07.445: INFO: (18) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 13.999664ms)
Jul  5 16:28:07.445: INFO: (18) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 14.027852ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 17.318112ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 17.403181ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 17.432851ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 17.40446ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 17.517701ms)
Jul  5 16:28:07.449: INFO: (18) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 17.411678ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.746557ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:160/proxy/: foo (200; 12.740724ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname1/proxy/: tls baz (200; 12.816766ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">... (200; 12.750103ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:460/proxy/: tls baz (200; 12.635782ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh/proxy/rewriteme">test</a> (200; 12.663005ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/services/https:proxy-service-7ggd2:tlsportname2/proxy/: tls qux (200; 12.879391ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:443/proxy/tlsrewritem... (200; 12.825438ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/http:proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.854074ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/https:proxy-service-7ggd2-g6knh:462/proxy/: tls qux (200; 12.775498ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:162/proxy/: bar (200; 12.910309ms)
Jul  5 16:28:07.462: INFO: (19) /api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3726/pods/proxy-service-7ggd2-g6knh:1080/proxy/rewriteme">test<... (200; 13.013365ms)
Jul  5 16:28:07.463: INFO: (19) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname2/proxy/: bar (200; 13.657521ms)
Jul  5 16:28:07.466: INFO: (19) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname1/proxy/: foo (200; 17.353328ms)
Jul  5 16:28:07.466: INFO: (19) /api/v1/namespaces/proxy-3726/services/proxy-service-7ggd2:portname2/proxy/: bar (200; 17.326172ms)
Jul  5 16:28:07.466: INFO: (19) /api/v1/namespaces/proxy-3726/services/http:proxy-service-7ggd2:portname1/proxy/: foo (200; 17.331888ms)
STEP: deleting ReplicationController proxy-service-7ggd2 in namespace proxy-3726, will wait for the garbage collector to delete the pods
Jul  5 16:28:07.532: INFO: Deleting ReplicationController proxy-service-7ggd2 took: 8.008115ms
Jul  5 16:28:07.632: INFO: Terminating ReplicationController proxy-service-7ggd2 pods took: 100.175132ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jul  5 16:28:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3726" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":118,"skipped":2331,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:09.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8745
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-8745
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 16:28:10.012: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  5 16:28:10.060: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:28:12.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:28:14.069: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:28:16.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:28:18.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:28:20.069: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:28:22.069: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  5 16:28:22.082: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul  5 16:28:24.120: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul  5 16:28:24.120: INFO: Breadth first check of 172.16.0.174 on host 10.250.25.206...
Jul  5 16:28:24.127: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.244:9080/dial?request=hostname&protocol=udp&host=172.16.0.174&port=8081&tries=1'] Namespace:pod-network-test-8745 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:28:24.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:28:24.127: INFO: ExecWithOptions: Clientset creation
Jul  5 16:28:24.128: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8745/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.1.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.0.174%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul  5 16:28:24.440: INFO: Waiting for responses: map[]
Jul  5 16:28:24.440: INFO: reached 172.16.0.174 after 0/1 tries
Jul  5 16:28:24.440: INFO: Breadth first check of 172.16.1.243 on host 10.250.25.207...
Jul  5 16:28:24.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.244:9080/dial?request=hostname&protocol=udp&host=172.16.1.243&port=8081&tries=1'] Namespace:pod-network-test-8745 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:28:24.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:28:24.448: INFO: ExecWithOptions: Clientset creation
Jul  5 16:28:24.448: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-8745/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.1.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.1.243%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul  5 16:28:24.712: INFO: Waiting for responses: map[]
Jul  5 16:28:24.712: INFO: reached 172.16.1.243 after 0/1 tries
Jul  5 16:28:24.712: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jul  5 16:28:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8745" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":119,"skipped":2340,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:24.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6462
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jul  5 16:28:24.915: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:28:26.923: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jul  5 16:28:26.948: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:28:28.957: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  5 16:28:29.028: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 16:28:29.035: INFO: Pod pod-with-poststart-http-hook still exists
Jul  5 16:28:31.036: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 16:28:31.043: INFO: Pod pod-with-poststart-http-hook still exists
Jul  5 16:28:33.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 16:28:33.044: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jul  5 16:28:33.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6462" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2342,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:33.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1477
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1477
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1477
I0705 16:28:33.263836    6089 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1477, replica count: 2
I0705 16:28:36.314393    6089 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:28:36.314: INFO: Creating new exec pod
Jul  5 16:28:39.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul  5 16:28:39.681: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul  5 16:28:39.681: INFO: stdout: "externalname-service-lcm2k"
Jul  5 16:28:39.681: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.168.181 80'
Jul  5 16:28:40.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.168.181 80\nConnection to 172.31.168.181 80 port [tcp/http] succeeded!\n"
Jul  5 16:28:40.064: INFO: stdout: ""
Jul  5 16:28:41.064: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.168.181 80'
Jul  5 16:28:41.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.168.181 80\nConnection to 172.31.168.181 80 port [tcp/http] succeeded!\n"
Jul  5 16:28:41.384: INFO: stdout: ""
Jul  5 16:28:42.065: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.168.181 80'
Jul  5 16:28:42.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.168.181 80\nConnection to 172.31.168.181 80 port [tcp/http] succeeded!\n"
Jul  5 16:28:42.436: INFO: stdout: "externalname-service-j9h2f"
Jul  5 16:28:42.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 32139'
Jul  5 16:28:42.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 32139\nConnection to 10.250.25.206 32139 port [tcp/*] succeeded!\n"
Jul  5 16:28:42.820: INFO: stdout: ""
Jul  5 16:28:43.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 32139'
Jul  5 16:28:44.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 32139\nConnection to 10.250.25.206 32139 port [tcp/*] succeeded!\n"
Jul  5 16:28:44.166: INFO: stdout: ""
Jul  5 16:28:44.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 32139'
Jul  5 16:28:45.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 32139\nConnection to 10.250.25.206 32139 port [tcp/*] succeeded!\n"
Jul  5 16:28:45.151: INFO: stdout: ""
Jul  5 16:28:45.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 32139'
Jul  5 16:28:46.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 32139\nConnection to 10.250.25.206 32139 port [tcp/*] succeeded!\n"
Jul  5 16:28:46.176: INFO: stdout: "externalname-service-lcm2k"
Jul  5 16:28:46.176: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1477 exec execpodjcc6l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.207 32139'
Jul  5 16:28:46.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.207 32139\nConnection to 10.250.25.207 32139 port [tcp/*] succeeded!\n"
Jul  5 16:28:46.544: INFO: stdout: "externalname-service-lcm2k"
Jul  5 16:28:46.544: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:28:46.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1477" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":121,"skipped":2345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:46.576: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3971
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jul  5 16:28:46.752: INFO: The status of Pod annotationupdatefb063f6f-d1d3-4896-a48c-4a0b65e7c1ff is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:28:48.760: INFO: The status of Pod annotationupdatefb063f6f-d1d3-4896-a48c-4a0b65e7c1ff is Running (Ready = true)
Jul  5 16:28:49.300: INFO: Successfully updated pod "annotationupdatefb063f6f-d1d3-4896-a48c-4a0b65e7c1ff"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:28:53.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3971" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":122,"skipped":2384,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:53.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5364
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:28:53.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:28:56.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:28:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5364" for this suite.
STEP: Destroying namespace "webhook-5364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":123,"skipped":2396,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:57.058: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1038
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jul  5 16:28:57.214: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1038 api-versions'
Jul  5 16:28:57.419: INFO: stderr: ""
Jul  5 16:28:57.420: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:28:57.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1038" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":124,"skipped":2412,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:28:57.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2837
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jul  5 16:28:57.622: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:28:59.630: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jul  5 16:28:59.665: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:29:01.672: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  5 16:29:01.704: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 16:29:01.711: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 16:29:03.712: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 16:29:03.738: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 16:29:05.712: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 16:29:05.719: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jul  5 16:29:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2837" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":125,"skipped":2422,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:05.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4833
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jul  5 16:29:05.919: INFO: The status of Pod labelsupdate2dad8c95-5c0a-46a9-b502-f907f0bd247a is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:29:07.927: INFO: The status of Pod labelsupdate2dad8c95-5c0a-46a9-b502-f907f0bd247a is Running (Ready = true)
Jul  5 16:29:08.467: INFO: Successfully updated pod "labelsupdate2dad8c95-5c0a-46a9-b502-f907f0bd247a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:29:12.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4833" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:12.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6790
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-55ecce28-1fea-4563-86c6-95dc669272f7
STEP: Creating a pod to test consume secrets
Jul  5 16:29:12.714: INFO: Waiting up to 5m0s for pod "pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34" in namespace "secrets-6790" to be "Succeeded or Failed"
Jul  5 16:29:12.721: INFO: Pod "pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643842ms
Jul  5 16:29:14.733: INFO: Pod "pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018195699s
Jul  5 16:29:16.740: INFO: Pod "pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026060282s
STEP: Saw pod success
Jul  5 16:29:16.740: INFO: Pod "pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34" satisfied condition "Succeeded or Failed"
Jul  5 16:29:16.747: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 16:29:16.775: INFO: Waiting for pod pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34 to disappear
Jul  5 16:29:16.782: INFO: Pod pod-secrets-5608c2e8-67ea-42db-94a9-d05b12d77e34 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:29:16.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6790" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":127,"skipped":2475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:16.802: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-688
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:29:17.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:29:20.491: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:29:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-688" for this suite.
STEP: Destroying namespace "webhook-688-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":128,"skipped":2519,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:20.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5601
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jul  5 16:29:20.991: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 create -f -'
Jul  5 16:29:21.619: INFO: stderr: ""
Jul  5 16:29:21.619: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 16:29:21.619: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:29:21.724: INFO: stderr: ""
Jul  5 16:29:21.724: INFO: stdout: "update-demo-nautilus-2txs2 update-demo-nautilus-6kf6w "
Jul  5 16:29:21.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods update-demo-nautilus-2txs2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:29:21.823: INFO: stderr: ""
Jul  5 16:29:21.823: INFO: stdout: ""
Jul  5 16:29:21.823: INFO: update-demo-nautilus-2txs2 is created but not running
Jul  5 16:29:26.824: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul  5 16:29:26.927: INFO: stderr: ""
Jul  5 16:29:26.928: INFO: stdout: "update-demo-nautilus-2txs2 update-demo-nautilus-6kf6w "
Jul  5 16:29:26.928: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods update-demo-nautilus-2txs2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:29:27.031: INFO: stderr: ""
Jul  5 16:29:27.031: INFO: stdout: "true"
Jul  5 16:29:27.031: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods update-demo-nautilus-2txs2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:29:27.138: INFO: stderr: ""
Jul  5 16:29:27.138: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:29:27.138: INFO: validating pod update-demo-nautilus-2txs2
Jul  5 16:29:27.204: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:29:27.204: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:29:27.204: INFO: update-demo-nautilus-2txs2 is verified up and running
Jul  5 16:29:27.204: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods update-demo-nautilus-6kf6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul  5 16:29:27.316: INFO: stderr: ""
Jul  5 16:29:27.316: INFO: stdout: "true"
Jul  5 16:29:27.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods update-demo-nautilus-6kf6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul  5 16:29:27.430: INFO: stderr: ""
Jul  5 16:29:27.430: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul  5 16:29:27.430: INFO: validating pod update-demo-nautilus-6kf6w
Jul  5 16:29:27.497: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 16:29:27.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 16:29:27.497: INFO: update-demo-nautilus-6kf6w is verified up and running
STEP: using delete to clean up resources
Jul  5 16:29:27.497: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 delete --grace-period=0 --force -f -'
Jul  5 16:29:27.601: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:29:27.601: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  5 16:29:27.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get rc,svc -l name=update-demo --no-headers'
Jul  5 16:29:27.738: INFO: stderr: "No resources found in kubectl-5601 namespace.\n"
Jul  5 16:29:27.738: INFO: stdout: ""
Jul  5 16:29:27.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5601 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 16:29:27.827: INFO: stderr: ""
Jul  5 16:29:27.827: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:29:27.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5601" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":129,"skipped":2525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:27.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9510
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jul  5 16:29:28.021: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  5 16:29:33.031: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 16:29:33.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9510" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":130,"skipped":2549,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:29:33.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4449
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-152be029-4e35-405c-ab36-7384eb57c252 in namespace container-probe-4449
Jul  5 16:29:35.273: INFO: Started pod test-webserver-152be029-4e35-405c-ab36-7384eb57c252 in namespace container-probe-4449
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 16:29:35.280: INFO: Initial restart count of pod test-webserver-152be029-4e35-405c-ab36-7384eb57c252 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 16:33:36.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4449" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":131,"skipped":2565,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:36.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6471
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:33:36.451: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul  5 16:33:37.499: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jul  5 16:33:37.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6471" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":132,"skipped":2573,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:37.525: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9953
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  5 16:33:37.694: INFO: Waiting up to 5m0s for pod "pod-c9363e93-1c12-43e2-b736-6906e74ab03d" in namespace "emptydir-9953" to be "Succeeded or Failed"
Jul  5 16:33:37.701: INFO: Pod "pod-c9363e93-1c12-43e2-b736-6906e74ab03d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370819ms
Jul  5 16:33:39.709: INFO: Pod "pod-c9363e93-1c12-43e2-b736-6906e74ab03d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015246528s
Jul  5 16:33:41.718: INFO: Pod "pod-c9363e93-1c12-43e2-b736-6906e74ab03d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023566697s
STEP: Saw pod success
Jul  5 16:33:41.718: INFO: Pod "pod-c9363e93-1c12-43e2-b736-6906e74ab03d" satisfied condition "Succeeded or Failed"
Jul  5 16:33:41.724: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-c9363e93-1c12-43e2-b736-6906e74ab03d container test-container: <nil>
STEP: delete the pod
Jul  5 16:33:41.758: INFO: Waiting for pod pod-c9363e93-1c12-43e2-b736-6906e74ab03d to disappear
Jul  5 16:33:41.767: INFO: Pod pod-c9363e93-1c12-43e2-b736-6906e74ab03d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:33:41.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9953" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":133,"skipped":2573,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:41.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9107
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul  5 16:33:42.647: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:33:42.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0705 16:33:42.647914    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-9107" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":134,"skipped":2590,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:42.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6006
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul  5 16:33:42.819: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6006 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jul  5 16:33:42.944: INFO: stderr: ""
Jul  5 16:33:42.944: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jul  5 16:33:42.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6006 delete pods e2e-test-httpd-pod'
Jul  5 16:33:45.513: INFO: stderr: ""
Jul  5 16:33:45.513: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:33:45.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6006" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":135,"skipped":2597,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:45.533: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3218
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jul  5 16:33:45.710: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  5 16:33:50.717: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jul  5 16:33:50.725: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jul  5 16:33:50.740: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jul  5 16:33:50.746: INFO: Observed &ReplicaSet event: ADDED
Jul  5 16:33:50.747: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.747: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.747: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.747: INFO: Found replicaset test-rs in namespace replicaset-3218 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul  5 16:33:50.747: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jul  5 16:33:50.747: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul  5 16:33:50.755: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jul  5 16:33:50.761: INFO: Observed &ReplicaSet event: ADDED
Jul  5 16:33:50.761: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.761: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.761: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.761: INFO: Observed replicaset test-rs in namespace replicaset-3218 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul  5 16:33:50.762: INFO: Observed &ReplicaSet event: MODIFIED
Jul  5 16:33:50.762: INFO: Found replicaset test-rs in namespace replicaset-3218 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jul  5 16:33:50.762: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 16:33:50.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3218" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":136,"skipped":2599,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:50.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6132
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jul  5 16:33:50.952: INFO: created test-pod-1
Jul  5 16:33:50.962: INFO: created test-pod-2
Jul  5 16:33:50.974: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jul  5 16:33:50.974: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6132' to be running and ready
Jul  5 16:33:50.994: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul  5 16:33:50.994: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul  5 16:33:50.994: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul  5 16:33:50.994: INFO: 0 / 3 pods in namespace 'pods-6132' are running and ready (0 seconds elapsed)
Jul  5 16:33:50.994: INFO: expected 0 pod replicas in namespace 'pods-6132', 0 are Running and Ready.
Jul  5 16:33:50.994: INFO: POD         NODE                     PHASE    GRACE  CONDITIONS
Jul  5 16:33:50.994: INFO: test-pod-1  izgw8bazids4c4cxzuus22z  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC  }]
Jul  5 16:33:50.994: INFO: test-pod-2  izgw8bazids4c4cxzuus22z  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC  }]
Jul  5 16:33:50.994: INFO: test-pod-3  izgw8bazids4c4cxzuus22z  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 16:33:50 +0000 UTC  }]
Jul  5 16:33:50.994: INFO: 
Jul  5 16:33:53.015: INFO: 3 / 3 pods in namespace 'pods-6132' are running and ready (2 seconds elapsed)
Jul  5 16:33:53.015: INFO: expected 0 pod replicas in namespace 'pods-6132', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jul  5 16:33:53.034: INFO: Pod quantity 3 is different from expected quantity 0
Jul  5 16:33:54.042: INFO: Pod quantity 3 is different from expected quantity 0
Jul  5 16:33:55.042: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 16:33:56.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6132" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":137,"skipped":2602,"failed":0}

------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:33:56.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3581
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-3516c8c4-7ecb-4c15-98f5-ba399c14aad0
STEP: Creating a pod to test consume secrets
Jul  5 16:33:56.239: INFO: Waiting up to 5m0s for pod "pod-secrets-5697883f-88be-4538-8a61-1c289a68296d" in namespace "secrets-3581" to be "Succeeded or Failed"
Jul  5 16:33:56.245: INFO: Pod "pod-secrets-5697883f-88be-4538-8a61-1c289a68296d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209222ms
Jul  5 16:33:58.253: INFO: Pod "pod-secrets-5697883f-88be-4538-8a61-1c289a68296d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014229953s
Jul  5 16:34:00.261: INFO: Pod "pod-secrets-5697883f-88be-4538-8a61-1c289a68296d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022134415s
STEP: Saw pod success
Jul  5 16:34:00.261: INFO: Pod "pod-secrets-5697883f-88be-4538-8a61-1c289a68296d" satisfied condition "Succeeded or Failed"
Jul  5 16:34:00.268: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-5697883f-88be-4538-8a61-1c289a68296d container secret-env-test: <nil>
STEP: delete the pod
Jul  5 16:34:00.293: INFO: Waiting for pod pod-secrets-5697883f-88be-4538-8a61-1c289a68296d to disappear
Jul  5 16:34:00.299: INFO: Pod pod-secrets-5697883f-88be-4538-8a61-1c289a68296d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:34:00.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3581" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":138,"skipped":2602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:34:00.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-496
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  5 16:34:00.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21921 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:00.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21921 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  5 16:34:00.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21922 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:00.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21922 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  5 16:34:00.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21923 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:00.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21923 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  5 16:34:00.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21924 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:00.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-496  5a6b2bdf-504b-4d7e-8a22-5d5fe54539d7 21924 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  5 16:34:00.543: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-496  076f3318-4975-4969-872d-a8703e043672 21925 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:00.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-496  076f3318-4975-4969-872d-a8703e043672 21925 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  5 16:34:10.555: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-496  076f3318-4975-4969-872d-a8703e043672 21980 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 16:34:10.555: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-496  076f3318-4975-4969-872d-a8703e043672 21980 0 2022-07-05 16:34:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-05 16:34:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jul  5 16:34:20.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-496" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":139,"skipped":2626,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:34:20.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-830
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jul  5 16:34:40.874: INFO: EndpointSlice for Service endpointslice-830/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jul  5 16:34:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-830" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":140,"skipped":2641,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:34:50.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9754
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-243124d3-ced3-416e-ba69-0f9b43adf71d
STEP: Creating the pod
Jul  5 16:34:51.153: INFO: The status of Pod pod-configmaps-a29b4285-d19e-4669-b0c4-5f03139249fd is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:34:53.162: INFO: The status of Pod pod-configmaps-a29b4285-d19e-4669-b0c4-5f03139249fd is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-243124d3-ced3-416e-ba69-0f9b43adf71d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:34:55.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9754" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":141,"skipped":2646,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:34:55.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-c4b82738-eba2-487b-a3c1-11d68fd609c1 in namespace container-probe-3160
Jul  5 16:34:57.476: INFO: Started pod liveness-c4b82738-eba2-487b-a3c1-11d68fd609c1 in namespace container-probe-3160
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 16:34:57.483: INFO: Initial restart count of pod liveness-c4b82738-eba2-487b-a3c1-11d68fd609c1 is 0
Jul  5 16:35:17.568: INFO: Restart count of pod container-probe-3160/liveness-c4b82738-eba2-487b-a3c1-11d68fd609c1 is now 1 (20.085395536s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 16:35:17.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3160" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":142,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:17.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5605
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jul  5 16:35:17.776: INFO: The status of Pod pod-hostip-5aced953-2004-4604-8c3d-7e2e26e23877 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:35:19.785: INFO: The status of Pod pod-hostip-5aced953-2004-4604-8c3d-7e2e26e23877 is Running (Ready = true)
Jul  5 16:35:19.798: INFO: Pod pod-hostip-5aced953-2004-4604-8c3d-7e2e26e23877 has hostIP: 10.250.25.207
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 16:35:19.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5605" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2687,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:19.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-74
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul  5 16:35:19.998: INFO: Waiting up to 5m0s for pod "security-context-190eec57-6f80-459c-b052-7f6962136270" in namespace "security-context-74" to be "Succeeded or Failed"
Jul  5 16:35:20.004: INFO: Pod "security-context-190eec57-6f80-459c-b052-7f6962136270": Phase="Pending", Reason="", readiness=false. Elapsed: 6.501546ms
Jul  5 16:35:22.016: INFO: Pod "security-context-190eec57-6f80-459c-b052-7f6962136270": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018304364s
Jul  5 16:35:24.024: INFO: Pod "security-context-190eec57-6f80-459c-b052-7f6962136270": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026316806s
STEP: Saw pod success
Jul  5 16:35:24.024: INFO: Pod "security-context-190eec57-6f80-459c-b052-7f6962136270" satisfied condition "Succeeded or Failed"
Jul  5 16:35:24.031: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod security-context-190eec57-6f80-459c-b052-7f6962136270 container test-container: <nil>
STEP: delete the pod
Jul  5 16:35:24.055: INFO: Waiting for pod security-context-190eec57-6f80-459c-b052-7f6962136270 to disappear
Jul  5 16:35:24.062: INFO: Pod security-context-190eec57-6f80-459c-b052-7f6962136270 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 16:35:24.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-74" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":144,"skipped":2706,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:24.081: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2697
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:35:24.594: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:35:27.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:35:28.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2697" for this suite.
STEP: Destroying namespace "webhook-2697-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":145,"skipped":2717,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:28.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9661
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jul  5 16:35:34.423: INFO: 90 pods remaining
Jul  5 16:35:34.423: INFO: 90 pods has nil DeletionTimestamp
Jul  5 16:35:34.423: INFO: 
Jul  5 16:35:35.396: INFO: 70 pods remaining
Jul  5 16:35:35.396: INFO: 70 pods has nil DeletionTimestamp
Jul  5 16:35:35.396: INFO: 
Jul  5 16:35:36.398: INFO: 70 pods remaining
Jul  5 16:35:36.398: INFO: 70 pods has nil DeletionTimestamp
Jul  5 16:35:36.398: INFO: 
Jul  5 16:35:37.405: INFO: 40 pods remaining
Jul  5 16:35:37.405: INFO: 40 pods has nil DeletionTimestamp
Jul  5 16:35:37.405: INFO: 
Jul  5 16:35:38.405: INFO: 40 pods remaining
Jul  5 16:35:38.405: INFO: 40 pods has nil DeletionTimestamp
Jul  5 16:35:38.405: INFO: 
Jul  5 16:35:39.394: INFO: 10 pods remaining
Jul  5 16:35:39.394: INFO: 10 pods has nil DeletionTimestamp
Jul  5 16:35:39.394: INFO: 
STEP: Gathering metrics
Jul  5 16:35:40.440: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0705 16:35:40.440864    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:35:40.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9661" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":146,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:40.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-7609
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jul  5 16:35:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7609" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":147,"skipped":2751,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:40.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9077
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-2df999bb-03bd-47f0-810a-c10c303e1133
STEP: Creating a pod to test consume configMaps
Jul  5 16:35:40.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2" in namespace "configmap-9077" to be "Succeeded or Failed"
Jul  5 16:35:40.847: INFO: Pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.886553ms
Jul  5 16:35:42.856: INFO: Pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015167894s
Jul  5 16:35:44.864: INFO: Pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023275364s
Jul  5 16:35:46.872: INFO: Pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031361624s
STEP: Saw pod success
Jul  5 16:35:46.872: INFO: Pod "pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2" satisfied condition "Succeeded or Failed"
Jul  5 16:35:46.879: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:35:46.947: INFO: Waiting for pod pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2 to disappear
Jul  5 16:35:46.953: INFO: Pod pod-configmaps-d1e06b20-eb75-462d-8ac1-4227dd4c18b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:35:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9077" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":148,"skipped":2762,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:46.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jul  5 16:35:47.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-374" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":149,"skipped":2765,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:47.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3197
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jul  5 16:35:47.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3197 create -f -'
Jul  5 16:35:47.559: INFO: stderr: ""
Jul  5 16:35:47.559: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul  5 16:35:48.567: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:35:48.567: INFO: Found 0 / 1
Jul  5 16:35:49.567: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:35:49.567: INFO: Found 1 / 1
Jul  5 16:35:49.567: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  5 16:35:49.574: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:35:49.574: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 16:35:49.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3197 patch pod agnhost-primary-qg76s -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  5 16:35:49.676: INFO: stderr: ""
Jul  5 16:35:49.676: INFO: stdout: "pod/agnhost-primary-qg76s patched\n"
STEP: checking annotations
Jul  5 16:35:49.683: INFO: Selector matched 1 pods for map[app:agnhost]
Jul  5 16:35:49.683: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:35:49.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3197" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":150,"skipped":2773,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:49.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2531
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jul  5 16:35:49.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2531" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":151,"skipped":2782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:49.933: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-3568
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul  5 16:35:50.148: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul  5 16:35:50.188: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jul  5 16:35:50.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3568" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":152,"skipped":2859,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:50.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7446
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:35:50.398: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jul  5 16:35:53.064: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 create -f -'
Jul  5 16:35:54.115: INFO: stderr: ""
Jul  5 16:35:54.115: INFO: stdout: "e2e-test-crd-publish-openapi-7854-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul  5 16:35:54.116: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 delete e2e-test-crd-publish-openapi-7854-crds test-foo'
Jul  5 16:35:54.231: INFO: stderr: ""
Jul  5 16:35:54.231: INFO: stdout: "e2e-test-crd-publish-openapi-7854-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul  5 16:35:54.231: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 apply -f -'
Jul  5 16:35:54.463: INFO: stderr: ""
Jul  5 16:35:54.463: INFO: stdout: "e2e-test-crd-publish-openapi-7854-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul  5 16:35:54.463: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 delete e2e-test-crd-publish-openapi-7854-crds test-foo'
Jul  5 16:35:54.585: INFO: stderr: ""
Jul  5 16:35:54.585: INFO: stdout: "e2e-test-crd-publish-openapi-7854-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jul  5 16:35:54.586: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 create -f -'
Jul  5 16:35:54.807: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul  5 16:35:54.807: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 create -f -'
Jul  5 16:35:55.020: INFO: rc: 1
Jul  5 16:35:55.020: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 apply -f -'
Jul  5 16:35:55.232: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jul  5 16:35:55.232: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 create -f -'
Jul  5 16:35:55.437: INFO: rc: 1
Jul  5 16:35:55.437: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 --namespace=crd-publish-openapi-7446 apply -f -'
Jul  5 16:35:55.649: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul  5 16:35:55.649: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 explain e2e-test-crd-publish-openapi-7854-crds'
Jul  5 16:35:55.848: INFO: stderr: ""
Jul  5 16:35:55.848: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7854-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul  5 16:35:55.849: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 explain e2e-test-crd-publish-openapi-7854-crds.metadata'
Jul  5 16:35:56.076: INFO: stderr: ""
Jul  5 16:35:56.076: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7854-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul  5 16:35:56.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 explain e2e-test-crd-publish-openapi-7854-crds.spec'
Jul  5 16:35:56.294: INFO: stderr: ""
Jul  5 16:35:56.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7854-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul  5 16:35:56.294: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 explain e2e-test-crd-publish-openapi-7854-crds.spec.bars'
Jul  5 16:35:56.502: INFO: stderr: ""
Jul  5 16:35:56.502: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7854-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul  5 16:35:56.502: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-7446 explain e2e-test-crd-publish-openapi-7854-crds.spec.bars2'
Jul  5 16:35:56.761: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:35:59.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7446" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":153,"skipped":2872,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:35:59.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2103
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul  5 16:35:59.429: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 16:35:59.455: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 16:35:59.471: INFO: 
Logging pods the apiserver thinks is on node izgw85sex2ooqi4ztetrj0z before test
Jul  5 16:35:59.486: INFO: addons-nginx-ingress-controller-696bcbf64f-m7q4w from kube-system started at 2022-07-05 16:09:49 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul  5 16:35:59.486: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul  5 16:35:59.486: INFO: apiserver-proxy-pdxqb from kube-system started at 2022-07-05 15:47:30 +0000 UTC (2 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container proxy ready: true, restart count 0
Jul  5 16:35:59.486: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 16:35:59.486: INFO: blackbox-exporter-ccdc4b99c-5trtq from kube-system started at 2022-07-05 15:55:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 16:35:59.486: INFO: calico-kube-controllers-6959b48bb7-bjpsr from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul  5 16:35:59.486: INFO: calico-node-gpt27 from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 16:35:59.486: INFO: calico-node-vertical-autoscaler-5b74b8f994-rthwq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:35:59.486: INFO: calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:35:59.486: INFO: calico-typha-vertical-autoscaler-78b946fc85-59kkq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 16:35:59.486: INFO: coredns-7f49f7db48-x8h24 from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container coredns ready: true, restart count 0
Jul  5 16:35:59.486: INFO: coredns-7f49f7db48-zglbj from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container coredns ready: true, restart count 0
Jul  5 16:35:59.486: INFO: csi-disk-plugin-alicloud-8pszw from kube-system started at 2022-07-05 15:47:30 +0000 UTC (3 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 16:35:59.486: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 16:35:59.486: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 16:35:59.486: INFO: egress-filter-applier-p45dc from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 16:35:59.486: INFO: kube-proxy-worker-1-v1.24.2-pjhw2 from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 16:35:59.486: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 16:35:59.486: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 16:35:59.487: INFO: metrics-server-788cb89-tfl5k from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 16:35:59.487: INFO: node-exporter-p2ztl from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 16:35:59.487: INFO: node-problem-detector-zs9bx from kube-system started at 2022-07-05 16:15:28 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul  5 16:35:59.487: INFO: vpn-shoot-5fcf58b56b-k9pwx from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul  5 16:35:59.487: INFO: dashboard-metrics-scraper-9c4f98cd5-sthht from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul  5 16:35:59.487: INFO: kubernetes-dashboard-55d4694cd7-fscg2 from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.487: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul  5 16:35:59.487: INFO: 
Logging pods the apiserver thinks is on node izgw8bazids4c4cxzuus22z before test
Jul  5 16:35:59.497: INFO: apiserver-proxy-nxtmb from kube-system started at 2022-07-05 15:47:45 +0000 UTC (2 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container proxy ready: true, restart count 0
Jul  5 16:35:59.497: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 16:35:59.497: INFO: blackbox-exporter-ccdc4b99c-m6ktb from kube-system started at 2022-07-05 15:54:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 16:35:59.497: INFO: calico-node-4fk5s from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 16:35:59.497: INFO: calico-typha-deploy-7f646d87dc-6rg66 from kube-system started at 2022-07-05 15:48:39 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container calico-typha ready: true, restart count 0
Jul  5 16:35:59.497: INFO: csi-disk-plugin-alicloud-7czsh from kube-system started at 2022-07-05 15:47:45 +0000 UTC (3 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 16:35:59.497: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 16:35:59.497: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 16:35:59.497: INFO: egress-filter-applier-p65qk from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 16:35:59.497: INFO: kube-proxy-worker-1-v1.24.2-mxz5v from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 16:35:59.497: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 16:35:59.497: INFO: node-exporter-gjd9k from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 16:35:59.497: INFO: node-problem-detector-qpx9f from kube-system started at 2022-07-05 16:15:27 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul  5 16:35:59.497: INFO: pod-qos-class-583f743a-ba4d-496a-8f17-928bd0101464 from pods-2531 started at 2022-07-05 16:35:49 +0000 UTC (1 container statuses recorded)
Jul  5 16:35:59.497: INFO: 	Container agnhost ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16fefc278141a248], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:36:00.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2103" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":154,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:36:00.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-8125
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul  5 16:36:00.750: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul  5 16:36:00.763: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul  5 16:36:00.763: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul  5 16:36:00.780: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul  5 16:36:00.780: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul  5 16:36:00.797: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul  5 16:36:00.797: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul  5 16:36:07.868: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jul  5 16:36:07.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8125" for this suite.
•{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":155,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:36:07.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jul  5 16:36:08.057: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul  5 16:36:08.143: INFO: stderr: ""
Jul  5 16:36:08.143: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jul  5 16:36:08.143: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul  5 16:36:08.143: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7180" to be "running and ready, or succeeded"
Jul  5 16:36:08.150: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.434789ms
Jul  5 16:36:10.157: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.013945441s
Jul  5 16:36:10.157: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul  5 16:36:10.157: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul  5 16:36:10.157: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator'
Jul  5 16:36:10.282: INFO: stderr: ""
Jul  5 16:36:10.282: INFO: stdout: "I0705 16:36:08.736712       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dczg 430\nI0705 16:36:08.937035       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/5r6z 469\nI0705 16:36:09.137336       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/znm 586\nI0705 16:36:09.337641       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/pkg8 388\nI0705 16:36:09.536889       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/zhz8 300\nI0705 16:36:09.737211       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/bkgq 341\nI0705 16:36:09.937534       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/wtjq 402\nI0705 16:36:10.136787       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/l8s 414\n"
STEP: limiting log lines
Jul  5 16:36:10.283: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator --tail=1'
Jul  5 16:36:10.423: INFO: stderr: ""
Jul  5 16:36:10.423: INFO: stdout: "I0705 16:36:10.337096       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/wgq 231\n"
Jul  5 16:36:10.423: INFO: got output "I0705 16:36:10.337096       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/wgq 231\n"
STEP: limiting log bytes
Jul  5 16:36:10.423: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator --limit-bytes=1'
Jul  5 16:36:10.539: INFO: stderr: ""
Jul  5 16:36:10.539: INFO: stdout: "I"
Jul  5 16:36:10.539: INFO: got output "I"
STEP: exposing timestamps
Jul  5 16:36:10.539: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator --tail=1 --timestamps'
Jul  5 16:36:10.660: INFO: stderr: ""
Jul  5 16:36:10.660: INFO: stdout: "2022-07-05T16:36:10.537546216Z I0705 16:36:10.537435       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/7pl 482\n"
Jul  5 16:36:10.660: INFO: got output "2022-07-05T16:36:10.537546216Z I0705 16:36:10.537435       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/7pl 482\n"
STEP: restricting to a time range
Jul  5 16:36:13.161: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator --since=1s'
Jul  5 16:36:13.246: INFO: stderr: ""
Jul  5 16:36:13.246: INFO: stdout: "I0705 16:36:12.337184       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/x4zl 265\nI0705 16:36:12.537474       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/8pk 574\nI0705 16:36:12.736730       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/xfkl 358\nI0705 16:36:12.937063       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/hf2 224\nI0705 16:36:13.137368       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/l8pt 322\n"
Jul  5 16:36:13.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 logs logs-generator logs-generator --since=24h'
Jul  5 16:36:13.362: INFO: stderr: ""
Jul  5 16:36:13.362: INFO: stdout: "I0705 16:36:08.736712       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dczg 430\nI0705 16:36:08.937035       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/5r6z 469\nI0705 16:36:09.137336       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/znm 586\nI0705 16:36:09.337641       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/pkg8 388\nI0705 16:36:09.536889       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/zhz8 300\nI0705 16:36:09.737211       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/bkgq 341\nI0705 16:36:09.937534       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/wtjq 402\nI0705 16:36:10.136787       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/l8s 414\nI0705 16:36:10.337096       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/wgq 231\nI0705 16:36:10.537435       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/7pl 482\nI0705 16:36:10.737778       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/qhxx 226\nI0705 16:36:10.937121       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/7hg 391\nI0705 16:36:11.137425       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/z7sp 550\nI0705 16:36:11.337733       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/f4s 542\nI0705 16:36:11.537046       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/wpjx 592\nI0705 16:36:11.737366       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/rtdp 595\nI0705 16:36:11.937688       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/8sn 546\nI0705 16:36:12.136874       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/gxmg 258\nI0705 16:36:12.337184       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/x4zl 265\nI0705 16:36:12.537474       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/8pk 574\nI0705 16:36:12.736730       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/xfkl 358\nI0705 16:36:12.937063       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/hf2 224\nI0705 16:36:13.137368       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/l8pt 322\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jul  5 16:36:13.362: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7180 delete pod logs-generator'
Jul  5 16:36:13.873: INFO: stderr: ""
Jul  5 16:36:13.873: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:36:13.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7180" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":156,"skipped":2917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:36:13.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4672
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4672 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4672;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4672 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4672;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4672.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4672.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4672.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4672.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4672.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4672.svc;check="$$(dig +notcp +noall +answer +search 223.212.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.212.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.212.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.212.223_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4672 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4672;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4672 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4672;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4672.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4672.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4672.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4672.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4672.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4672.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4672.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4672.svc;check="$$(dig +notcp +noall +answer +search 223.212.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.212.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.212.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.212.223_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 16:36:16.169: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.216: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.288: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.299: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.320: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.384: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.394: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.405: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.417: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.427: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.438: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.449: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.460: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:16.503: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:21.515: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.526: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.573: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.595: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.616: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.627: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.684: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.695: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.706: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.729: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.740: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.751: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.763: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:21.852: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:26.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.527: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.573: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.594: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.605: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.616: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.627: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.682: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.692: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.703: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.714: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.725: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.735: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.746: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.757: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:26.801: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:31.517: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.570: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.609: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.620: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.631: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.644: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.656: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.767: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.780: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.792: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.805: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.816: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.842: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.853: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:31.898: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:36.516: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.561: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.572: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.595: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.607: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.618: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.685: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.697: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.708: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.731: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.742: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.753: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.765: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:36.812: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:41.515: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.526: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.577: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.598: INFO: Unable to read wheezy_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.609: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.620: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.631: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.695: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.708: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.719: INFO: Unable to read jessie_udp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672 from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.741: INFO: Unable to read jessie_udp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.753: INFO: Unable to read jessie_tcp@dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.763: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.774: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc from pod dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90: the server could not find the requested resource (get pods dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90)
Jul  5 16:36:41.818: INFO: Lookups using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4672 wheezy_tcp@dns-test-service.dns-4672 wheezy_udp@dns-test-service.dns-4672.svc wheezy_tcp@dns-test-service.dns-4672.svc wheezy_udp@_http._tcp.dns-test-service.dns-4672.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4672.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4672 jessie_tcp@dns-test-service.dns-4672 jessie_udp@dns-test-service.dns-4672.svc jessie_tcp@dns-test-service.dns-4672.svc jessie_udp@_http._tcp.dns-test-service.dns-4672.svc jessie_tcp@_http._tcp.dns-test-service.dns-4672.svc]

Jul  5 16:36:46.885: INFO: DNS probes using dns-4672/dns-test-f77ef91f-b60e-4f5d-8b2f-a291334ddf90 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 16:36:46.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4672" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":157,"skipped":2955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:36:46.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5254
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-hkzn
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 16:36:47.127: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hkzn" in namespace "subpath-5254" to be "Succeeded or Failed"
Jul  5 16:36:47.134: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23427ms
Jul  5 16:36:49.142: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.014181493s
Jul  5 16:36:51.149: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 4.021146162s
Jul  5 16:36:53.156: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 6.028418846s
Jul  5 16:36:55.164: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 8.036437662s
Jul  5 16:36:57.172: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 10.044631745s
Jul  5 16:36:59.181: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 12.053505835s
Jul  5 16:37:01.189: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 14.061719252s
Jul  5 16:37:03.197: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 16.069053776s
Jul  5 16:37:05.204: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 18.076881204s
Jul  5 16:37:07.213: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=true. Elapsed: 20.085297189s
Jul  5 16:37:09.220: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Running", Reason="", readiness=false. Elapsed: 22.091999697s
Jul  5 16:37:11.228: INFO: Pod "pod-subpath-test-downwardapi-hkzn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.100038057s
STEP: Saw pod success
Jul  5 16:37:11.228: INFO: Pod "pod-subpath-test-downwardapi-hkzn" satisfied condition "Succeeded or Failed"
Jul  5 16:37:11.234: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-subpath-test-downwardapi-hkzn container test-container-subpath-downwardapi-hkzn: <nil>
STEP: delete the pod
Jul  5 16:37:11.259: INFO: Waiting for pod pod-subpath-test-downwardapi-hkzn to disappear
Jul  5 16:37:11.266: INFO: Pod pod-subpath-test-downwardapi-hkzn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-hkzn
Jul  5 16:37:11.266: INFO: Deleting pod "pod-subpath-test-downwardapi-hkzn" in namespace "subpath-5254"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jul  5 16:37:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5254" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":158,"skipped":3004,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:37:11.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7476
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-63acb0f5-e971-4c84-b546-374b705e57b6
STEP: Creating secret with name secret-projected-all-test-volume-1e7abf7c-c356-44e1-bc8d-f143eae8b0ac
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  5 16:37:11.471: INFO: Waiting up to 5m0s for pod "projected-volume-18a318ce-938a-4c9e-864e-4793e798f547" in namespace "projected-7476" to be "Succeeded or Failed"
Jul  5 16:37:11.477: INFO: Pod "projected-volume-18a318ce-938a-4c9e-864e-4793e798f547": Phase="Pending", Reason="", readiness=false. Elapsed: 6.531011ms
Jul  5 16:37:13.485: INFO: Pod "projected-volume-18a318ce-938a-4c9e-864e-4793e798f547": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014548258s
Jul  5 16:37:15.493: INFO: Pod "projected-volume-18a318ce-938a-4c9e-864e-4793e798f547": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02232604s
STEP: Saw pod success
Jul  5 16:37:15.493: INFO: Pod "projected-volume-18a318ce-938a-4c9e-864e-4793e798f547" satisfied condition "Succeeded or Failed"
Jul  5 16:37:15.500: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod projected-volume-18a318ce-938a-4c9e-864e-4793e798f547 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  5 16:37:15.524: INFO: Waiting for pod projected-volume-18a318ce-938a-4c9e-864e-4793e798f547 to disappear
Jul  5 16:37:15.530: INFO: Pod projected-volume-18a318ce-938a-4c9e-864e-4793e798f547 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jul  5 16:37:15.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7476" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":159,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:37:15.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-3175
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jul  5 16:37:15.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3175" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":160,"skipped":3069,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:37:15.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9861
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul  5 16:37:15.931: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:38:15.994: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:16.001: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4075
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:38:16.189: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul  5 16:38:16.199: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jul  5 16:38:16.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4075" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:38:16.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9861" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":161,"skipped":3078,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:16.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4025
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:38:16.490: INFO: The status of Pod busybox-host-aliases6cd2f585-7137-46d5-a089-c9a2b717aeb2 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:38:18.498: INFO: The status of Pod busybox-host-aliases6cd2f585-7137-46d5-a089-c9a2b717aeb2 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jul  5 16:38:18.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4025" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":162,"skipped":3098,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:18.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-37
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:38:18.705: INFO: Waiting up to 5m0s for pod "downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7" in namespace "projected-37" to be "Succeeded or Failed"
Jul  5 16:38:18.711: INFO: Pod "downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.476476ms
Jul  5 16:38:20.732: INFO: Pod "downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026728464s
Jul  5 16:38:22.740: INFO: Pod "downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034985552s
STEP: Saw pod success
Jul  5 16:38:22.740: INFO: Pod "downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7" satisfied condition "Succeeded or Failed"
Jul  5 16:38:22.746: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7 container client-container: <nil>
STEP: delete the pod
Jul  5 16:38:22.772: INFO: Waiting for pod downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7 to disappear
Jul  5 16:38:22.778: INFO: Pod downwardapi-volume-253c17b2-ac13-4c93-8a38-e1ff253010f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:38:22.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-37" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3103,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:22.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-3849
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:38:22.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3849
I0705 16:38:22.959241    6089 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3849, replica count: 1
I0705 16:38:24.009593    6089 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 16:38:25.010712    6089 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:38:25.122: INFO: Created: latency-svc-b5n2v
Jul  5 16:38:25.126: INFO: Got endpoints: latency-svc-b5n2v [15.34607ms]
Jul  5 16:38:25.136: INFO: Created: latency-svc-sz66f
Jul  5 16:38:25.138: INFO: Got endpoints: latency-svc-sz66f [11.886315ms]
Jul  5 16:38:25.139: INFO: Created: latency-svc-nkxgl
Jul  5 16:38:25.142: INFO: Got endpoints: latency-svc-nkxgl [15.306244ms]
Jul  5 16:38:25.142: INFO: Created: latency-svc-6zp46
Jul  5 16:38:25.144: INFO: Got endpoints: latency-svc-6zp46 [17.708467ms]
Jul  5 16:38:25.145: INFO: Created: latency-svc-4tgsl
Jul  5 16:38:25.147: INFO: Got endpoints: latency-svc-4tgsl [20.084238ms]
Jul  5 16:38:25.149: INFO: Created: latency-svc-tbnlg
Jul  5 16:38:25.159: INFO: Got endpoints: latency-svc-tbnlg [31.950624ms]
Jul  5 16:38:25.159: INFO: Created: latency-svc-j8rng
Jul  5 16:38:25.162: INFO: Got endpoints: latency-svc-j8rng [35.412732ms]
Jul  5 16:38:25.162: INFO: Created: latency-svc-t92qd
Jul  5 16:38:25.166: INFO: Got endpoints: latency-svc-t92qd [38.981872ms]
Jul  5 16:38:25.166: INFO: Created: latency-svc-4mnvq
Jul  5 16:38:25.169: INFO: Got endpoints: latency-svc-4mnvq [42.474419ms]
Jul  5 16:38:25.169: INFO: Created: latency-svc-pv8sn
Jul  5 16:38:25.172: INFO: Got endpoints: latency-svc-pv8sn [44.887138ms]
Jul  5 16:38:25.210: INFO: Created: latency-svc-wghjh
Jul  5 16:38:25.210: INFO: Created: latency-svc-ww2qv
Jul  5 16:38:25.210: INFO: Created: latency-svc-rqzzz
Jul  5 16:38:25.210: INFO: Created: latency-svc-9cq9q
Jul  5 16:38:25.210: INFO: Created: latency-svc-bckd7
Jul  5 16:38:25.210: INFO: Created: latency-svc-h25xt
Jul  5 16:38:25.210: INFO: Created: latency-svc-ndgv8
Jul  5 16:38:25.211: INFO: Created: latency-svc-vbfdk
Jul  5 16:38:25.211: INFO: Created: latency-svc-27cdg
Jul  5 16:38:25.211: INFO: Created: latency-svc-7gvw2
Jul  5 16:38:25.211: INFO: Created: latency-svc-ltcdc
Jul  5 16:38:25.214: INFO: Created: latency-svc-glgz2
Jul  5 16:38:25.214: INFO: Created: latency-svc-vgw8z
Jul  5 16:38:25.214: INFO: Created: latency-svc-dvkb7
Jul  5 16:38:25.214: INFO: Created: latency-svc-c9kzb
Jul  5 16:38:25.214: INFO: Got endpoints: latency-svc-9cq9q [87.551862ms]
Jul  5 16:38:25.214: INFO: Got endpoints: latency-svc-wghjh [48.67247ms]
Jul  5 16:38:25.214: INFO: Got endpoints: latency-svc-bckd7 [45.542906ms]
Jul  5 16:38:25.215: INFO: Got endpoints: latency-svc-rqzzz [55.939936ms]
Jul  5 16:38:25.216: INFO: Got endpoints: latency-svc-ndgv8 [89.729618ms]
Jul  5 16:38:25.216: INFO: Got endpoints: latency-svc-h25xt [89.822547ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-vbfdk [44.976351ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-ww2qv [89.87597ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-27cdg [78.324746ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-ltcdc [54.946495ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-7gvw2 [75.248049ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-dvkb7 [70.514062ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-glgz2 [90.538785ms]
Jul  5 16:38:25.217: INFO: Got endpoints: latency-svc-c9kzb [72.930085ms]
Jul  5 16:38:25.218: INFO: Got endpoints: latency-svc-vgw8z [91.035153ms]
Jul  5 16:38:25.223: INFO: Created: latency-svc-4qnnx
Jul  5 16:38:25.226: INFO: Got endpoints: latency-svc-4qnnx [11.669368ms]
Jul  5 16:38:25.280: INFO: Created: latency-svc-7k25f
Jul  5 16:38:25.281: INFO: Created: latency-svc-thdx7
Jul  5 16:38:25.282: INFO: Created: latency-svc-w4d8c
Jul  5 16:38:25.282: INFO: Created: latency-svc-wpth4
Jul  5 16:38:25.282: INFO: Created: latency-svc-2c7w6
Jul  5 16:38:25.282: INFO: Created: latency-svc-8qg4r
Jul  5 16:38:25.282: INFO: Created: latency-svc-zcc54
Jul  5 16:38:25.282: INFO: Created: latency-svc-vmwkl
Jul  5 16:38:25.283: INFO: Created: latency-svc-jhlq9
Jul  5 16:38:25.284: INFO: Created: latency-svc-gvl9j
Jul  5 16:38:25.284: INFO: Created: latency-svc-d92vb
Jul  5 16:38:25.285: INFO: Created: latency-svc-lm9nm
Jul  5 16:38:25.285: INFO: Created: latency-svc-xbpnx
Jul  5 16:38:25.285: INFO: Created: latency-svc-5ldjx
Jul  5 16:38:25.285: INFO: Created: latency-svc-qhntd
Jul  5 16:38:25.291: INFO: Got endpoints: latency-svc-thdx7 [76.089174ms]
Jul  5 16:38:25.291: INFO: Got endpoints: latency-svc-gvl9j [73.33903ms]
Jul  5 16:38:25.295: INFO: Got endpoints: latency-svc-7k25f [77.554038ms]
Jul  5 16:38:25.295: INFO: Got endpoints: latency-svc-8qg4r [80.196931ms]
Jul  5 16:38:25.297: INFO: Got endpoints: latency-svc-vmwkl [70.48459ms]
Jul  5 16:38:25.299: INFO: Got endpoints: latency-svc-2c7w6 [81.890368ms]
Jul  5 16:38:25.300: INFO: Got endpoints: latency-svc-wpth4 [82.420662ms]
Jul  5 16:38:25.311: INFO: Created: latency-svc-vhwtc
Jul  5 16:38:25.313: INFO: Created: latency-svc-hlg2f
Jul  5 16:38:25.318: INFO: Created: latency-svc-dsjrm
Jul  5 16:38:25.325: INFO: Created: latency-svc-c9nq7
Jul  5 16:38:25.330: INFO: Got endpoints: latency-svc-w4d8c [113.218102ms]
Jul  5 16:38:25.331: INFO: Created: latency-svc-4mpgl
Jul  5 16:38:25.333: INFO: Created: latency-svc-kjrrk
Jul  5 16:38:25.337: INFO: Created: latency-svc-rg6ng
Jul  5 16:38:25.347: INFO: Created: latency-svc-lwbnb
Jul  5 16:38:25.382: INFO: Got endpoints: latency-svc-d92vb [165.232664ms]
Jul  5 16:38:25.396: INFO: Created: latency-svc-5hzl8
Jul  5 16:38:25.426: INFO: Got endpoints: latency-svc-lm9nm [208.919451ms]
Jul  5 16:38:25.437: INFO: Created: latency-svc-45wr5
Jul  5 16:38:25.475: INFO: Got endpoints: latency-svc-zcc54 [258.055651ms]
Jul  5 16:38:25.485: INFO: Created: latency-svc-drsvx
Jul  5 16:38:25.526: INFO: Got endpoints: latency-svc-qhntd [309.192997ms]
Jul  5 16:38:25.535: INFO: Created: latency-svc-7vhgv
Jul  5 16:38:25.576: INFO: Got endpoints: latency-svc-xbpnx [359.418771ms]
Jul  5 16:38:25.585: INFO: Created: latency-svc-r779q
Jul  5 16:38:25.625: INFO: Got endpoints: latency-svc-jhlq9 [411.127217ms]
Jul  5 16:38:25.635: INFO: Created: latency-svc-92cf7
Jul  5 16:38:25.675: INFO: Got endpoints: latency-svc-5ldjx [457.191138ms]
Jul  5 16:38:25.686: INFO: Created: latency-svc-dfrpc
Jul  5 16:38:25.725: INFO: Got endpoints: latency-svc-vhwtc [434.527459ms]
Jul  5 16:38:25.735: INFO: Created: latency-svc-hstg6
Jul  5 16:38:25.776: INFO: Got endpoints: latency-svc-hlg2f [485.153409ms]
Jul  5 16:38:25.786: INFO: Created: latency-svc-qqdwb
Jul  5 16:38:25.826: INFO: Got endpoints: latency-svc-dsjrm [531.508697ms]
Jul  5 16:38:25.836: INFO: Created: latency-svc-wcrxr
Jul  5 16:38:25.876: INFO: Got endpoints: latency-svc-c9nq7 [580.801703ms]
Jul  5 16:38:25.887: INFO: Created: latency-svc-sg6db
Jul  5 16:38:25.926: INFO: Got endpoints: latency-svc-4mpgl [629.275571ms]
Jul  5 16:38:25.936: INFO: Created: latency-svc-t4c6n
Jul  5 16:38:25.977: INFO: Got endpoints: latency-svc-kjrrk [678.328712ms]
Jul  5 16:38:25.989: INFO: Created: latency-svc-mjvcn
Jul  5 16:38:26.025: INFO: Got endpoints: latency-svc-rg6ng [725.588327ms]
Jul  5 16:38:26.035: INFO: Created: latency-svc-mbtd7
Jul  5 16:38:26.076: INFO: Got endpoints: latency-svc-lwbnb [745.162113ms]
Jul  5 16:38:26.085: INFO: Created: latency-svc-l9xlp
Jul  5 16:38:26.126: INFO: Got endpoints: latency-svc-5hzl8 [743.685291ms]
Jul  5 16:38:26.136: INFO: Created: latency-svc-sdhcd
Jul  5 16:38:26.176: INFO: Got endpoints: latency-svc-45wr5 [750.346474ms]
Jul  5 16:38:26.185: INFO: Created: latency-svc-pjlc7
Jul  5 16:38:26.226: INFO: Got endpoints: latency-svc-drsvx [750.772009ms]
Jul  5 16:38:26.236: INFO: Created: latency-svc-t6hvz
Jul  5 16:38:26.276: INFO: Got endpoints: latency-svc-7vhgv [749.788776ms]
Jul  5 16:38:26.285: INFO: Created: latency-svc-jk44t
Jul  5 16:38:26.327: INFO: Got endpoints: latency-svc-r779q [750.888928ms]
Jul  5 16:38:26.337: INFO: Created: latency-svc-4lz5x
Jul  5 16:38:26.376: INFO: Got endpoints: latency-svc-92cf7 [750.38778ms]
Jul  5 16:38:26.386: INFO: Created: latency-svc-sgqfh
Jul  5 16:38:26.426: INFO: Got endpoints: latency-svc-dfrpc [750.804363ms]
Jul  5 16:38:26.436: INFO: Created: latency-svc-rs8cc
Jul  5 16:38:26.476: INFO: Got endpoints: latency-svc-hstg6 [750.758871ms]
Jul  5 16:38:26.487: INFO: Created: latency-svc-hp97c
Jul  5 16:38:26.526: INFO: Got endpoints: latency-svc-qqdwb [749.803102ms]
Jul  5 16:38:26.536: INFO: Created: latency-svc-kc4f9
Jul  5 16:38:26.576: INFO: Got endpoints: latency-svc-wcrxr [749.242691ms]
Jul  5 16:38:26.586: INFO: Created: latency-svc-x4gpt
Jul  5 16:38:26.626: INFO: Got endpoints: latency-svc-sg6db [750.491059ms]
Jul  5 16:38:26.636: INFO: Created: latency-svc-mv4j4
Jul  5 16:38:26.675: INFO: Got endpoints: latency-svc-t4c6n [749.205752ms]
Jul  5 16:38:26.685: INFO: Created: latency-svc-ft57f
Jul  5 16:38:26.726: INFO: Got endpoints: latency-svc-mjvcn [748.990924ms]
Jul  5 16:38:26.736: INFO: Created: latency-svc-444dq
Jul  5 16:38:26.776: INFO: Got endpoints: latency-svc-mbtd7 [751.037241ms]
Jul  5 16:38:26.786: INFO: Created: latency-svc-pd4f8
Jul  5 16:38:26.826: INFO: Got endpoints: latency-svc-l9xlp [750.362794ms]
Jul  5 16:38:26.836: INFO: Created: latency-svc-gcvvv
Jul  5 16:38:26.876: INFO: Got endpoints: latency-svc-sdhcd [750.239234ms]
Jul  5 16:38:26.888: INFO: Created: latency-svc-ghckb
Jul  5 16:38:26.931: INFO: Got endpoints: latency-svc-pjlc7 [754.901272ms]
Jul  5 16:38:26.954: INFO: Created: latency-svc-qldkj
Jul  5 16:38:26.975: INFO: Got endpoints: latency-svc-t6hvz [749.117158ms]
Jul  5 16:38:26.985: INFO: Created: latency-svc-4rpcc
Jul  5 16:38:27.026: INFO: Got endpoints: latency-svc-jk44t [750.354152ms]
Jul  5 16:38:27.036: INFO: Created: latency-svc-lqnrm
Jul  5 16:38:27.076: INFO: Got endpoints: latency-svc-4lz5x [749.142407ms]
Jul  5 16:38:27.086: INFO: Created: latency-svc-lf777
Jul  5 16:38:27.125: INFO: Got endpoints: latency-svc-sgqfh [748.898864ms]
Jul  5 16:38:27.134: INFO: Created: latency-svc-xrlxm
Jul  5 16:38:27.175: INFO: Got endpoints: latency-svc-rs8cc [749.546322ms]
Jul  5 16:38:27.185: INFO: Created: latency-svc-tn54c
Jul  5 16:38:27.226: INFO: Got endpoints: latency-svc-hp97c [749.503569ms]
Jul  5 16:38:27.235: INFO: Created: latency-svc-ctnmx
Jul  5 16:38:27.276: INFO: Got endpoints: latency-svc-kc4f9 [750.515163ms]
Jul  5 16:38:27.286: INFO: Created: latency-svc-mmjsh
Jul  5 16:38:27.325: INFO: Got endpoints: latency-svc-x4gpt [749.746018ms]
Jul  5 16:38:27.335: INFO: Created: latency-svc-hrkgh
Jul  5 16:38:27.375: INFO: Got endpoints: latency-svc-mv4j4 [749.0167ms]
Jul  5 16:38:27.385: INFO: Created: latency-svc-f4tkf
Jul  5 16:38:27.426: INFO: Got endpoints: latency-svc-ft57f [750.578165ms]
Jul  5 16:38:27.436: INFO: Created: latency-svc-pxt22
Jul  5 16:38:27.475: INFO: Got endpoints: latency-svc-444dq [749.207831ms]
Jul  5 16:38:27.485: INFO: Created: latency-svc-psd5w
Jul  5 16:38:27.525: INFO: Got endpoints: latency-svc-pd4f8 [748.182364ms]
Jul  5 16:38:27.534: INFO: Created: latency-svc-gfzz5
Jul  5 16:38:27.575: INFO: Got endpoints: latency-svc-gcvvv [749.233796ms]
Jul  5 16:38:27.585: INFO: Created: latency-svc-srss7
Jul  5 16:38:27.625: INFO: Got endpoints: latency-svc-ghckb [749.096613ms]
Jul  5 16:38:27.635: INFO: Created: latency-svc-s8k7c
Jul  5 16:38:27.676: INFO: Got endpoints: latency-svc-qldkj [744.652988ms]
Jul  5 16:38:27.685: INFO: Created: latency-svc-42dfr
Jul  5 16:38:27.726: INFO: Got endpoints: latency-svc-4rpcc [750.662056ms]
Jul  5 16:38:27.736: INFO: Created: latency-svc-gttkz
Jul  5 16:38:27.776: INFO: Got endpoints: latency-svc-lqnrm [749.516826ms]
Jul  5 16:38:27.785: INFO: Created: latency-svc-wvhgt
Jul  5 16:38:27.824: INFO: Got endpoints: latency-svc-lf777 [748.227349ms]
Jul  5 16:38:27.834: INFO: Created: latency-svc-nkpw2
Jul  5 16:38:27.876: INFO: Got endpoints: latency-svc-xrlxm [750.639507ms]
Jul  5 16:38:27.886: INFO: Created: latency-svc-7vwnv
Jul  5 16:38:27.926: INFO: Got endpoints: latency-svc-tn54c [750.055876ms]
Jul  5 16:38:27.935: INFO: Created: latency-svc-7cth8
Jul  5 16:38:27.975: INFO: Got endpoints: latency-svc-ctnmx [749.851264ms]
Jul  5 16:38:27.985: INFO: Created: latency-svc-qdp28
Jul  5 16:38:28.025: INFO: Got endpoints: latency-svc-mmjsh [748.341386ms]
Jul  5 16:38:28.035: INFO: Created: latency-svc-ljpjq
Jul  5 16:38:28.076: INFO: Got endpoints: latency-svc-hrkgh [750.355685ms]
Jul  5 16:38:28.085: INFO: Created: latency-svc-2c88k
Jul  5 16:38:28.125: INFO: Got endpoints: latency-svc-f4tkf [750.057477ms]
Jul  5 16:38:28.136: INFO: Created: latency-svc-tg22k
Jul  5 16:38:28.176: INFO: Got endpoints: latency-svc-pxt22 [750.374321ms]
Jul  5 16:38:28.186: INFO: Created: latency-svc-nfjcn
Jul  5 16:38:28.225: INFO: Got endpoints: latency-svc-psd5w [749.902789ms]
Jul  5 16:38:28.236: INFO: Created: latency-svc-h5qj2
Jul  5 16:38:28.275: INFO: Got endpoints: latency-svc-gfzz5 [750.099149ms]
Jul  5 16:38:28.284: INFO: Created: latency-svc-729vf
Jul  5 16:38:28.326: INFO: Got endpoints: latency-svc-srss7 [750.177144ms]
Jul  5 16:38:28.335: INFO: Created: latency-svc-7hld6
Jul  5 16:38:28.376: INFO: Got endpoints: latency-svc-s8k7c [750.650655ms]
Jul  5 16:38:28.386: INFO: Created: latency-svc-nxpvd
Jul  5 16:38:28.427: INFO: Got endpoints: latency-svc-42dfr [751.448173ms]
Jul  5 16:38:28.437: INFO: Created: latency-svc-94gnh
Jul  5 16:38:28.480: INFO: Got endpoints: latency-svc-gttkz [753.876896ms]
Jul  5 16:38:28.493: INFO: Created: latency-svc-crjnw
Jul  5 16:38:28.527: INFO: Got endpoints: latency-svc-wvhgt [751.635511ms]
Jul  5 16:38:28.537: INFO: Created: latency-svc-cdcmn
Jul  5 16:38:28.576: INFO: Got endpoints: latency-svc-nkpw2 [751.900529ms]
Jul  5 16:38:28.586: INFO: Created: latency-svc-4pjsl
Jul  5 16:38:28.625: INFO: Got endpoints: latency-svc-7vwnv [749.316271ms]
Jul  5 16:38:28.635: INFO: Created: latency-svc-pxc6p
Jul  5 16:38:28.676: INFO: Got endpoints: latency-svc-7cth8 [750.390997ms]
Jul  5 16:38:28.686: INFO: Created: latency-svc-rzkjv
Jul  5 16:38:28.725: INFO: Got endpoints: latency-svc-qdp28 [749.64303ms]
Jul  5 16:38:28.735: INFO: Created: latency-svc-p8zs5
Jul  5 16:38:28.775: INFO: Got endpoints: latency-svc-ljpjq [750.233587ms]
Jul  5 16:38:28.784: INFO: Created: latency-svc-qp7r7
Jul  5 16:38:28.825: INFO: Got endpoints: latency-svc-2c88k [748.946914ms]
Jul  5 16:38:28.835: INFO: Created: latency-svc-scqbz
Jul  5 16:38:28.876: INFO: Got endpoints: latency-svc-tg22k [750.356856ms]
Jul  5 16:38:28.886: INFO: Created: latency-svc-5k4qc
Jul  5 16:38:28.926: INFO: Got endpoints: latency-svc-nfjcn [749.779715ms]
Jul  5 16:38:28.936: INFO: Created: latency-svc-cq2hg
Jul  5 16:38:28.976: INFO: Got endpoints: latency-svc-h5qj2 [750.508775ms]
Jul  5 16:38:28.985: INFO: Created: latency-svc-8h8zn
Jul  5 16:38:29.028: INFO: Got endpoints: latency-svc-729vf [753.569206ms]
Jul  5 16:38:29.039: INFO: Created: latency-svc-qs85l
Jul  5 16:38:29.076: INFO: Got endpoints: latency-svc-7hld6 [750.406844ms]
Jul  5 16:38:29.086: INFO: Created: latency-svc-4zqj5
Jul  5 16:38:29.125: INFO: Got endpoints: latency-svc-nxpvd [749.317572ms]
Jul  5 16:38:29.135: INFO: Created: latency-svc-tjv2k
Jul  5 16:38:29.176: INFO: Got endpoints: latency-svc-94gnh [748.619649ms]
Jul  5 16:38:29.186: INFO: Created: latency-svc-srqxc
Jul  5 16:38:29.226: INFO: Got endpoints: latency-svc-crjnw [745.760645ms]
Jul  5 16:38:29.235: INFO: Created: latency-svc-dwjjw
Jul  5 16:38:29.275: INFO: Got endpoints: latency-svc-cdcmn [747.904024ms]
Jul  5 16:38:29.285: INFO: Created: latency-svc-99jzq
Jul  5 16:38:29.326: INFO: Got endpoints: latency-svc-4pjsl [749.758147ms]
Jul  5 16:38:29.336: INFO: Created: latency-svc-7p5bb
Jul  5 16:38:29.376: INFO: Got endpoints: latency-svc-pxc6p [751.216849ms]
Jul  5 16:38:29.387: INFO: Created: latency-svc-dqkfv
Jul  5 16:38:29.426: INFO: Got endpoints: latency-svc-rzkjv [750.035598ms]
Jul  5 16:38:29.436: INFO: Created: latency-svc-zncrx
Jul  5 16:38:29.476: INFO: Got endpoints: latency-svc-p8zs5 [750.908963ms]
Jul  5 16:38:29.486: INFO: Created: latency-svc-bmp6j
Jul  5 16:38:29.526: INFO: Got endpoints: latency-svc-qp7r7 [750.647892ms]
Jul  5 16:38:29.546: INFO: Created: latency-svc-d2tm9
Jul  5 16:38:29.576: INFO: Got endpoints: latency-svc-scqbz [751.024013ms]
Jul  5 16:38:29.586: INFO: Created: latency-svc-wfx68
Jul  5 16:38:29.626: INFO: Got endpoints: latency-svc-5k4qc [749.942542ms]
Jul  5 16:38:29.636: INFO: Created: latency-svc-9ghcz
Jul  5 16:38:29.676: INFO: Got endpoints: latency-svc-cq2hg [749.498215ms]
Jul  5 16:38:29.685: INFO: Created: latency-svc-xzbf6
Jul  5 16:38:29.725: INFO: Got endpoints: latency-svc-8h8zn [749.534555ms]
Jul  5 16:38:29.736: INFO: Created: latency-svc-s4gh4
Jul  5 16:38:29.776: INFO: Got endpoints: latency-svc-qs85l [747.092504ms]
Jul  5 16:38:29.788: INFO: Created: latency-svc-tvr66
Jul  5 16:38:29.826: INFO: Got endpoints: latency-svc-4zqj5 [749.386107ms]
Jul  5 16:38:29.835: INFO: Created: latency-svc-t7js6
Jul  5 16:38:29.876: INFO: Got endpoints: latency-svc-tjv2k [750.637724ms]
Jul  5 16:38:29.886: INFO: Created: latency-svc-d2p45
Jul  5 16:38:29.927: INFO: Got endpoints: latency-svc-srqxc [750.662068ms]
Jul  5 16:38:29.936: INFO: Created: latency-svc-52msz
Jul  5 16:38:29.976: INFO: Got endpoints: latency-svc-dwjjw [750.267669ms]
Jul  5 16:38:29.986: INFO: Created: latency-svc-mzj9j
Jul  5 16:38:30.026: INFO: Got endpoints: latency-svc-99jzq [750.243408ms]
Jul  5 16:38:30.039: INFO: Created: latency-svc-nhp26
Jul  5 16:38:30.083: INFO: Got endpoints: latency-svc-7p5bb [756.908251ms]
Jul  5 16:38:30.106: INFO: Created: latency-svc-cqdlf
Jul  5 16:38:30.129: INFO: Got endpoints: latency-svc-dqkfv [753.121548ms]
Jul  5 16:38:30.142: INFO: Created: latency-svc-gqv7l
Jul  5 16:38:30.176: INFO: Got endpoints: latency-svc-zncrx [749.900081ms]
Jul  5 16:38:30.186: INFO: Created: latency-svc-4w8hf
Jul  5 16:38:30.226: INFO: Got endpoints: latency-svc-bmp6j [749.370149ms]
Jul  5 16:38:30.236: INFO: Created: latency-svc-z5jgb
Jul  5 16:38:30.275: INFO: Got endpoints: latency-svc-d2tm9 [749.426179ms]
Jul  5 16:38:30.286: INFO: Created: latency-svc-chcwx
Jul  5 16:38:30.326: INFO: Got endpoints: latency-svc-wfx68 [750.032362ms]
Jul  5 16:38:30.337: INFO: Created: latency-svc-xhfnm
Jul  5 16:38:30.376: INFO: Got endpoints: latency-svc-9ghcz [749.964509ms]
Jul  5 16:38:30.387: INFO: Created: latency-svc-vnmxj
Jul  5 16:38:30.425: INFO: Got endpoints: latency-svc-xzbf6 [749.690119ms]
Jul  5 16:38:30.435: INFO: Created: latency-svc-swbld
Jul  5 16:38:30.475: INFO: Got endpoints: latency-svc-s4gh4 [749.237327ms]
Jul  5 16:38:30.484: INFO: Created: latency-svc-t84pn
Jul  5 16:38:30.525: INFO: Got endpoints: latency-svc-tvr66 [749.271417ms]
Jul  5 16:38:30.535: INFO: Created: latency-svc-sj7x2
Jul  5 16:38:30.575: INFO: Got endpoints: latency-svc-t7js6 [749.43884ms]
Jul  5 16:38:30.586: INFO: Created: latency-svc-frdvx
Jul  5 16:38:30.625: INFO: Got endpoints: latency-svc-d2p45 [748.975563ms]
Jul  5 16:38:30.642: INFO: Created: latency-svc-l9grx
Jul  5 16:38:30.676: INFO: Got endpoints: latency-svc-52msz [749.115086ms]
Jul  5 16:38:30.686: INFO: Created: latency-svc-2txbs
Jul  5 16:38:30.727: INFO: Got endpoints: latency-svc-mzj9j [750.663082ms]
Jul  5 16:38:30.736: INFO: Created: latency-svc-ld8zr
Jul  5 16:38:30.775: INFO: Got endpoints: latency-svc-nhp26 [749.677886ms]
Jul  5 16:38:30.785: INFO: Created: latency-svc-t42tn
Jul  5 16:38:30.826: INFO: Got endpoints: latency-svc-cqdlf [742.935866ms]
Jul  5 16:38:30.837: INFO: Created: latency-svc-gl4gm
Jul  5 16:38:30.875: INFO: Got endpoints: latency-svc-gqv7l [745.449642ms]
Jul  5 16:38:30.885: INFO: Created: latency-svc-k5b5b
Jul  5 16:38:30.926: INFO: Got endpoints: latency-svc-4w8hf [749.483285ms]
Jul  5 16:38:30.936: INFO: Created: latency-svc-mfmqm
Jul  5 16:38:30.987: INFO: Got endpoints: latency-svc-z5jgb [761.677811ms]
Jul  5 16:38:30.998: INFO: Created: latency-svc-rnjx6
Jul  5 16:38:31.025: INFO: Got endpoints: latency-svc-chcwx [750.154293ms]
Jul  5 16:38:31.035: INFO: Created: latency-svc-nkcrs
Jul  5 16:38:31.076: INFO: Got endpoints: latency-svc-xhfnm [749.624449ms]
Jul  5 16:38:31.086: INFO: Created: latency-svc-9pxhh
Jul  5 16:38:31.125: INFO: Got endpoints: latency-svc-vnmxj [749.238881ms]
Jul  5 16:38:31.135: INFO: Created: latency-svc-hfdrw
Jul  5 16:38:31.175: INFO: Got endpoints: latency-svc-swbld [749.786711ms]
Jul  5 16:38:31.185: INFO: Created: latency-svc-wkmb2
Jul  5 16:38:31.225: INFO: Got endpoints: latency-svc-t84pn [750.676322ms]
Jul  5 16:38:31.235: INFO: Created: latency-svc-mb7d4
Jul  5 16:38:31.275: INFO: Got endpoints: latency-svc-sj7x2 [749.982145ms]
Jul  5 16:38:31.285: INFO: Created: latency-svc-cptdg
Jul  5 16:38:31.327: INFO: Got endpoints: latency-svc-frdvx [751.552043ms]
Jul  5 16:38:31.337: INFO: Created: latency-svc-d75gx
Jul  5 16:38:31.376: INFO: Got endpoints: latency-svc-l9grx [751.024805ms]
Jul  5 16:38:31.386: INFO: Created: latency-svc-xrc2l
Jul  5 16:38:31.425: INFO: Got endpoints: latency-svc-2txbs [749.419206ms]
Jul  5 16:38:31.435: INFO: Created: latency-svc-5b7gd
Jul  5 16:38:31.475: INFO: Got endpoints: latency-svc-ld8zr [748.41049ms]
Jul  5 16:38:31.485: INFO: Created: latency-svc-kbtnn
Jul  5 16:38:31.525: INFO: Got endpoints: latency-svc-t42tn [749.5309ms]
Jul  5 16:38:31.535: INFO: Created: latency-svc-4rzjv
Jul  5 16:38:31.575: INFO: Got endpoints: latency-svc-gl4gm [748.431854ms]
Jul  5 16:38:31.585: INFO: Created: latency-svc-tvhc9
Jul  5 16:38:31.626: INFO: Got endpoints: latency-svc-k5b5b [751.229634ms]
Jul  5 16:38:31.638: INFO: Created: latency-svc-t2qq4
Jul  5 16:38:31.682: INFO: Got endpoints: latency-svc-mfmqm [756.449373ms]
Jul  5 16:38:31.702: INFO: Created: latency-svc-dzdhl
Jul  5 16:38:31.726: INFO: Got endpoints: latency-svc-rnjx6 [738.404836ms]
Jul  5 16:38:31.736: INFO: Created: latency-svc-rs756
Jul  5 16:38:31.776: INFO: Got endpoints: latency-svc-nkcrs [750.155977ms]
Jul  5 16:38:31.786: INFO: Created: latency-svc-66rk6
Jul  5 16:38:31.826: INFO: Got endpoints: latency-svc-9pxhh [750.156001ms]
Jul  5 16:38:31.837: INFO: Created: latency-svc-8v898
Jul  5 16:38:31.876: INFO: Got endpoints: latency-svc-hfdrw [750.894423ms]
Jul  5 16:38:31.886: INFO: Created: latency-svc-s5khk
Jul  5 16:38:31.926: INFO: Got endpoints: latency-svc-wkmb2 [750.297919ms]
Jul  5 16:38:31.935: INFO: Created: latency-svc-5nwgg
Jul  5 16:38:31.976: INFO: Got endpoints: latency-svc-mb7d4 [750.37195ms]
Jul  5 16:38:31.986: INFO: Created: latency-svc-vznvq
Jul  5 16:38:32.026: INFO: Got endpoints: latency-svc-cptdg [750.764245ms]
Jul  5 16:38:32.036: INFO: Created: latency-svc-r5g2m
Jul  5 16:38:32.075: INFO: Got endpoints: latency-svc-d75gx [748.442592ms]
Jul  5 16:38:32.085: INFO: Created: latency-svc-plxrv
Jul  5 16:38:32.126: INFO: Got endpoints: latency-svc-xrc2l [749.740842ms]
Jul  5 16:38:32.136: INFO: Created: latency-svc-h584j
Jul  5 16:38:32.176: INFO: Got endpoints: latency-svc-5b7gd [750.871388ms]
Jul  5 16:38:32.186: INFO: Created: latency-svc-nrk7f
Jul  5 16:38:32.225: INFO: Got endpoints: latency-svc-kbtnn [749.832701ms]
Jul  5 16:38:32.236: INFO: Created: latency-svc-djsck
Jul  5 16:38:32.276: INFO: Got endpoints: latency-svc-4rzjv [750.648593ms]
Jul  5 16:38:32.287: INFO: Created: latency-svc-2hl5d
Jul  5 16:38:32.326: INFO: Got endpoints: latency-svc-tvhc9 [751.477831ms]
Jul  5 16:38:32.339: INFO: Created: latency-svc-zjml5
Jul  5 16:38:32.376: INFO: Got endpoints: latency-svc-t2qq4 [749.923821ms]
Jul  5 16:38:32.386: INFO: Created: latency-svc-npftj
Jul  5 16:38:32.426: INFO: Got endpoints: latency-svc-dzdhl [743.687138ms]
Jul  5 16:38:32.436: INFO: Created: latency-svc-44xhs
Jul  5 16:38:32.476: INFO: Got endpoints: latency-svc-rs756 [749.635835ms]
Jul  5 16:38:32.486: INFO: Created: latency-svc-v2q9r
Jul  5 16:38:32.526: INFO: Got endpoints: latency-svc-66rk6 [749.876307ms]
Jul  5 16:38:32.536: INFO: Created: latency-svc-spkzb
Jul  5 16:38:32.575: INFO: Got endpoints: latency-svc-8v898 [749.261593ms]
Jul  5 16:38:32.585: INFO: Created: latency-svc-84brc
Jul  5 16:38:32.626: INFO: Got endpoints: latency-svc-s5khk [750.368367ms]
Jul  5 16:38:32.636: INFO: Created: latency-svc-nmw7z
Jul  5 16:38:32.676: INFO: Got endpoints: latency-svc-5nwgg [749.969633ms]
Jul  5 16:38:32.685: INFO: Created: latency-svc-6s92n
Jul  5 16:38:32.726: INFO: Got endpoints: latency-svc-vznvq [749.714049ms]
Jul  5 16:38:32.735: INFO: Created: latency-svc-lr5bq
Jul  5 16:38:32.775: INFO: Got endpoints: latency-svc-r5g2m [749.349913ms]
Jul  5 16:38:32.786: INFO: Created: latency-svc-wlpfp
Jul  5 16:38:32.826: INFO: Got endpoints: latency-svc-plxrv [750.209693ms]
Jul  5 16:38:32.835: INFO: Created: latency-svc-nps66
Jul  5 16:38:32.875: INFO: Got endpoints: latency-svc-h584j [749.323486ms]
Jul  5 16:38:32.885: INFO: Created: latency-svc-m75q8
Jul  5 16:38:32.926: INFO: Got endpoints: latency-svc-nrk7f [749.625263ms]
Jul  5 16:38:32.937: INFO: Created: latency-svc-pbp4s
Jul  5 16:38:32.976: INFO: Got endpoints: latency-svc-djsck [751.032028ms]
Jul  5 16:38:33.026: INFO: Got endpoints: latency-svc-2hl5d [749.804627ms]
Jul  5 16:38:33.076: INFO: Got endpoints: latency-svc-zjml5 [750.09146ms]
Jul  5 16:38:33.126: INFO: Got endpoints: latency-svc-npftj [749.351979ms]
Jul  5 16:38:33.179: INFO: Got endpoints: latency-svc-44xhs [753.093438ms]
Jul  5 16:38:33.237: INFO: Got endpoints: latency-svc-v2q9r [760.996833ms]
Jul  5 16:38:33.275: INFO: Got endpoints: latency-svc-spkzb [749.631083ms]
Jul  5 16:38:33.326: INFO: Got endpoints: latency-svc-84brc [750.882457ms]
Jul  5 16:38:33.377: INFO: Got endpoints: latency-svc-nmw7z [750.458036ms]
Jul  5 16:38:33.427: INFO: Got endpoints: latency-svc-6s92n [751.441449ms]
Jul  5 16:38:33.475: INFO: Got endpoints: latency-svc-lr5bq [749.623065ms]
Jul  5 16:38:33.526: INFO: Got endpoints: latency-svc-wlpfp [750.263564ms]
Jul  5 16:38:33.575: INFO: Got endpoints: latency-svc-nps66 [749.756522ms]
Jul  5 16:38:33.627: INFO: Got endpoints: latency-svc-m75q8 [751.611726ms]
Jul  5 16:38:33.676: INFO: Got endpoints: latency-svc-pbp4s [749.767227ms]
Jul  5 16:38:33.676: INFO: Latencies: [11.669368ms 11.886315ms 15.306244ms 17.708467ms 20.084238ms 31.950624ms 35.412732ms 38.981872ms 42.474419ms 44.887138ms 44.976351ms 45.542906ms 48.67247ms 54.946495ms 55.939936ms 70.48459ms 70.514062ms 72.930085ms 73.33903ms 75.248049ms 76.089174ms 77.554038ms 78.324746ms 80.196931ms 81.890368ms 82.420662ms 87.551862ms 89.729618ms 89.822547ms 89.87597ms 90.538785ms 91.035153ms 113.218102ms 165.232664ms 208.919451ms 258.055651ms 309.192997ms 359.418771ms 411.127217ms 434.527459ms 457.191138ms 485.153409ms 531.508697ms 580.801703ms 629.275571ms 678.328712ms 725.588327ms 738.404836ms 742.935866ms 743.685291ms 743.687138ms 744.652988ms 745.162113ms 745.449642ms 745.760645ms 747.092504ms 747.904024ms 748.182364ms 748.227349ms 748.341386ms 748.41049ms 748.431854ms 748.442592ms 748.619649ms 748.898864ms 748.946914ms 748.975563ms 748.990924ms 749.0167ms 749.096613ms 749.115086ms 749.117158ms 749.142407ms 749.205752ms 749.207831ms 749.233796ms 749.237327ms 749.238881ms 749.242691ms 749.261593ms 749.271417ms 749.316271ms 749.317572ms 749.323486ms 749.349913ms 749.351979ms 749.370149ms 749.386107ms 749.419206ms 749.426179ms 749.43884ms 749.483285ms 749.498215ms 749.503569ms 749.516826ms 749.5309ms 749.534555ms 749.546322ms 749.623065ms 749.624449ms 749.625263ms 749.631083ms 749.635835ms 749.64303ms 749.677886ms 749.690119ms 749.714049ms 749.740842ms 749.746018ms 749.756522ms 749.758147ms 749.767227ms 749.779715ms 749.786711ms 749.788776ms 749.803102ms 749.804627ms 749.832701ms 749.851264ms 749.876307ms 749.900081ms 749.902789ms 749.923821ms 749.942542ms 749.964509ms 749.969633ms 749.982145ms 750.032362ms 750.035598ms 750.055876ms 750.057477ms 750.09146ms 750.099149ms 750.154293ms 750.155977ms 750.156001ms 750.177144ms 750.209693ms 750.233587ms 750.239234ms 750.243408ms 750.263564ms 750.267669ms 750.297919ms 750.346474ms 750.354152ms 750.355685ms 750.356856ms 750.362794ms 750.368367ms 750.37195ms 750.374321ms 750.38778ms 750.390997ms 750.406844ms 750.458036ms 750.491059ms 750.508775ms 750.515163ms 750.578165ms 750.637724ms 750.639507ms 750.647892ms 750.648593ms 750.650655ms 750.662056ms 750.662068ms 750.663082ms 750.676322ms 750.758871ms 750.764245ms 750.772009ms 750.804363ms 750.871388ms 750.882457ms 750.888928ms 750.894423ms 750.908963ms 751.024013ms 751.024805ms 751.032028ms 751.037241ms 751.216849ms 751.229634ms 751.441449ms 751.448173ms 751.477831ms 751.552043ms 751.611726ms 751.635511ms 751.900529ms 753.093438ms 753.121548ms 753.569206ms 753.876896ms 754.901272ms 756.449373ms 756.908251ms 760.996833ms 761.677811ms]
Jul  5 16:38:33.676: INFO: 50 %ile: 749.625263ms
Jul  5 16:38:33.676: INFO: 90 %ile: 751.032028ms
Jul  5 16:38:33.676: INFO: 99 %ile: 760.996833ms
Jul  5 16:38:33.676: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jul  5 16:38:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3849" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":164,"skipped":3118,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:33.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8013
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jul  5 16:38:33.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8013" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":165,"skipped":3122,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:38:33.871: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7084
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul  5 16:38:34.031: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul  5 16:38:46.041: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:38:48.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:39:00.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7084" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":166,"skipped":3130,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:00.153: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6854
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6854
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6854
I0705 16:39:00.412554    6089 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6854, replica count: 2
Jul  5 16:39:03.464: INFO: Creating new exec pod
I0705 16:39:03.464246    6089 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:39:06.507: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6854 exec execpodtszvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul  5 16:39:06.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul  5 16:39:06.814: INFO: stdout: "externalname-service-rr8sj"
Jul  5 16:39:06.814: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6854 exec execpodtszvt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.211.115 80'
Jul  5 16:39:07.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.26.211.115 80\nConnection to 172.26.211.115 80 port [tcp/http] succeeded!\n"
Jul  5 16:39:07.152: INFO: stdout: "externalname-service-rr8sj"
Jul  5 16:39:07.152: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:39:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6854" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":167,"skipped":3137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:07.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8614
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:39:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8614" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":168,"skipped":3172,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:23.556: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7030
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jul  5 16:39:23.780: INFO: Waiting up to 5m0s for pod "var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c" in namespace "var-expansion-7030" to be "Succeeded or Failed"
Jul  5 16:39:23.797: INFO: Pod "var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.038284ms
Jul  5 16:39:25.811: INFO: Pod "var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031266163s
Jul  5 16:39:27.825: INFO: Pod "var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044884505s
STEP: Saw pod success
Jul  5 16:39:27.825: INFO: Pod "var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c" satisfied condition "Succeeded or Failed"
Jul  5 16:39:27.837: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:39:27.880: INFO: Waiting for pod var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c to disappear
Jul  5 16:39:27.892: INFO: Pod var-expansion-7a50ab3f-e65d-468f-a08b-ee398d482b6c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 16:39:27.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7030" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":169,"skipped":3189,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:27.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5893
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul  5 16:39:28.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:39:31.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:39:42.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5893" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":170,"skipped":3210,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:42.655: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3367
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:39:42.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3367" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":171,"skipped":3221,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:42.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5824
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  5 16:39:43.016: INFO: Waiting up to 5m0s for pod "pod-65a3e167-bc75-477c-a834-bf7d7c308b3b" in namespace "emptydir-5824" to be "Succeeded or Failed"
Jul  5 16:39:43.024: INFO: Pod "pod-65a3e167-bc75-477c-a834-bf7d7c308b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.266869ms
Jul  5 16:39:45.032: INFO: Pod "pod-65a3e167-bc75-477c-a834-bf7d7c308b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015677163s
Jul  5 16:39:47.040: INFO: Pod "pod-65a3e167-bc75-477c-a834-bf7d7c308b3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024077734s
STEP: Saw pod success
Jul  5 16:39:47.040: INFO: Pod "pod-65a3e167-bc75-477c-a834-bf7d7c308b3b" satisfied condition "Succeeded or Failed"
Jul  5 16:39:47.047: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-65a3e167-bc75-477c-a834-bf7d7c308b3b container test-container: <nil>
STEP: delete the pod
Jul  5 16:39:47.115: INFO: Waiting for pod pod-65a3e167-bc75-477c-a834-bf7d7c308b3b to disappear
Jul  5 16:39:47.121: INFO: Pod pod-65a3e167-bc75-477c-a834-bf7d7c308b3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:39:47.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5824" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":172,"skipped":3241,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:47.141: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-337
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jul  5 16:39:47.298: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-337 proxy --unix-socket=/tmp/kubectl-proxy-unix3675652040/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:39:47.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-337" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":173,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:47.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1906
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jul  5 16:39:47.540: INFO: The status of Pod labelsupdate0106eca9-bc72-4ab5-b1e2-220e306b845e is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:39:49.548: INFO: The status of Pod labelsupdate0106eca9-bc72-4ab5-b1e2-220e306b845e is Running (Ready = true)
Jul  5 16:39:50.088: INFO: Successfully updated pod "labelsupdate0106eca9-bc72-4ab5-b1e2-220e306b845e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 16:39:54.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1906" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":174,"skipped":3313,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:39:54.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7424
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7424.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7424.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.215.28.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.28.215.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.215.28.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.28.215.166_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7424.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7424.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7424.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7424.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7424.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.215.28.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.28.215.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.215.28.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.28.215.166_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 16:39:56.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.455: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.505: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.564: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.621: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.633: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.644: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.655: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:39:56.700: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:01.713: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.761: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.786: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.850: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.874: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.886: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:01.934: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:06.713: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.761: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.784: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.909: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.920: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.944: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:06.990: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:11.718: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.777: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.790: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.847: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.870: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.881: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:11.926: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:16.712: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.724: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.781: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.838: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.850: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.872: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:16.918: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:21.714: INFO: Unable to read wheezy_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.725: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.740: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.786: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.844: INFO: Unable to read jessie_udp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.856: INFO: Unable to read jessie_tcp@dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.867: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local from pod dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15: the server could not find the requested resource (get pods dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15)
Jul  5 16:40:21.924: INFO: Lookups using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 failed for: [wheezy_udp@dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@dns-test-service.dns-7424.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_udp@dns-test-service.dns-7424.svc.cluster.local jessie_tcp@dns-test-service.dns-7424.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7424.svc.cluster.local]

Jul  5 16:40:26.922: INFO: DNS probes using dns-7424/dns-test-abe68e70-4aa0-4ff1-87b3-6f0d5abc3b15 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 16:40:26.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7424" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":175,"skipped":3323,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:40:26.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6865
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6865
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6865
STEP: Waiting until pod test-pod will start running in namespace statefulset-6865
STEP: Creating statefulset with conflicting port in namespace statefulset-6865
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6865
Jul  5 16:40:29.201: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: 3d09192f-9c99-43ad-b118-fbf034a52fe9, status phase: Pending. Waiting for statefulset controller to delete.
Jul  5 16:40:29.216: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: 3d09192f-9c99-43ad-b118-fbf034a52fe9, status phase: Failed. Waiting for statefulset controller to delete.
Jul  5 16:40:29.220: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: 3d09192f-9c99-43ad-b118-fbf034a52fe9, status phase: Failed. Waiting for statefulset controller to delete.
Jul  5 16:40:29.221: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6865
STEP: Removing pod with conflicting port in namespace statefulset-6865
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6865 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 16:40:31.244: INFO: Deleting all statefulset in ns statefulset-6865
Jul  5 16:40:31.251: INFO: Scaling statefulset ss to 0
Jul  5 16:40:41.282: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 16:40:41.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 16:40:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6865" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":176,"skipped":3323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:40:41.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9152
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-l8xq
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 16:40:41.512: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l8xq" in namespace "subpath-9152" to be "Succeeded or Failed"
Jul  5 16:40:41.519: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.613834ms
Jul  5 16:40:43.527: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 2.014861518s
Jul  5 16:40:45.535: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 4.022576057s
Jul  5 16:40:47.542: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 6.029853182s
Jul  5 16:40:49.550: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 8.038077633s
Jul  5 16:40:51.558: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 10.045908534s
Jul  5 16:40:53.567: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 12.054433162s
Jul  5 16:40:55.574: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 14.061930847s
Jul  5 16:40:57.582: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 16.070235903s
Jul  5 16:40:59.590: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 18.07825206s
Jul  5 16:41:01.599: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=true. Elapsed: 20.086823867s
Jul  5 16:41:03.607: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Running", Reason="", readiness=false. Elapsed: 22.094883019s
Jul  5 16:41:05.616: INFO: Pod "pod-subpath-test-configmap-l8xq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.103411204s
STEP: Saw pod success
Jul  5 16:41:05.616: INFO: Pod "pod-subpath-test-configmap-l8xq" satisfied condition "Succeeded or Failed"
Jul  5 16:41:05.623: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-subpath-test-configmap-l8xq container test-container-subpath-configmap-l8xq: <nil>
STEP: delete the pod
Jul  5 16:41:05.649: INFO: Waiting for pod pod-subpath-test-configmap-l8xq to disappear
Jul  5 16:41:05.655: INFO: Pod pod-subpath-test-configmap-l8xq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l8xq
Jul  5 16:41:05.655: INFO: Deleting pod "pod-subpath-test-configmap-l8xq" in namespace "subpath-9152"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jul  5 16:41:05.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9152" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":177,"skipped":3345,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:41:05.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8034
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8034
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:41:05.861: INFO: Found 0 stateful pods, waiting for 1
Jul  5 16:41:15.869: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jul  5 16:41:15.909: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:41:15.909: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jul  5 16:41:25.917: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:41:25.917: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 16:41:25.953: INFO: Deleting all statefulset in ns statefulset-8034
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 16:41:25.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8034" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":178,"skipped":3349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:41:25.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-41560177-3d77-45be-959f-89153a398385
STEP: Creating a pod to test consume configMaps
Jul  5 16:41:26.170: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755" in namespace "projected-3592" to be "Succeeded or Failed"
Jul  5 16:41:26.176: INFO: Pod "pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485742ms
Jul  5 16:41:28.185: INFO: Pod "pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014535404s
Jul  5 16:41:30.193: INFO: Pod "pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0228202s
STEP: Saw pod success
Jul  5 16:41:30.193: INFO: Pod "pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755" satisfied condition "Succeeded or Failed"
Jul  5 16:41:30.200: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:41:30.224: INFO: Waiting for pod pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755 to disappear
Jul  5 16:41:30.231: INFO: Pod pod-projected-configmaps-51a6ea81-da8d-4d34-8071-1c06c8d0f755 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:41:30.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3592" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3404,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:41:30.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4258
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jul  5 16:41:30.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:41:46.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4258" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":180,"skipped":3412,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:41:46.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9860
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jul  5 16:43:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9860" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":181,"skipped":3429,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:43:00.802: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5210
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-78925847-0bab-471d-b535-0f782a271e3a
STEP: Creating configMap with name cm-test-opt-upd-906b8f65-30c7-48f7-b517-1a324037f6e4
STEP: Creating the pod
Jul  5 16:43:01.013: INFO: The status of Pod pod-projected-configmaps-ee85dcc5-f853-490a-bf3e-336d67460e27 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:43:03.021: INFO: The status of Pod pod-projected-configmaps-ee85dcc5-f853-490a-bf3e-336d67460e27 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-78925847-0bab-471d-b535-0f782a271e3a
STEP: Updating configmap cm-test-opt-upd-906b8f65-30c7-48f7-b517-1a324037f6e4
STEP: Creating configMap with name cm-test-opt-create-bccc49a9-8d4d-4840-8fdb-17663e90bb00
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:43:07.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5210" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3429,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:43:07.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7268
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:43:07.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-8069
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7268
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jul  5 16:43:07.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8069" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jul  5 16:43:07.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7268" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":183,"skipped":3450,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:43:07.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4546
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 16:43:12.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4546" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":184,"skipped":3465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:43:12.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9335
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 16:44:12.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9335" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":185,"skipped":3504,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:44:12.258: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1586
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:44:12.435: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul  5 16:44:17.443: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 16:44:17.443: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 16:44:19.500: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1586  785595dc-326d-41fc-a9e9-f5ea5e87cb54 28601 1 2022-07-05 16:44:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-07-05 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005392d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-05 16:44:17 +0000 UTC,LastTransitionTime:2022-07-05 16:44:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-6755c7b765" has successfully progressed.,LastUpdateTime:2022-07-05 16:44:18 +0000 UTC,LastTransitionTime:2022-07-05 16:44:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 16:44:19.508: INFO: New ReplicaSet "test-cleanup-deployment-6755c7b765" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-6755c7b765  deployment-1586  e60e413d-cc54-4a8c-97c1-1c01e0e3832f 28594 1 2022-07-05 16:44:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 785595dc-326d-41fc-a9e9-f5ea5e87cb54 0xc00484b167 0xc00484b168}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"785595dc-326d-41fc-a9e9-f5ea5e87cb54\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:44:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6755c7b765,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00484b218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:44:19.516: INFO: Pod "test-cleanup-deployment-6755c7b765-q2ms8" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-6755c7b765-q2ms8 test-cleanup-deployment-6755c7b765- deployment-1586  89d9612e-7d0d-45d2-976d-c95d7504a406 28593 0 2022-07-05 16:44:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[cni.projectcalico.org/containerID:4e5f8867960b8100ce439d81f2dcce468115a70b707d27d70d346a19449743e0 cni.projectcalico.org/podIP:172.16.1.93/32 cni.projectcalico.org/podIPs:172.16.1.93/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-6755c7b765 e60e413d-cc54-4a8c-97c1-1c01e0e3832f 0xc002f63877 0xc002f63878}] []  [{calico Update v1 2022-07-05 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-07-05 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e60e413d-cc54-4a8c-97c1-1c01e0e3832f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jkd8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jkd8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.93,StartTime:2022-07-05 16:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://9833453c551e3f266627f210dccc67b922eafc2b3866ac8af2c75908de7b3a00,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 16:44:19.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1586" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":186,"skipped":3513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:44:19.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6681
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jul  5 16:44:19.724: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:44:21.732: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  5 16:44:21.763: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 16:44:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6681" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":187,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:44:21.818: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8695
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-e7248844-316c-4611-83ca-5240f4bbf562 in namespace container-probe-8695
Jul  5 16:44:24.012: INFO: Started pod liveness-e7248844-316c-4611-83ca-5240f4bbf562 in namespace container-probe-8695
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 16:44:24.020: INFO: Initial restart count of pod liveness-e7248844-316c-4611-83ca-5240f4bbf562 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 16:48:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8695" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3573,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:25.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1382
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 16:48:25.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 16:48:25.287: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:48:26.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 16:48:26.308: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 16:48:27.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 16:48:27.309: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jul  5 16:48:27.355: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29847"},"items":null}

Jul  5 16:48:27.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29847"},"items":[{"metadata":{"name":"daemon-set-bdvkp","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"35be74bd-b014-4a4b-96a0-8a3ae99090f1","resourceVersion":"29846","creationTimestamp":"2022-07-05T16:48:25Z","deletionTimestamp":"2022-07-05T16:48:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4b00a005aac298c9200ae5ba77f8b29e5a06b0f9a65d79211465b983b033b764","cni.projectcalico.org/podIP":"172.16.0.238/32","cni.projectcalico.org/podIPs":"172.16.0.238/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7af55d1a-41c4-4b5d-baca-6473c547be14","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af55d1a-41c4-4b5d-baca-6473c547be14\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.0.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-44ksn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-44ksn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw85sex2ooqi4ztetrj0z","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw85sex2ooqi4ztetrj0z"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:25Z"}],"hostIP":"10.250.25.206","podIP":"172.16.0.238","podIPs":[{"ip":"172.16.0.238"}],"startTime":"2022-07-05T16:48:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-05T16:48:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://621a5239dd98c799fbc8278f0066a8e707685bccd6d3dd01e739a5351bb7f354","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-spgbs","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"2cef0575-87a1-49d0-b71b-939479f023a5","resourceVersion":"29847","creationTimestamp":"2022-07-05T16:48:25Z","deletionTimestamp":"2022-07-05T16:48:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8426ea72e0010556ad887ebd355f01fbcfea2d76f44d4440c8dfbd42570dc46b","cni.projectcalico.org/podIP":"172.16.1.95/32","cni.projectcalico.org/podIPs":"172.16.1.95/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7af55d1a-41c4-4b5d-baca-6473c547be14","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af55d1a-41c4-4b5d-baca-6473c547be14\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-05T16:48:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tbnqv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tbnqv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"izgw8bazids4c4cxzuus22z","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["izgw8bazids4c4cxzuus22z"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-05T16:48:25Z"}],"hostIP":"10.250.25.207","podIP":"172.16.1.95","podIPs":[{"ip":"172.16.1.95"}],"startTime":"2022-07-05T16:48:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-05T16:48:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://54dec9a3a5683d70297eb87c3e1bab376d769dafa92233575c031667483106d8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:48:27.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1382" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":189,"skipped":3585,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:27.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-679
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jul  5 16:48:29.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-679" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3602,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:29.635: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4154
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:48:30.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:48:33.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:48:33.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3409-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:48:36.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4154" for this suite.
STEP: Destroying namespace "webhook-4154-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":191,"skipped":3618,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:36.256: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4987
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:48:36.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4987" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":192,"skipped":3621,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:36.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9970
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:48:36.675: INFO: The status of Pod pod-secrets-115dfecf-12de-46e5-9a7f-5affb694345e is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:48:38.683: INFO: The status of Pod pod-secrets-115dfecf-12de-46e5-9a7f-5affb694345e is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jul  5 16:48:38.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9970" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":193,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:38.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4390
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jul  5 16:48:38.937: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jul  5 16:48:41.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4390" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":194,"skipped":3648,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:41.028: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4204
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:48:41.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5" in namespace "downward-api-4204" to be "Succeeded or Failed"
Jul  5 16:48:41.212: INFO: Pod "downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822424ms
Jul  5 16:48:43.220: INFO: Pod "downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015035151s
Jul  5 16:48:45.229: INFO: Pod "downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023454999s
STEP: Saw pod success
Jul  5 16:48:45.229: INFO: Pod "downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5" satisfied condition "Succeeded or Failed"
Jul  5 16:48:45.236: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5 container client-container: <nil>
STEP: delete the pod
Jul  5 16:48:45.264: INFO: Waiting for pod downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5 to disappear
Jul  5 16:48:45.271: INFO: Pod downwardapi-volume-81adb6b1-4c0f-4ee7-a47e-6e245359f7f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:48:45.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4204" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":195,"skipped":3657,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:45.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7883
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jul  5 16:48:45.468: INFO: Waiting up to 5m0s for pod "downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4" in namespace "downward-api-7883" to be "Succeeded or Failed"
Jul  5 16:48:45.475: INFO: Pod "downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892703ms
Jul  5 16:48:47.484: INFO: Pod "downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016123036s
Jul  5 16:48:49.492: INFO: Pod "downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023857472s
STEP: Saw pod success
Jul  5 16:48:49.492: INFO: Pod "downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4" satisfied condition "Succeeded or Failed"
Jul  5 16:48:49.499: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4 container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:48:49.526: INFO: Waiting for pod downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4 to disappear
Jul  5 16:48:49.533: INFO: Pod downward-api-25f94455-c0ae-4c84-937e-f40a35ab91d4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jul  5 16:48:49.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7883" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":196,"skipped":3666,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:49.554: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-507
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:48:49.738: INFO: The status of Pod busybox-readonly-fsceea1b23-7a7b-46ab-825e-36d725227611 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:48:51.746: INFO: The status of Pod busybox-readonly-fsceea1b23-7a7b-46ab-825e-36d725227611 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jul  5 16:48:51.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-507" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3673,"failed":0}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:51.790: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-1980
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jul  5 16:48:51.958: INFO: Major version: 1
STEP: Confirm minor version
Jul  5 16:48:51.958: INFO: cleanMinorVersion: 24
Jul  5 16:48:51.958: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jul  5 16:48:51.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1980" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":198,"skipped":3674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:51.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4137
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul  5 16:48:52.191: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jul  5 16:48:54.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4137" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":199,"skipped":3708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:48:54.230: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4843
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 16:49:10.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4843" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":200,"skipped":3760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:10.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:49:12.737: INFO: Deleting pod "var-expansion-ec8e128d-c1c2-4378-9620-9272fe198e44" in namespace "var-expansion-8773"
Jul  5 16:49:12.745: INFO: Wait up to 5m0s for pod "var-expansion-ec8e128d-c1c2-4378-9620-9272fe198e44" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 16:49:14.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8773" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":201,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:14.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6859
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jul  5 16:49:14.945: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul  5 16:49:14.945: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:15.780: INFO: stderr: ""
Jul  5 16:49:15.780: INFO: stdout: "service/agnhost-replica created\n"
Jul  5 16:49:15.780: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul  5 16:49:15.780: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:16.012: INFO: stderr: ""
Jul  5 16:49:16.012: INFO: stdout: "service/agnhost-primary created\n"
Jul  5 16:49:16.012: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  5 16:49:16.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:16.235: INFO: stderr: ""
Jul  5 16:49:16.235: INFO: stdout: "service/frontend created\n"
Jul  5 16:49:16.235: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul  5 16:49:16.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:16.453: INFO: stderr: ""
Jul  5 16:49:16.453: INFO: stdout: "deployment.apps/frontend created\n"
Jul  5 16:49:16.453: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  5 16:49:16.453: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:16.644: INFO: stderr: ""
Jul  5 16:49:16.644: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul  5 16:49:16.644: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  5 16:49:16.644: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 create -f -'
Jul  5 16:49:16.871: INFO: stderr: ""
Jul  5 16:49:16.871: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul  5 16:49:16.871: INFO: Waiting for all frontend pods to be Running.
Jul  5 16:49:21.922: INFO: Waiting for frontend to serve content.
Jul  5 16:49:21.988: INFO: Trying to add a new entry to the guestbook.
Jul  5 16:49:22.006: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul  5 16:49:22.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.207: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.207: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 16:49:22.208: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.318: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.318: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 16:49:22.318: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.403: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.403: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 16:49:22.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.488: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 16:49:22.488: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.568: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.568: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 16:49:22.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6859 delete --grace-period=0 --force -f -'
Jul  5 16:49:22.649: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 16:49:22.649: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 16:49:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6859" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":202,"skipped":3807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:22.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1136
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 16:49:26.895: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jul  5 16:49:26.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1136" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:26.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9843
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:49:27.105: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:49:27.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9843" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":204,"skipped":3854,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:27.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7082
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:49:27.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d" in namespace "downward-api-7082" to be "Succeeded or Failed"
Jul  5 16:49:27.358: INFO: Pod "downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.172937ms
Jul  5 16:49:29.367: INFO: Pod "downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015289433s
Jul  5 16:49:31.375: INFO: Pod "downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02401055s
STEP: Saw pod success
Jul  5 16:49:31.375: INFO: Pod "downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d" satisfied condition "Succeeded or Failed"
Jul  5 16:49:31.382: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d container client-container: <nil>
STEP: delete the pod
Jul  5 16:49:31.409: INFO: Waiting for pod downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d to disappear
Jul  5 16:49:31.416: INFO: Pod downwardapi-volume-0714be10-953e-48f5-a4ad-81487489556d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:49:31.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7082" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3870,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:31.436: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1981
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jul  5 16:49:31.615: INFO: Waiting up to 5m0s for pod "downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602" in namespace "downward-api-1981" to be "Succeeded or Failed"
Jul  5 16:49:31.622: INFO: Pod "downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887311ms
Jul  5 16:49:33.629: INFO: Pod "downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014843044s
Jul  5 16:49:35.638: INFO: Pod "downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023835473s
STEP: Saw pod success
Jul  5 16:49:35.639: INFO: Pod "downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602" satisfied condition "Succeeded or Failed"
Jul  5 16:49:35.646: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602 container dapi-container: <nil>
STEP: delete the pod
Jul  5 16:49:35.674: INFO: Waiting for pod downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602 to disappear
Jul  5 16:49:35.681: INFO: Pod downward-api-2f7355b3-c3d3-4a16-bf18-7c86e4bb3602 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jul  5 16:49:35.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1981" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3878,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:35.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2384
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul  5 16:49:45.922: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:49:45.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0705 16:49:45.922499    6089 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-2384" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":207,"skipped":3886,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:45.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2608
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jul  5 16:49:46.121: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jul  5 16:49:46.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2608" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":208,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:46.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:49:46.300: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  5 16:49:46.314: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  5 16:49:51.323: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 16:49:51.323: INFO: Creating deployment "test-rolling-update-deployment"
Jul  5 16:49:51.331: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  5 16:49:51.347: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jul  5 16:49:53.363: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  5 16:49:53.370: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 16:49:53.392: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4399  c839da40-92ec-464e-97e9-2c4b123c0e0a 30911 1 2022-07-05 16:49:51 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-07-05 16:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002428148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-05 16:49:51 +0000 UTC,LastTransitionTime:2022-07-05 16:49:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2022-07-05 16:49:52 +0000 UTC,LastTransitionTime:2022-07-05 16:49:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 16:49:53.399: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-4399  572cea48-0ba9-42a4-8a26-ce6538db4814 30904 1 2022-07-05 16:49:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c839da40-92ec-464e-97e9-2c4b123c0e0a 0xc002f63227 0xc002f63228}] []  [{kube-controller-manager Update apps/v1 2022-07-05 16:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c839da40-92ec-464e-97e9-2c4b123c0e0a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:49:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f632d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:49:53.399: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  5 16:49:53.399: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4399  05078746-ed89-4f3a-adc0-f0d8c050ae51 30910 2 2022-07-05 16:49:46 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c839da40-92ec-464e-97e9-2c4b123c0e0a 0xc002f630f7 0xc002f630f8}] []  [{e2e.test Update apps/v1 2022-07-05 16:49:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c839da40-92ec-464e-97e9-2c4b123c0e0a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-07-05 16:49:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f631b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul  5 16:49:53.407: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-v2jzl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-v2jzl test-rolling-update-deployment-67c8f74c6c- deployment-4399  f23de03a-f260-4378-8dd1-25caab573a95 30903 0 2022-07-05 16:49:51 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:163deb928f3e3a7fc8830bec36f28dd64fe035b4d10715d636e70e550686a4cc cni.projectcalico.org/podIP:172.16.1.115/32 cni.projectcalico.org/podIPs:172.16.1.115/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c 572cea48-0ba9-42a4-8a26-ce6538db4814 0xc005392167 0xc005392168}] []  [{calico Update v1 2022-07-05 16:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-07-05 16:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"572cea48-0ba9-42a4-8a26-ce6538db4814\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 16:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjh6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjh6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 16:49:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.115,StartTime:2022-07-05 16:49:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 16:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://e511463f991d8fccc24f1afac7c5d10ae48c62b55700989aaba6f31cdf206dc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 16:49:53.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4399" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":209,"skipped":3914,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:53.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2733
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-b9b9de4d-014d-4099-a553-aec5de66e071
STEP: Creating a pod to test consume configMaps
Jul  5 16:49:53.612: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249" in namespace "projected-2733" to be "Succeeded or Failed"
Jul  5 16:49:53.619: INFO: Pod "pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249": Phase="Pending", Reason="", readiness=false. Elapsed: 6.912172ms
Jul  5 16:49:55.627: INFO: Pod "pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01501997s
Jul  5 16:49:57.640: INFO: Pod "pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028377661s
STEP: Saw pod success
Jul  5 16:49:57.640: INFO: Pod "pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249" satisfied condition "Succeeded or Failed"
Jul  5 16:49:57.647: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 16:49:57.715: INFO: Waiting for pod pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249 to disappear
Jul  5 16:49:57.722: INFO: Pod pod-projected-configmaps-903fee17-77ad-4694-bcfe-ec28fbfae249 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 16:49:57.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2733" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":3917,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:49:57.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4778
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  5 16:49:57.926: INFO: The status of Pod pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:49:59.934: INFO: The status of Pod pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  5 16:50:00.467: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620"
Jul  5 16:50:00.467: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620" in namespace "pods-4778" to be "terminated due to deadline exceeded"
Jul  5 16:50:00.474: INFO: Pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620": Phase="Running", Reason="", readiness=true. Elapsed: 7.236073ms
Jul  5 16:50:02.483: INFO: Pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620": Phase="Running", Reason="", readiness=false. Elapsed: 2.016079003s
Jul  5 16:50:04.491: INFO: Pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.024222784s
Jul  5 16:50:04.491: INFO: Pod "pod-update-activedeadlineseconds-ee53138b-1505-4908-9750-66a84960a620" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 16:50:04.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4778" for this suite.
•{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":3920,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:50:04.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2530
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:50:05.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:50:08.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:50:20.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2530" for this suite.
STEP: Destroying namespace "webhook-2530-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":212,"skipped":3924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:50:20.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1366
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jul  5 16:50:20.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jul  5 16:50:21.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:23.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:25.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:27.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:29.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:31.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:33.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:35.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 50, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 16:50:38.706: INFO: Waited 1.63497471s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jul  5 16:50:39.035: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jul  5 16:50:39.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1366" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":213,"skipped":3962,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:50:39.520: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6369
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6369, will wait for the garbage collector to delete the pods
Jul  5 16:50:41.764: INFO: Deleting Job.batch foo took: 8.176201ms
Jul  5 16:50:41.864: INFO: Terminating Job.batch foo pods took: 100.197455ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jul  5 16:51:14.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6369" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":214,"skipped":3964,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:14.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7164
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:51:14.976: INFO: Waiting up to 5m0s for pod "downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b" in namespace "downward-api-7164" to be "Succeeded or Failed"
Jul  5 16:51:14.983: INFO: Pod "downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999215ms
Jul  5 16:51:16.991: INFO: Pod "downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b": Phase="Running", Reason="", readiness=false. Elapsed: 2.015246495s
Jul  5 16:51:18.999: INFO: Pod "downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023679283s
STEP: Saw pod success
Jul  5 16:51:18.999: INFO: Pod "downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b" satisfied condition "Succeeded or Failed"
Jul  5 16:51:19.007: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b container client-container: <nil>
STEP: delete the pod
Jul  5 16:51:19.033: INFO: Waiting for pod downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b to disappear
Jul  5 16:51:19.040: INFO: Pod downwardapi-volume-686eb76e-3642-4a30-b312-e23d0142ac1b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:51:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7164" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":215,"skipped":3972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:19.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-744
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  5 16:51:19.238: INFO: Waiting up to 5m0s for pod "pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94" in namespace "emptydir-744" to be "Succeeded or Failed"
Jul  5 16:51:19.245: INFO: Pod "pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.929046ms
Jul  5 16:51:21.253: INFO: Pod "pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0149383s
Jul  5 16:51:23.261: INFO: Pod "pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022702877s
STEP: Saw pod success
Jul  5 16:51:23.261: INFO: Pod "pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94" satisfied condition "Succeeded or Failed"
Jul  5 16:51:23.268: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94 container test-container: <nil>
STEP: delete the pod
Jul  5 16:51:23.295: INFO: Waiting for pod pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94 to disappear
Jul  5 16:51:23.302: INFO: Pod pod-4639289d-df3e-456e-ba2d-4c9e4e2f1e94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:51:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-744" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":216,"skipped":4003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:23.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7118
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-6341601a-f614-46df-9dbd-c8f31d4670cf
STEP: Creating secret with name s-test-opt-upd-25e906cb-5e2f-4e8a-9fc8-82dd1621aa62
STEP: Creating the pod
Jul  5 16:51:23.542: INFO: The status of Pod pod-projected-secrets-b98ebf55-423a-4f59-88df-e64e8f05e000 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:51:25.551: INFO: The status of Pod pod-projected-secrets-b98ebf55-423a-4f59-88df-e64e8f05e000 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-6341601a-f614-46df-9dbd-c8f31d4670cf
STEP: Updating secret s-test-opt-upd-25e906cb-5e2f-4e8a-9fc8-82dd1621aa62
STEP: Creating secret with name s-test-opt-create-d81c7878-6f26-46d1-83c7-1514f875a95d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 16:51:27.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7118" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":217,"skipped":4053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:27.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5971
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 16:51:30.217: INFO: DNS probes using dns-5971/dns-test-dd9aafc5-3279-4ebc-829c-6a44a4de361a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 16:51:30.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5971" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":218,"skipped":4084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:30.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4387
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jul  5 16:51:30.436: INFO: Pod name sample-pod: Found 0 pods out of 3
Jul  5 16:51:35.455: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jul  5 16:51:35.462: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 16:51:35.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4387" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":219,"skipped":4160,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:35.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8271
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:51:35.710: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b42bf39f-6daa-42b9-bfeb-af94a80e18a0", Controller:(*bool)(0xc004b11446), BlockOwnerDeletion:(*bool)(0xc004b11447)}}
Jul  5 16:51:35.720: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0fd83d92-7d9d-43ab-8dae-c9a7942fdf38", Controller:(*bool)(0xc004b116ee), BlockOwnerDeletion:(*bool)(0xc004b116ef)}}
Jul  5 16:51:35.728: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"928d2e01-d332-4045-8e14-bae478110726", Controller:(*bool)(0xc0030a287a), BlockOwnerDeletion:(*bool)(0xc0030a287b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jul  5 16:51:40.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8271" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":220,"skipped":4169,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:51:40.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9337
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jul  5 16:53:41.484: INFO: Successfully updated pod "var-expansion-2f804a0a-7dd3-4736-86af-3f9e3d355dc4"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul  5 16:53:43.500: INFO: Deleting pod "var-expansion-2f804a0a-7dd3-4736-86af-3f9e3d355dc4" in namespace "var-expansion-9337"
Jul  5 16:53:43.508: INFO: Wait up to 5m0s for pod "var-expansion-2f804a0a-7dd3-4736-86af-3f9e3d355dc4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 16:54:15.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9337" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":221,"skipped":4186,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:15.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9816
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jul  5 16:54:15.716: INFO: created test-event-1
Jul  5 16:54:15.723: INFO: created test-event-2
Jul  5 16:54:15.731: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul  5 16:54:15.738: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul  5 16:54:15.754: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jul  5 16:54:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9816" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":222,"skipped":4205,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:15.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2212
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-vr2t
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 16:54:15.986: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vr2t" in namespace "subpath-2212" to be "Succeeded or Failed"
Jul  5 16:54:15.994: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Pending", Reason="", readiness=false. Elapsed: 7.078918ms
Jul  5 16:54:18.002: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.015886729s
Jul  5 16:54:20.011: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.024026928s
Jul  5 16:54:22.019: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.032959824s
Jul  5 16:54:24.028: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.041983169s
Jul  5 16:54:26.037: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.050961404s
Jul  5 16:54:28.046: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.059969257s
Jul  5 16:54:30.056: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.06914804s
Jul  5 16:54:32.064: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.077675363s
Jul  5 16:54:34.073: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.08693584s
Jul  5 16:54:36.082: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.095690353s
Jul  5 16:54:38.091: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Running", Reason="", readiness=false. Elapsed: 22.104487567s
Jul  5 16:54:40.100: INFO: Pod "pod-subpath-test-secret-vr2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.113188457s
STEP: Saw pod success
Jul  5 16:54:40.100: INFO: Pod "pod-subpath-test-secret-vr2t" satisfied condition "Succeeded or Failed"
Jul  5 16:54:40.107: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-subpath-test-secret-vr2t container test-container-subpath-secret-vr2t: <nil>
STEP: delete the pod
Jul  5 16:54:40.142: INFO: Waiting for pod pod-subpath-test-secret-vr2t to disappear
Jul  5 16:54:40.149: INFO: Pod pod-subpath-test-secret-vr2t no longer exists
STEP: Deleting pod pod-subpath-test-secret-vr2t
Jul  5 16:54:40.149: INFO: Deleting pod "pod-subpath-test-secret-vr2t" in namespace "subpath-2212"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jul  5 16:54:40.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2212" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":223,"skipped":4209,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:40.177: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3853
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 16:54:40.356: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6" in namespace "downward-api-3853" to be "Succeeded or Failed"
Jul  5 16:54:40.363: INFO: Pod "downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.327611ms
Jul  5 16:54:42.372: INFO: Pod "downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6": Phase="Running", Reason="", readiness=false. Elapsed: 2.016337425s
Jul  5 16:54:44.381: INFO: Pod "downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02520747s
STEP: Saw pod success
Jul  5 16:54:44.381: INFO: Pod "downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6" satisfied condition "Succeeded or Failed"
Jul  5 16:54:44.388: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6 container client-container: <nil>
STEP: delete the pod
Jul  5 16:54:44.415: INFO: Waiting for pod downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6 to disappear
Jul  5 16:54:44.422: INFO: Pod downwardapi-volume-7a1ccbe5-b34e-45d8-b026-5dd8c7f812a6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 16:54:44.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3853" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:44.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2692
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-cf1e14a8-ff1c-48fa-b3b0-b1b3109ea7df
STEP: Creating a pod to test consume secrets
Jul  5 16:54:44.628: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3" in namespace "projected-2692" to be "Succeeded or Failed"
Jul  5 16:54:44.635: INFO: Pod "pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.173876ms
Jul  5 16:54:46.644: INFO: Pod "pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015499183s
Jul  5 16:54:48.652: INFO: Pod "pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023656812s
STEP: Saw pod success
Jul  5 16:54:48.652: INFO: Pod "pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3" satisfied condition "Succeeded or Failed"
Jul  5 16:54:48.659: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 16:54:48.686: INFO: Waiting for pod pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3 to disappear
Jul  5 16:54:48.693: INFO: Pod pod-projected-secrets-a3f872fe-f8a1-4af4-bdd1-85f5e850e8a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 16:54:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2692" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":225,"skipped":4279,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:48.714: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5806
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5806
STEP: creating service affinity-nodeport in namespace services-5806
STEP: creating replication controller affinity-nodeport in namespace services-5806
I0705 16:54:48.897626    6089 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5806, replica count: 3
I0705 16:54:51.949219    6089 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:54:51.976: INFO: Creating new exec pod
Jul  5 16:54:55.018: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5806 exec execpod-affinityp54f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul  5 16:54:55.380: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul  5 16:54:55.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:54:55.380: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5806 exec execpod-affinityp54f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.193.151 80'
Jul  5 16:54:55.739: INFO: stderr: "+ nc -v -t -w 2 172.31.193.151 80\n+ Connection to 172.31.193.151 80 port [tcp/http] succeeded!\necho hostName\n"
Jul  5 16:54:55.739: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:54:55.739: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5806 exec execpod-affinityp54f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 31220'
Jul  5 16:54:56.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 31220\nConnection to 10.250.25.206 31220 port [tcp/*] succeeded!\n"
Jul  5 16:54:56.122: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:54:56.122: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5806 exec execpod-affinityp54f6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.207 31220'
Jul  5 16:54:56.438: INFO: stderr: "+ nc -v -t -w 2 10.250.25.207 31220\n+ echo hostName\nConnection to 10.250.25.207 31220 port [tcp/*] succeeded!\n"
Jul  5 16:54:56.439: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:54:56.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5806 exec execpod-affinityp54f6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.25.206:31220/ ; done'
Jul  5 16:54:56.804: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:31220/\n"
Jul  5 16:54:56.804: INFO: stdout: "\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z\naffinity-nodeport-59w7z"
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Received response from host: affinity-nodeport-59w7z
Jul  5 16:54:56.804: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5806, will wait for the garbage collector to delete the pods
Jul  5 16:54:56.880: INFO: Deleting ReplicationController affinity-nodeport took: 8.280663ms
Jul  5 16:54:56.981: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.849768ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:54:59.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5806" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":226,"skipped":4281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:54:59.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9944
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  5 16:55:00.654: INFO: starting watch
STEP: patching
STEP: updating
Jul  5 16:55:00.676: INFO: waiting for watch events with expected annotations
Jul  5 16:55:00.676: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:55:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9944" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":227,"skipped":4306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:00.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8157
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:55:00.969: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e" in namespace "security-context-test-8157" to be "Succeeded or Failed"
Jul  5 16:55:00.977: INFO: Pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.348289ms
Jul  5 16:55:02.986: INFO: Pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016026761s
Jul  5 16:55:04.995: INFO: Pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025103524s
Jul  5 16:55:07.004: INFO: Pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034115666s
Jul  5 16:55:07.004: INFO: Pod "alpine-nnp-false-ec3e77e3-afde-46d5-8ec1-5441bd42d59e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 16:55:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8157" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":228,"skipped":4361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:07.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-9975
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  5 16:55:07.272: INFO: starting watch
STEP: patching
STEP: updating
Jul  5 16:55:07.294: INFO: waiting for watch events with expected annotations
Jul  5 16:55:07.294: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jul  5 16:55:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9975" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":229,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:07.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5704
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:55:08.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 5, 16, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 16, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 16, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:55:11.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:55:11.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5704" for this suite.
STEP: Destroying namespace "webhook-5704-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":230,"skipped":4432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:11.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2881
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jul  5 16:55:13.499: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2881 pod-service-account-94bd256d-7a17-4f87-b6da-4947b4bd9afb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  5 16:55:13.836: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2881 pod-service-account-94bd256d-7a17-4f87-b6da-4947b4bd9afb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  5 16:55:14.184: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2881 pod-service-account-94bd256d-7a17-4f87-b6da-4947b4bd9afb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jul  5 16:55:14.534: INFO: Got root ca configmap in namespace "svcaccounts-2881"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 16:55:14.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2881" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":231,"skipped":4462,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:14.563: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6648
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jul  5 16:55:14.725: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 16:55:18.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6648" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":232,"skipped":4464,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:18.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3422
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-abae35ea-c9c7-4917-8d12-0f1e15929fa5
STEP: Creating a pod to test consume configMaps
Jul  5 16:55:18.301: INFO: Waiting up to 5m0s for pod "pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439" in namespace "configmap-3422" to be "Succeeded or Failed"
Jul  5 16:55:18.309: INFO: Pod "pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439": Phase="Pending", Reason="", readiness=false. Elapsed: 7.623351ms
Jul  5 16:55:20.318: INFO: Pod "pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016578162s
Jul  5 16:55:22.327: INFO: Pod "pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025902583s
STEP: Saw pod success
Jul  5 16:55:22.327: INFO: Pod "pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439" satisfied condition "Succeeded or Failed"
Jul  5 16:55:22.335: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 16:55:22.362: INFO: Waiting for pod pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439 to disappear
Jul  5 16:55:22.369: INFO: Pod pod-configmaps-29a19c2b-0402-48ad-b656-0f8753f25439 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:55:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3422" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":4479,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:22.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1652
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-fe933e2d-c768-42ac-8f9c-f1c5d0f791f8
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 16:55:24.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1652" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":234,"skipped":4489,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:24.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5158
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5158
Jul  5 16:55:24.883: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:55:26.891: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul  5 16:55:26.899: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul  5 16:55:27.282: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul  5 16:55:27.282: INFO: stdout: "iptables"
Jul  5 16:55:27.282: INFO: proxyMode: iptables
Jul  5 16:55:27.293: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul  5 16:55:27.300: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5158
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5158
I0705 16:55:27.321650    6089 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5158, replica count: 3
I0705 16:55:30.373383    6089 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 16:55:30.401: INFO: Creating new exec pod
Jul  5 16:55:33.443: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul  5 16:55:33.764: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul  5 16:55:33.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:55:33.764: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.72.203 80'
Jul  5 16:55:34.088: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.26.72.203 80\nConnection to 172.26.72.203 80 port [tcp/http] succeeded!\n"
Jul  5 16:55:34.088: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:55:34.088: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.206 30446'
Jul  5 16:55:34.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.25.206 30446\nConnection to 10.250.25.206 30446 port [tcp/*] succeeded!\n"
Jul  5 16:55:34.393: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:55:34.394: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.25.207 30446'
Jul  5 16:55:34.751: INFO: stderr: "+ + nc -v -t -w 2 10.250.25.207 30446\necho hostName\nConnection to 10.250.25.207 30446 port [tcp/*] succeeded!\n"
Jul  5 16:55:34.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 16:55:34.751: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.25.206:30446/ ; done'
Jul  5 16:55:35.108: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n"
Jul  5 16:55:35.108: INFO: stdout: "\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh\naffinity-nodeport-timeout-7t9zh"
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Received response from host: affinity-nodeport-timeout-7t9zh
Jul  5 16:55:35.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.25.206:30446/'
Jul  5 16:55:35.443: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n"
Jul  5 16:55:35.444: INFO: stdout: "affinity-nodeport-timeout-7t9zh"
Jul  5 16:55:55.446: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5158 exec execpod-affinityrv49g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.25.206:30446/'
Jul  5 16:55:55.829: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.25.206:30446/\n"
Jul  5 16:55:55.829: INFO: stdout: "affinity-nodeport-timeout-6p5tm"
Jul  5 16:55:55.829: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5158, will wait for the garbage collector to delete the pods
Jul  5 16:55:55.906: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.57157ms
Jul  5 16:55:56.007: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.906367ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 16:55:58.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5158" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":235,"skipped":4508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:55:58.344: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3277
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul  5 16:55:58.525: INFO: Waiting up to 1m0s for all nodes to be ready
Jul  5 16:56:58.597: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:56:58.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5895
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul  5 16:57:00.813: INFO: found a healthy node: izgw8bazids4c4cxzuus22z
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:57:08.940: INFO: pods created so far: [1 1 1]
Jul  5 16:57:08.940: INFO: length of pods created so far: 3
Jul  5 16:57:10.959: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jul  5 16:57:17.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5895" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jul  5 16:57:18.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3277" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":236,"skipped":4531,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:57:18.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6886
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 16:57:18.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Jul  5 16:57:20.341: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:20Z]] name:name1 resourceVersion:33935 uid:6ac2f828-089b-4af2-9ebd-a5ba7d3229bb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul  5 16:57:30.354: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:30Z]] name:name2 resourceVersion:34034 uid:725c38eb-b61d-4845-8a1e-a0777180210c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul  5 16:57:40.364: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:40Z]] name:name1 resourceVersion:34077 uid:6ac2f828-089b-4af2-9ebd-a5ba7d3229bb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul  5 16:57:50.373: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:50Z]] name:name2 resourceVersion:34120 uid:725c38eb-b61d-4845-8a1e-a0777180210c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul  5 16:58:00.386: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:40Z]] name:name1 resourceVersion:34164 uid:6ac2f828-089b-4af2-9ebd-a5ba7d3229bb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul  5 16:58:10.398: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-05T16:57:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-05T16:57:50Z]] name:name2 resourceVersion:34208 uid:725c38eb-b61d-4845-8a1e-a0777180210c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:58:20.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6886" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":237,"skipped":4547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:20.944: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6763
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1251
STEP: Creating secret with name secret-test-c380c65b-a1ca-4acc-834e-5b53f22127b7
STEP: Creating a pod to test consume secrets
Jul  5 16:58:21.270: INFO: Waiting up to 5m0s for pod "pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8" in namespace "secrets-6763" to be "Succeeded or Failed"
Jul  5 16:58:21.277: INFO: Pod "pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.0235ms
Jul  5 16:58:23.287: INFO: Pod "pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016417507s
Jul  5 16:58:25.295: INFO: Pod "pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025237633s
STEP: Saw pod success
Jul  5 16:58:25.295: INFO: Pod "pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8" satisfied condition "Succeeded or Failed"
Jul  5 16:58:25.305: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 16:58:25.332: INFO: Waiting for pod pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8 to disappear
Jul  5 16:58:25.339: INFO: Pod pod-secrets-2f4dbe94-dd77-4738-85fb-a15477c095f8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 16:58:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6763" for this suite.
STEP: Destroying namespace "secret-namespace-1251" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":238,"skipped":4575,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:25.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7045
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jul  5 16:58:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7045" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":239,"skipped":4589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:27.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9740
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 16:58:28.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 16:58:31.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 16:58:31.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9740" for this suite.
STEP: Destroying namespace "webhook-9740-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":240,"skipped":4649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:31.730: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6965
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  5 16:58:31.905: INFO: Waiting up to 5m0s for pod "pod-8721308b-263b-4cf2-ae0b-a72d4628e955" in namespace "emptydir-6965" to be "Succeeded or Failed"
Jul  5 16:58:31.912: INFO: Pod "pod-8721308b-263b-4cf2-ae0b-a72d4628e955": Phase="Pending", Reason="", readiness=false. Elapsed: 6.910091ms
Jul  5 16:58:33.919: INFO: Pod "pod-8721308b-263b-4cf2-ae0b-a72d4628e955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014817373s
Jul  5 16:58:35.927: INFO: Pod "pod-8721308b-263b-4cf2-ae0b-a72d4628e955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022712779s
STEP: Saw pod success
Jul  5 16:58:35.927: INFO: Pod "pod-8721308b-263b-4cf2-ae0b-a72d4628e955" satisfied condition "Succeeded or Failed"
Jul  5 16:58:35.935: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-8721308b-263b-4cf2-ae0b-a72d4628e955 container test-container: <nil>
STEP: delete the pod
Jul  5 16:58:35.963: INFO: Waiting for pod pod-8721308b-263b-4cf2-ae0b-a72d4628e955 to disappear
Jul  5 16:58:35.970: INFO: Pod pod-8721308b-263b-4cf2-ae0b-a72d4628e955 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:58:35.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6965" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":241,"skipped":4681,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:35.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7042
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  5 16:58:36.168: INFO: Waiting up to 5m0s for pod "pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c" in namespace "emptydir-7042" to be "Succeeded or Failed"
Jul  5 16:58:36.175: INFO: Pod "pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885489ms
Jul  5 16:58:38.184: INFO: Pod "pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015624935s
Jul  5 16:58:40.193: INFO: Pod "pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024971293s
STEP: Saw pod success
Jul  5 16:58:40.193: INFO: Pod "pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c" satisfied condition "Succeeded or Failed"
Jul  5 16:58:40.200: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c container test-container: <nil>
STEP: delete the pod
Jul  5 16:58:40.226: INFO: Waiting for pod pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c to disappear
Jul  5 16:58:40.233: INFO: Pod pod-dd39fb98-5a0d-4ee5-bdeb-9d50dc5a2b7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 16:58:40.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7042" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4682,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:58:40.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9818
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-9818
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 16:58:40.418: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul  5 16:58:40.465: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 16:58:42.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:44.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:46.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:48.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:50.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:52.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:54.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:56.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:58:58.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:59:00.475: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul  5 16:59:02.474: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul  5 16:59:02.488: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul  5 16:59:04.556: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul  5 16:59:04.556: INFO: Going to poll 172.16.0.247 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jul  5 16:59:04.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.247:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:59:04.563: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:59:04.564: INFO: ExecWithOptions: Clientset creation
Jul  5 16:59:04.564: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.0.247%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul  5 16:59:04.812: INFO: Found all 1 expected endpoints: [netserver-0]
Jul  5 16:59:04.812: INFO: Going to poll 172.16.1.155 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jul  5 16:59:04.820: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.1.155:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 16:59:04.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 16:59:04.820: INFO: ExecWithOptions: Clientset creation
Jul  5 16:59:04.820: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/pod-network-test-9818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.1.155%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul  5 16:59:05.064: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jul  5 16:59:05.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9818" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":243,"skipped":4702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 16:59:05.086: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7270
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7270
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7270
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7270
Jul  5 16:59:05.279: INFO: Found 0 stateful pods, waiting for 1
Jul  5 16:59:15.290: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  5 16:59:15.297: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:59:15.660: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:59:15.660: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:59:15.660: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 16:59:15.667: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  5 16:59:25.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 16:59:25.676: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 16:59:25.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999663s
Jul  5 16:59:26.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992332793s
Jul  5 16:59:27.722: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984459285s
Jul  5 16:59:28.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976629649s
Jul  5 16:59:29.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9686426s
Jul  5 16:59:30.747: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.95974524s
Jul  5 16:59:31.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951413467s
Jul  5 16:59:32.764: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943028143s
Jul  5 16:59:33.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.935053646s
Jul  5 16:59:34.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 926.684289ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7270
Jul  5 16:59:35.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 16:59:36.094: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 16:59:36.094: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 16:59:36.094: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 16:59:36.101: INFO: Found 1 stateful pods, waiting for 3
Jul  5 16:59:46.110: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:59:46.110: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 16:59:46.110: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  5 16:59:46.125: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:59:46.500: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:59:46.500: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:59:46.500: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 16:59:46.500: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:59:46.876: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:59:46.876: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:59:46.876: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 16:59:46.876: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 16:59:47.200: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 16:59:47.200: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 16:59:47.200: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 16:59:47.200: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 16:59:47.207: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul  5 16:59:57.222: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 16:59:57.223: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 16:59:57.223: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 16:59:57.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999636s
Jul  5 16:59:58.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991650362s
Jul  5 16:59:59.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983270826s
Jul  5 17:00:00.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974986796s
Jul  5 17:00:01.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966227938s
Jul  5 17:00:02.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957826808s
Jul  5 17:00:03.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949348928s
Jul  5 17:00:04.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93938534s
Jul  5 17:00:05.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930608835s
Jul  5 17:00:06.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 918.656117ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7270
Jul  5 17:00:07.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:00:27.464: INFO: rc: 1
Jul  5 17:00:27.464: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: error dialing backend: proxy error from vpn-seed-server:9443 while dialing 10.250.25.207:10250, code 503: 503 Service Unavailable

error:
exit status 1
Jul  5 17:00:37.464: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:00:37.779: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 17:00:37.779: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:00:37.779: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:00:37.779: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:00:38.109: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 17:00:38.109: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:00:38.109: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:00:38.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-7270 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:00:38.437: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 17:00:38.437: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:00:38.437: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:00:38.437: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 17:00:48.471: INFO: Deleting all statefulset in ns statefulset-7270
Jul  5 17:00:48.479: INFO: Scaling statefulset ss to 0
Jul  5 17:00:48.502: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:00:48.509: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 17:00:48.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7270" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":244,"skipped":4729,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:00:48.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4890
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:00:48.739: INFO: The status of Pod server-envvars-5aec27bd-6ff0-476e-a987-e2fa2f31c741 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:00:50.748: INFO: The status of Pod server-envvars-5aec27bd-6ff0-476e-a987-e2fa2f31c741 is Running (Ready = true)
Jul  5 17:00:50.778: INFO: Waiting up to 5m0s for pod "client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e" in namespace "pods-4890" to be "Succeeded or Failed"
Jul  5 17:00:50.785: INFO: Pod "client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114793ms
Jul  5 17:00:52.794: INFO: Pod "client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015960202s
Jul  5 17:00:54.803: INFO: Pod "client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025173951s
STEP: Saw pod success
Jul  5 17:00:54.803: INFO: Pod "client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e" satisfied condition "Succeeded or Failed"
Jul  5 17:00:54.810: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e container env3cont: <nil>
STEP: delete the pod
Jul  5 17:00:54.846: INFO: Waiting for pod client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e to disappear
Jul  5 17:00:54.853: INFO: Pod client-envvars-5db95f99-e779-405b-8403-fcd7f035fa0e no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 17:00:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4890" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":245,"skipped":4736,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:00:54.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9384
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:00:55.044: INFO: Got root ca configmap in namespace "svcaccounts-9384"
Jul  5 17:00:55.052: INFO: Deleted root ca configmap in namespace "svcaccounts-9384"
STEP: waiting for a new root ca configmap created
Jul  5 17:00:55.560: INFO: Recreated root ca configmap in namespace "svcaccounts-9384"
Jul  5 17:00:55.568: INFO: Updated root ca configmap in namespace "svcaccounts-9384"
STEP: waiting for the root ca configmap reconciled
Jul  5 17:00:56.077: INFO: Reconciled root ca configmap in namespace "svcaccounts-9384"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 17:00:56.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9384" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":246,"skipped":4738,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:00:56.098: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1884
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-ltng
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 17:00:56.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ltng" in namespace "subpath-1884" to be "Succeeded or Failed"
Jul  5 17:00:56.296: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Pending", Reason="", readiness=false. Elapsed: 7.055079ms
Jul  5 17:00:58.305: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 2.015496653s
Jul  5 17:01:00.315: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 4.025722596s
Jul  5 17:01:02.324: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 6.03480885s
Jul  5 17:01:04.332: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 8.043259587s
Jul  5 17:01:06.341: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 10.052184826s
Jul  5 17:01:08.350: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 12.060703535s
Jul  5 17:01:10.358: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 14.068695211s
Jul  5 17:01:12.366: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 16.076942592s
Jul  5 17:01:14.374: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 18.085143456s
Jul  5 17:01:16.383: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=true. Elapsed: 20.093512644s
Jul  5 17:01:18.392: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Running", Reason="", readiness=false. Elapsed: 22.102886502s
Jul  5 17:01:20.400: INFO: Pod "pod-subpath-test-configmap-ltng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.111397535s
STEP: Saw pod success
Jul  5 17:01:20.401: INFO: Pod "pod-subpath-test-configmap-ltng" satisfied condition "Succeeded or Failed"
Jul  5 17:01:20.408: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-subpath-test-configmap-ltng container test-container-subpath-configmap-ltng: <nil>
STEP: delete the pod
Jul  5 17:01:20.436: INFO: Waiting for pod pod-subpath-test-configmap-ltng to disappear
Jul  5 17:01:20.443: INFO: Pod pod-subpath-test-configmap-ltng no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ltng
Jul  5 17:01:20.443: INFO: Deleting pod "pod-subpath-test-configmap-ltng" in namespace "subpath-1884"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jul  5 17:01:20.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1884" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":247,"skipped":4747,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:20.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9087
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-4a36bc42-61be-4753-876c-a45caaac3da9
STEP: Creating a pod to test consume secrets
Jul  5 17:01:20.653: INFO: Waiting up to 5m0s for pod "pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6" in namespace "secrets-9087" to be "Succeeded or Failed"
Jul  5 17:01:20.660: INFO: Pod "pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.666666ms
Jul  5 17:01:22.669: INFO: Pod "pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015347013s
Jul  5 17:01:24.678: INFO: Pod "pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024307738s
STEP: Saw pod success
Jul  5 17:01:24.678: INFO: Pod "pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6" satisfied condition "Succeeded or Failed"
Jul  5 17:01:24.685: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:01:24.714: INFO: Waiting for pod pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6 to disappear
Jul  5 17:01:24.720: INFO: Pod pod-secrets-364b1f5e-d188-4505-94b3-dcd1bdf9f4c6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 17:01:24.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9087" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4750,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:24.741: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9053
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 17:01:27.955: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jul  5 17:01:27.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9053" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":249,"skipped":4758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:27.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3384
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:01:28.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  5 17:01:28.176: INFO: The status of Pod pod-exec-websocket-121338dd-e4d2-4bf3-824a-21294981ff20 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:01:30.185: INFO: The status of Pod pod-exec-websocket-121338dd-e4d2-4bf3-824a-21294981ff20 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 17:01:30.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3384" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4798,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:30.332: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6481
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:01:30.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6481" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":251,"skipped":4799,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:30.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7491
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7491
STEP: creating service affinity-clusterip in namespace services-7491
STEP: creating replication controller affinity-clusterip in namespace services-7491
I0705 17:01:30.698880    6089 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7491, replica count: 3
I0705 17:01:33.751527    6089 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 17:01:33.765: INFO: Creating new exec pod
Jul  5 17:01:36.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7491 exec execpod-affinitykkqkw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul  5 17:01:37.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul  5 17:01:37.103: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:01:37.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7491 exec execpod-affinitykkqkw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.222.62 80'
Jul  5 17:01:37.486: INFO: stderr: "+ nc -v -t -w 2 172.31.222.62 80\nConnection to 172.31.222.62 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jul  5 17:01:37.486: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:01:37.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7491 exec execpod-affinitykkqkw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.222.62:80/ ; done'
Jul  5 17:01:37.859: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.222.62:80/\n"
Jul  5 17:01:37.859: INFO: stdout: "\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn\naffinity-clusterip-wvmvn"
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Received response from host: affinity-clusterip-wvmvn
Jul  5 17:01:37.859: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7491, will wait for the garbage collector to delete the pods
Jul  5 17:01:37.936: INFO: Deleting ReplicationController affinity-clusterip took: 8.482971ms
Jul  5 17:01:38.037: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.025786ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:01:39.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7491" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":252,"skipped":4820,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:39.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-3941
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  5 17:01:40.163: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  5 17:01:40.177: INFO: starting watch
STEP: patching
STEP: updating
Jul  5 17:01:40.206: INFO: waiting for watch events with expected annotations
Jul  5 17:01:40.207: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jul  5 17:01:40.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3941" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":253,"skipped":4838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:40.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8107
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 17:01:40.974: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 17:01:44.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:01:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8107" for this suite.
STEP: Destroying namespace "webhook-8107-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":254,"skipped":4888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:44.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5093
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 17:01:51.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5093" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":255,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:51.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5196
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jul  5 17:01:51.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5196" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":256,"skipped":4932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:51.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5598
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul  5 17:01:53.940: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5598 PodName:pod-sharedvolume-c36ff1b4-2ada-41e4-896a-b7eeeb54bbe3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:01:53.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:01:53.941: INFO: ExecWithOptions: Clientset creation
Jul  5 17:01:53.941: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/emptydir-5598/pods/pod-sharedvolume-c36ff1b4-2ada-41e4-896a-b7eeeb54bbe3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jul  5 17:01:54.197: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 17:01:54.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5598" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":257,"skipped":4956,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:01:54.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2595
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:01:54.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jul  5 17:01:57.852: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2595 --namespace=crd-publish-openapi-2595 create -f -'
Jul  5 17:01:58.779: INFO: stderr: ""
Jul  5 17:01:58.779: INFO: stdout: "e2e-test-crd-publish-openapi-251-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul  5 17:01:58.779: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2595 --namespace=crd-publish-openapi-2595 delete e2e-test-crd-publish-openapi-251-crds test-cr'
Jul  5 17:01:58.884: INFO: stderr: ""
Jul  5 17:01:58.884: INFO: stdout: "e2e-test-crd-publish-openapi-251-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul  5 17:01:58.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2595 --namespace=crd-publish-openapi-2595 apply -f -'
Jul  5 17:01:59.135: INFO: stderr: ""
Jul  5 17:01:59.135: INFO: stdout: "e2e-test-crd-publish-openapi-251-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul  5 17:01:59.135: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2595 --namespace=crd-publish-openapi-2595 delete e2e-test-crd-publish-openapi-251-crds test-cr'
Jul  5 17:01:59.236: INFO: stderr: ""
Jul  5 17:01:59.236: INFO: stdout: "e2e-test-crd-publish-openapi-251-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul  5 17:01:59.236: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2595 explain e2e-test-crd-publish-openapi-251-crds'
Jul  5 17:01:59.440: INFO: stderr: ""
Jul  5 17:01:59.440: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-251-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:02:02.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2595" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":258,"skipped":4959,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:02.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9758
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  5 17:02:03.119: INFO: Waiting up to 5m0s for pod "pod-cb71e287-ad60-4a70-9e14-a6a918af02bd" in namespace "emptydir-9758" to be "Succeeded or Failed"
Jul  5 17:02:03.125: INFO: Pod "pod-cb71e287-ad60-4a70-9e14-a6a918af02bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802762ms
Jul  5 17:02:05.132: INFO: Pod "pod-cb71e287-ad60-4a70-9e14-a6a918af02bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013104096s
Jul  5 17:02:07.139: INFO: Pod "pod-cb71e287-ad60-4a70-9e14-a6a918af02bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019994569s
STEP: Saw pod success
Jul  5 17:02:07.139: INFO: Pod "pod-cb71e287-ad60-4a70-9e14-a6a918af02bd" satisfied condition "Succeeded or Failed"
Jul  5 17:02:07.145: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-cb71e287-ad60-4a70-9e14-a6a918af02bd container test-container: <nil>
STEP: delete the pod
Jul  5 17:02:07.171: INFO: Waiting for pod pod-cb71e287-ad60-4a70-9e14-a6a918af02bd to disappear
Jul  5 17:02:07.177: INFO: Pod pod-cb71e287-ad60-4a70-9e14-a6a918af02bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 17:02:07.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9758" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":259,"skipped":4959,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:07.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9812
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9812
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-9812
Jul  5 17:02:07.370: INFO: Found 0 stateful pods, waiting for 1
Jul  5 17:02:17.379: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jul  5 17:02:17.404: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jul  5 17:02:17.416: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jul  5 17:02:17.422: INFO: Observed &StatefulSet event: ADDED
Jul  5 17:02:17.422: INFO: Found Statefulset ss in namespace statefulset-9812 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul  5 17:02:17.422: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jul  5 17:02:17.422: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul  5 17:02:17.430: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jul  5 17:02:17.435: INFO: Observed &StatefulSet event: ADDED
Jul  5 17:02:17.435: INFO: Observed Statefulset ss in namespace statefulset-9812 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul  5 17:02:17.435: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 17:02:17.435: INFO: Deleting all statefulset in ns statefulset-9812
Jul  5 17:02:17.441: INFO: Scaling statefulset ss to 0
Jul  5 17:02:27.466: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:02:27.472: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 17:02:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9812" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":260,"skipped":4973,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:27.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1296
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:02:27.655: INFO: Creating pod...
Jul  5 17:02:29.681: INFO: Creating service...
Jul  5 17:02:29.690: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=DELETE
Jul  5 17:02:29.757: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  5 17:02:29.757: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=OPTIONS
Jul  5 17:02:29.772: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  5 17:02:29.772: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=PATCH
Jul  5 17:02:29.833: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  5 17:02:29.833: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=POST
Jul  5 17:02:29.888: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  5 17:02:29.888: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=PUT
Jul  5 17:02:29.900: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul  5 17:02:29.900: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=DELETE
Jul  5 17:02:29.912: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  5 17:02:29.912: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jul  5 17:02:29.923: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  5 17:02:29.923: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=PATCH
Jul  5 17:02:29.934: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  5 17:02:29.934: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=POST
Jul  5 17:02:29.945: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  5 17:02:29.945: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=PUT
Jul  5 17:02:29.956: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul  5 17:02:29.957: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=GET
Jul  5 17:02:29.962: INFO: http.Client request:GET StatusCode:301
Jul  5 17:02:29.962: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=GET
Jul  5 17:02:29.968: INFO: http.Client request:GET StatusCode:301
Jul  5 17:02:29.968: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/pods/agnhost/proxy?method=HEAD
Jul  5 17:02:29.973: INFO: http.Client request:HEAD StatusCode:301
Jul  5 17:02:29.973: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-1296/services/e2e-proxy-test-service/proxy?method=HEAD
Jul  5 17:02:29.979: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jul  5 17:02:29.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1296" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":261,"skipped":4976,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:29.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6042
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:02:30.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:02:33.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6042" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":262,"skipped":4989,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:33.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6711
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-6711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6711 to expose endpoints map[]
Jul  5 17:02:33.469: INFO: successfully validated that service endpoint-test2 in namespace services-6711 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6711
Jul  5 17:02:33.486: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:02:35.494: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6711 to expose endpoints map[pod1:[80]]
Jul  5 17:02:35.521: INFO: successfully validated that service endpoint-test2 in namespace services-6711 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jul  5 17:02:35.521: INFO: Creating new exec pod
Jul  5 17:02:38.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul  5 17:02:38.922: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:38.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:02:38.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.162.232 80'
Jul  5 17:02:39.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.162.232 80\nConnection to 172.28.162.232 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:39.302: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6711
Jul  5 17:02:39.318: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:02:41.326: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6711 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  5 17:02:41.360: INFO: successfully validated that service endpoint-test2 in namespace services-6711 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jul  5 17:02:42.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul  5 17:02:42.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:42.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:02:42.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.162.232 80'
Jul  5 17:02:43.026: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.162.232 80\nConnection to 172.28.162.232 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:43.026: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6711 to expose endpoints map[pod2:[80]]
Jul  5 17:02:43.058: INFO: successfully validated that service endpoint-test2 in namespace services-6711 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jul  5 17:02:44.059: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul  5 17:02:44.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:44.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:02:44.461: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6711 exec execpodnvvpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.28.162.232 80'
Jul  5 17:02:44.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.28.162.232 80\nConnection to 172.28.162.232 80 port [tcp/http] succeeded!\n"
Jul  5 17:02:44.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6711 to expose endpoints map[]
Jul  5 17:02:44.885: INFO: successfully validated that service endpoint-test2 in namespace services-6711 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:02:44.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6711" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":263,"skipped":4994,"failed":0}

------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:44.912: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-4404
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:02:45.347: INFO: Checking APIGroup: apiregistration.k8s.io
Jul  5 17:02:45.352: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul  5 17:02:45.352: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jul  5 17:02:45.352: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul  5 17:02:45.352: INFO: Checking APIGroup: apps
Jul  5 17:02:45.357: INFO: PreferredVersion.GroupVersion: apps/v1
Jul  5 17:02:45.357: INFO: Versions found [{apps/v1 v1}]
Jul  5 17:02:45.357: INFO: apps/v1 matches apps/v1
Jul  5 17:02:45.357: INFO: Checking APIGroup: events.k8s.io
Jul  5 17:02:45.361: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul  5 17:02:45.361: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.361: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul  5 17:02:45.361: INFO: Checking APIGroup: authentication.k8s.io
Jul  5 17:02:45.366: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul  5 17:02:45.366: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jul  5 17:02:45.366: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul  5 17:02:45.366: INFO: Checking APIGroup: authorization.k8s.io
Jul  5 17:02:45.370: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul  5 17:02:45.371: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jul  5 17:02:45.371: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul  5 17:02:45.371: INFO: Checking APIGroup: autoscaling
Jul  5 17:02:45.375: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jul  5 17:02:45.375: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul  5 17:02:45.375: INFO: autoscaling/v2 matches autoscaling/v2
Jul  5 17:02:45.375: INFO: Checking APIGroup: batch
Jul  5 17:02:45.380: INFO: PreferredVersion.GroupVersion: batch/v1
Jul  5 17:02:45.380: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul  5 17:02:45.380: INFO: batch/v1 matches batch/v1
Jul  5 17:02:45.380: INFO: Checking APIGroup: certificates.k8s.io
Jul  5 17:02:45.385: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul  5 17:02:45.385: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jul  5 17:02:45.385: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul  5 17:02:45.385: INFO: Checking APIGroup: networking.k8s.io
Jul  5 17:02:45.389: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul  5 17:02:45.389: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jul  5 17:02:45.389: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul  5 17:02:45.389: INFO: Checking APIGroup: policy
Jul  5 17:02:45.394: INFO: PreferredVersion.GroupVersion: policy/v1
Jul  5 17:02:45.394: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jul  5 17:02:45.394: INFO: policy/v1 matches policy/v1
Jul  5 17:02:45.394: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul  5 17:02:45.399: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul  5 17:02:45.399: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jul  5 17:02:45.399: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul  5 17:02:45.399: INFO: Checking APIGroup: storage.k8s.io
Jul  5 17:02:45.403: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul  5 17:02:45.403: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.403: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul  5 17:02:45.403: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul  5 17:02:45.408: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul  5 17:02:45.408: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jul  5 17:02:45.408: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul  5 17:02:45.408: INFO: Checking APIGroup: apiextensions.k8s.io
Jul  5 17:02:45.413: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul  5 17:02:45.413: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jul  5 17:02:45.413: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul  5 17:02:45.413: INFO: Checking APIGroup: scheduling.k8s.io
Jul  5 17:02:45.417: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul  5 17:02:45.417: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jul  5 17:02:45.417: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul  5 17:02:45.417: INFO: Checking APIGroup: coordination.k8s.io
Jul  5 17:02:45.422: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul  5 17:02:45.422: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jul  5 17:02:45.422: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul  5 17:02:45.422: INFO: Checking APIGroup: node.k8s.io
Jul  5 17:02:45.426: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul  5 17:02:45.426: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.426: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul  5 17:02:45.426: INFO: Checking APIGroup: discovery.k8s.io
Jul  5 17:02:45.431: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul  5 17:02:45.431: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.431: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul  5 17:02:45.431: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul  5 17:02:45.435: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jul  5 17:02:45.435: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.435: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jul  5 17:02:45.435: INFO: Checking APIGroup: autoscaling.k8s.io
Jul  5 17:02:45.440: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Jul  5 17:02:45.440: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Jul  5 17:02:45.440: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Jul  5 17:02:45.440: INFO: Checking APIGroup: crd.projectcalico.org
Jul  5 17:02:45.445: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul  5 17:02:45.445: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul  5 17:02:45.445: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul  5 17:02:45.445: INFO: Checking APIGroup: cert.gardener.cloud
Jul  5 17:02:45.449: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Jul  5 17:02:45.449: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Jul  5 17:02:45.449: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Jul  5 17:02:45.449: INFO: Checking APIGroup: dns.gardener.cloud
Jul  5 17:02:45.454: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Jul  5 17:02:45.454: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Jul  5 17:02:45.454: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Jul  5 17:02:45.454: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jul  5 17:02:45.458: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Jul  5 17:02:45.458: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.458: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Jul  5 17:02:45.458: INFO: Checking APIGroup: metrics.k8s.io
Jul  5 17:02:45.463: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul  5 17:02:45.463: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul  5 17:02:45.463: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jul  5 17:02:45.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4404" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":264,"skipped":4994,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:45.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3883
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:02:45.641: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6b98c452-c4ed-47a4-adf3-e09428ae07ad" in namespace "security-context-test-3883" to be "Succeeded or Failed"
Jul  5 17:02:45.647: INFO: Pod "busybox-readonly-false-6b98c452-c4ed-47a4-adf3-e09428ae07ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.656482ms
Jul  5 17:02:47.654: INFO: Pod "busybox-readonly-false-6b98c452-c4ed-47a4-adf3-e09428ae07ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012806445s
Jul  5 17:02:49.661: INFO: Pod "busybox-readonly-false-6b98c452-c4ed-47a4-adf3-e09428ae07ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01952393s
Jul  5 17:02:49.661: INFO: Pod "busybox-readonly-false-6b98c452-c4ed-47a4-adf3-e09428ae07ad" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 17:02:49.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3883" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":5006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:49.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9257
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-9257/configmap-test-27551bb4-f714-4a4f-bfb4-41e0c73dc263
STEP: Creating a pod to test consume configMaps
Jul  5 17:02:49.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509" in namespace "configmap-9257" to be "Succeeded or Failed"
Jul  5 17:02:49.853: INFO: Pod "pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509": Phase="Pending", Reason="", readiness=false. Elapsed: 5.95502ms
Jul  5 17:02:51.861: INFO: Pod "pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013264893s
Jul  5 17:02:53.868: INFO: Pod "pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020558507s
STEP: Saw pod success
Jul  5 17:02:53.868: INFO: Pod "pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509" satisfied condition "Succeeded or Failed"
Jul  5 17:02:53.874: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509 container env-test: <nil>
STEP: delete the pod
Jul  5 17:02:53.899: INFO: Waiting for pod pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509 to disappear
Jul  5 17:02:53.904: INFO: Pod pod-configmaps-d50c629c-5393-47ba-848e-86a163ec4509 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:02:53.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9257" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":266,"skipped":5038,"failed":0}

------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:53.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6946
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jul  5 17:02:54.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6946" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":267,"skipped":5038,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:02:54.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-899
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jul  5 17:02:54.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:03:14.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-899" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":268,"skipped":5058,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:03:14.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7031
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7031
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jul  5 17:03:14.807: INFO: Found 0 stateful pods, waiting for 3
Jul  5 17:03:24.815: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:03:24.815: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:03:24.815: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jul  5 17:03:24.854: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  5 17:03:34.899: INFO: Updating stateful set ss2
Jul  5 17:03:34.910: INFO: Waiting for Pod statefulset-7031/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jul  5 17:03:44.952: INFO: Found 1 stateful pods, waiting for 3
Jul  5 17:03:54.961: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:03:54.961: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:03:54.961: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  5 17:03:54.994: INFO: Updating stateful set ss2
Jul  5 17:03:55.006: INFO: Waiting for Pod statefulset-7031/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jul  5 17:04:05.044: INFO: Updating stateful set ss2
Jul  5 17:04:05.055: INFO: Waiting for StatefulSet statefulset-7031/ss2 to complete update
Jul  5 17:04:05.055: INFO: Waiting for Pod statefulset-7031/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 17:04:15.071: INFO: Deleting all statefulset in ns statefulset-7031
Jul  5 17:04:15.076: INFO: Scaling statefulset ss2 to 0
Jul  5 17:04:25.102: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:04:25.107: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 17:04:25.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7031" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":269,"skipped":5065,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:04:25.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9739
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9739.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9739.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 17:04:27.500: INFO: DNS probes using dns-9739/dns-test-53a0df04-dd7b-4efb-a1ce-4681426315fc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 17:04:27.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9739" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":270,"skipped":5076,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:04:27.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:04:27.666: INFO: Creating pod...
Jul  5 17:04:29.689: INFO: Creating service...
Jul  5 17:04:29.697: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/DELETE
Jul  5 17:04:29.808: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  5 17:04:29.808: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/GET
Jul  5 17:04:29.827: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul  5 17:04:29.827: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/HEAD
Jul  5 17:04:29.842: INFO: http.Client request:HEAD | StatusCode:200
Jul  5 17:04:29.842: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/OPTIONS
Jul  5 17:04:29.889: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  5 17:04:29.889: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/PATCH
Jul  5 17:04:29.903: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  5 17:04:29.903: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/POST
Jul  5 17:04:29.914: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  5 17:04:29.914: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/pods/agnhost/proxy/some/path/with/PUT
Jul  5 17:04:29.924: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul  5 17:04:29.924: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/DELETE
Jul  5 17:04:29.936: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul  5 17:04:29.936: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/GET
Jul  5 17:04:29.947: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul  5 17:04:29.947: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/HEAD
Jul  5 17:04:29.962: INFO: http.Client request:HEAD | StatusCode:200
Jul  5 17:04:29.962: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/OPTIONS
Jul  5 17:04:29.974: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul  5 17:04:29.974: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/PATCH
Jul  5 17:04:29.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul  5 17:04:29.985: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/POST
Jul  5 17:04:29.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul  5 17:04:29.995: INFO: Starting http.Client for https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-6593/services/test-service/proxy/some/path/with/PUT
Jul  5 17:04:30.006: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jul  5 17:04:30.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6593" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":271,"skipped":5081,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:04:30.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3494
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3494
Jul  5 17:04:30.178: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:04:32.184: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul  5 17:04:32.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul  5 17:04:32.489: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul  5 17:04:32.489: INFO: stdout: "iptables"
Jul  5 17:04:32.489: INFO: proxyMode: iptables
Jul  5 17:04:32.498: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul  5 17:04:32.503: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3494
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3494
I0705 17:04:32.516924    6089 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3494, replica count: 3
I0705 17:04:35.568801    6089 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 17:04:35.579: INFO: Creating new exec pod
Jul  5 17:04:38.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec execpod-affinitydpbkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul  5 17:04:38.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul  5 17:04:38.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:04:38.887: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec execpod-affinitydpbkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.178.1 80'
Jul  5 17:04:39.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.26.178.1 80\nConnection to 172.26.178.1 80 port [tcp/http] succeeded!\n"
Jul  5 17:04:39.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:04:39.200: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec execpod-affinitydpbkr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.26.178.1:80/ ; done'
Jul  5 17:04:39.603: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n"
Jul  5 17:04:39.603: INFO: stdout: "\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h\naffinity-clusterip-timeout-cc98h"
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Received response from host: affinity-clusterip-timeout-cc98h
Jul  5 17:04:39.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec execpod-affinitydpbkr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.26.178.1:80/'
Jul  5 17:04:39.942: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n"
Jul  5 17:04:39.942: INFO: stdout: "affinity-clusterip-timeout-cc98h"
Jul  5 17:04:59.945: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3494 exec execpod-affinitydpbkr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.26.178.1:80/'
Jul  5 17:05:00.309: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.26.178.1:80/\n"
Jul  5 17:05:00.309: INFO: stdout: "affinity-clusterip-timeout-zbh2k"
Jul  5 17:05:00.309: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3494, will wait for the garbage collector to delete the pods
Jul  5 17:05:00.379: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.36606ms
Jul  5 17:05:00.480: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.472326ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:05:02.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3494" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":272,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:05:02.407: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2190
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jul  5 17:05:02.567: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:05:04.574: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jul  5 17:05:05.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2190" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":273,"skipped":5117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:05:05.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3513
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3513
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-3513
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3513
Jul  5 17:05:05.770: INFO: Found 0 stateful pods, waiting for 1
Jul  5 17:05:15.777: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  5 17:05:15.783: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 17:05:16.186: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 17:05:16.186: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 17:05:16.186: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 17:05:16.192: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  5 17:05:26.199: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 17:05:26.199: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:05:26.221: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Jul  5 17:05:26.221: INFO: ss-0  izgw8bazids4c4cxzuus22z  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  }]
Jul  5 17:05:26.221: INFO: 
Jul  5 17:05:26.221: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  5 17:05:27.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993590798s
Jul  5 17:05:28.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987196411s
Jul  5 17:05:29.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981023404s
Jul  5 17:05:30.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975137067s
Jul  5 17:05:31.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967853993s
Jul  5 17:05:32.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961617503s
Jul  5 17:05:33.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95524901s
Jul  5 17:05:34.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948033027s
Jul  5 17:05:35.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 941.359533ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3513
Jul  5 17:05:36.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:05:36.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul  5 17:05:36.634: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:05:36.634: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:05:36.634: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:05:36.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  5 17:05:36.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:05:36.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:05:36.925: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul  5 17:05:37.296: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  5 17:05:37.297: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul  5 17:05:37.297: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul  5 17:05:37.302: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:05:37.302: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 17:05:37.302: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  5 17:05:37.308: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 17:05:37.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 17:05:37.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 17:05:37.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 17:05:37.638: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 17:05:37.961: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 17:05:37.961: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 17:05:37.961: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 17:05:37.961: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3513 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul  5 17:05:38.252: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul  5 17:05:38.252: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul  5 17:05:38.252: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul  5 17:05:38.252: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:05:38.257: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  5 17:05:48.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 17:05:48.270: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 17:05:48.270: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 17:05:48.287: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Jul  5 17:05:48.287: INFO: ss-0  izgw8bazids4c4cxzuus22z  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  }]
Jul  5 17:05:48.287: INFO: ss-1  izgw85sex2ooqi4ztetrj0z  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  }]
Jul  5 17:05:48.287: INFO: ss-2  izgw8bazids4c4cxzuus22z  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  }]
Jul  5 17:05:48.287: INFO: 
Jul  5 17:05:48.287: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  5 17:05:49.293: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Jul  5 17:05:49.293: INFO: ss-0  izgw8bazids4c4cxzuus22z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:05 +0000 UTC  }]
Jul  5 17:05:49.293: INFO: ss-2  izgw8bazids4c4cxzuus22z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-05 17:05:26 +0000 UTC  }]
Jul  5 17:05:49.293: INFO: 
Jul  5 17:05:49.293: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  5 17:05:50.299: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988281668s
Jul  5 17:05:51.304: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982780823s
Jul  5 17:05:52.310: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977358624s
Jul  5 17:05:53.315: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.971561267s
Jul  5 17:05:54.321: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966344492s
Jul  5 17:05:55.327: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960230639s
Jul  5 17:05:56.333: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954257577s
Jul  5 17:05:57.338: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.763885ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3513
Jul  5 17:05:58.344: INFO: Scaling statefulset ss to 0
Jul  5 17:05:58.360: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul  5 17:05:58.365: INFO: Deleting all statefulset in ns statefulset-3513
Jul  5 17:05:58.371: INFO: Scaling statefulset ss to 0
Jul  5 17:05:58.387: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 17:05:58.392: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jul  5 17:05:58.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3513" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":274,"skipped":5151,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:05:58.422: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8062
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-6772fb2d-fa3e-4049-a2d3-12ffe2cac4d7
STEP: Creating a pod to test consume secrets
Jul  5 17:05:58.581: INFO: Waiting up to 5m0s for pod "pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed" in namespace "secrets-8062" to be "Succeeded or Failed"
Jul  5 17:05:58.586: INFO: Pod "pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660202ms
Jul  5 17:06:00.591: INFO: Pod "pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01029873s
Jul  5 17:06:02.598: INFO: Pod "pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017189841s
STEP: Saw pod success
Jul  5 17:06:02.598: INFO: Pod "pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed" satisfied condition "Succeeded or Failed"
Jul  5 17:06:02.604: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:06:02.637: INFO: Waiting for pod pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed to disappear
Jul  5 17:06:02.642: INFO: Pod pod-secrets-bafcc38c-1f7b-45c9-a348-80f9208405ed no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 17:06:02.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8062" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":275,"skipped":5163,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:02.657: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-4688
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jul  5 17:06:02.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4688" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":276,"skipped":5163,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:02.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5340
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  5 17:06:03.005: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  5 17:06:08.012: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jul  5 17:06:08.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5340" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":277,"skipped":5165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:08.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8722
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul  5 17:06:08.191: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:06:10.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:06:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8722" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":278,"skipped":5239,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:23.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8078
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jul  5 17:06:23.374: INFO: Waiting up to 5m0s for pod "var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5" in namespace "var-expansion-8078" to be "Succeeded or Failed"
Jul  5 17:06:23.379: INFO: Pod "var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.779395ms
Jul  5 17:06:25.386: INFO: Pod "var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012115431s
Jul  5 17:06:27.394: INFO: Pod "var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01985802s
STEP: Saw pod success
Jul  5 17:06:27.394: INFO: Pod "var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5" satisfied condition "Succeeded or Failed"
Jul  5 17:06:27.399: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5 container dapi-container: <nil>
STEP: delete the pod
Jul  5 17:06:27.425: INFO: Waiting for pod var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5 to disappear
Jul  5 17:06:27.430: INFO: Pod var-expansion-08c9cbc0-4739-4a20-9578-5c71afd520d5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 17:06:27.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8078" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5252,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:27.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2891
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:06:27.618: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 17:06:27.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:06:27.636: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:06:28.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:06:28.653: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:06:29.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 17:06:29.653: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  5 17:06:29.695: INFO: Wrong image for pod: daemon-set-b7xds. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul  5 17:06:30.712: INFO: Wrong image for pod: daemon-set-b7xds. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul  5 17:06:31.712: INFO: Wrong image for pod: daemon-set-b7xds. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul  5 17:06:31.712: INFO: Pod daemon-set-jzhdq is not available
Jul  5 17:06:33.713: INFO: Pod daemon-set-6mngd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  5 17:06:33.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:06:33.735: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:06:34.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 17:06:34.752: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2891, will wait for the garbage collector to delete the pods
Jul  5 17:06:34.843: INFO: Deleting DaemonSet.extensions daemon-set took: 7.229361ms
Jul  5 17:06:34.944: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.961685ms
Jul  5 17:06:37.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:06:37.151: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 17:06:37.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38203"},"items":null}

Jul  5 17:06:37.161: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38203"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 17:06:37.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2891" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":280,"skipped":5272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:37.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5540
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul  5 17:06:37.699: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 17:06:40.727: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:06:40.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:06:44.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5540" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":281,"skipped":5300,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:06:44.090: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1439
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul  5 17:06:44.237: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 17:06:44.250: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 17:06:44.255: INFO: 
Logging pods the apiserver thinks is on node izgw85sex2ooqi4ztetrj0z before test
Jul  5 17:06:44.269: INFO: addons-nginx-ingress-controller-696bcbf64f-m7q4w from kube-system started at 2022-07-05 16:09:49 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul  5 17:06:44.269: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-b6c66fdff-kc56j from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul  5 17:06:44.269: INFO: apiserver-proxy-pdxqb from kube-system started at 2022-07-05 15:47:30 +0000 UTC (2 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container proxy ready: true, restart count 0
Jul  5 17:06:44.269: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 17:06:44.269: INFO: blackbox-exporter-ccdc4b99c-jndnq from kube-system started at 2022-07-05 16:43:27 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 17:06:44.269: INFO: calico-kube-controllers-6959b48bb7-bjpsr from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul  5 17:06:44.269: INFO: calico-node-gpt27 from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 17:06:44.269: INFO: calico-node-vertical-autoscaler-5b74b8f994-rthwq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 17:06:44.269: INFO: calico-typha-horizontal-autoscaler-55ff99f5cf-8rvnc from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 17:06:44.269: INFO: calico-typha-vertical-autoscaler-78b946fc85-59kkq from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container autoscaler ready: true, restart count 0
Jul  5 17:06:44.269: INFO: coredns-7f49f7db48-x8h24 from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container coredns ready: true, restart count 0
Jul  5 17:06:44.269: INFO: coredns-7f49f7db48-zglbj from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container coredns ready: true, restart count 0
Jul  5 17:06:44.269: INFO: csi-disk-plugin-alicloud-8pszw from kube-system started at 2022-07-05 15:47:30 +0000 UTC (3 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 17:06:44.269: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 17:06:44.269: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 17:06:44.269: INFO: egress-filter-applier-p45dc from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 17:06:44.269: INFO: kube-proxy-worker-1-v1.24.2-pjhw2 from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 17:06:44.269: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 17:06:44.269: INFO: metrics-server-788cb89-tfl5k from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 17:06:44.269: INFO: node-exporter-p2ztl from kube-system started at 2022-07-05 15:47:30 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 17:06:44.269: INFO: node-problem-detector-zs9bx from kube-system started at 2022-07-05 16:15:28 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul  5 17:06:44.269: INFO: vpn-shoot-5fcf58b56b-k9pwx from kube-system started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul  5 17:06:44.269: INFO: dashboard-metrics-scraper-9c4f98cd5-sthht from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul  5 17:06:44.269: INFO: kubernetes-dashboard-55d4694cd7-fscg2 from kubernetes-dashboard started at 2022-07-05 15:48:06 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.269: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul  5 17:06:44.269: INFO: 
Logging pods the apiserver thinks is on node izgw8bazids4c4cxzuus22z before test
Jul  5 17:06:44.278: INFO: apiserver-proxy-nxtmb from kube-system started at 2022-07-05 15:47:45 +0000 UTC (2 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container proxy ready: true, restart count 0
Jul  5 17:06:44.278: INFO: 	Container sidecar ready: true, restart count 0
Jul  5 17:06:44.278: INFO: blackbox-exporter-ccdc4b99c-qpfrc from kube-system started at 2022-07-05 16:42:27 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul  5 17:06:44.278: INFO: calico-node-4fk5s from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 17:06:44.278: INFO: calico-typha-deploy-7f646d87dc-6rg66 from kube-system started at 2022-07-05 15:48:39 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container calico-typha ready: true, restart count 0
Jul  5 17:06:44.278: INFO: csi-disk-plugin-alicloud-7czsh from kube-system started at 2022-07-05 15:47:45 +0000 UTC (3 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container csi-diskplugin ready: true, restart count 0
Jul  5 17:06:44.278: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul  5 17:06:44.278: INFO: 	Container driver-registrar ready: true, restart count 0
Jul  5 17:06:44.278: INFO: egress-filter-applier-p65qk from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container egress-filter-applier ready: true, restart count 0
Jul  5 17:06:44.278: INFO: kube-proxy-worker-1-v1.24.2-mxz5v from kube-system started at 2022-07-05 15:54:28 +0000 UTC (2 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul  5 17:06:44.278: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 17:06:44.278: INFO: node-exporter-gjd9k from kube-system started at 2022-07-05 15:47:45 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container node-exporter ready: true, restart count 0
Jul  5 17:06:44.278: INFO: node-problem-detector-qpx9f from kube-system started at 2022-07-05 16:15:27 +0000 UTC (1 container statuses recorded)
Jul  5 17:06:44.278: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8ba41d75-a14a-4fb2-8d62-4de536abe191 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.25.207 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-8ba41d75-a14a-4fb2-8d62-4de536abe191 off the node izgw8bazids4c4cxzuus22z
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8ba41d75-a14a-4fb2-8d62-4de536abe191
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jul  5 17:11:48.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1439" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.361 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":282,"skipped":5305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:48.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2985
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jul  5 17:11:48.599: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jul  5 17:11:54.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2985" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":283,"skipped":5332,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:54.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4383
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-5c8bded1-897c-4f5a-ae73-4bb3dc5f5b4b
STEP: Creating a pod to test consume configMaps
Jul  5 17:11:54.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e" in namespace "configmap-4383" to be "Succeeded or Failed"
Jul  5 17:11:54.238: INFO: Pod "pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09938ms
Jul  5 17:11:56.244: INFO: Pod "pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011355791s
Jul  5 17:11:58.252: INFO: Pod "pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018972112s
STEP: Saw pod success
Jul  5 17:11:58.252: INFO: Pod "pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e" satisfied condition "Succeeded or Failed"
Jul  5 17:11:58.258: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:11:58.330: INFO: Waiting for pod pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e to disappear
Jul  5 17:11:58.335: INFO: Pod pod-configmaps-ce844ce4-f0e4-4a09-839e-e72ecebb8d8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:11:58.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4383" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:58.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4882
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:11:58.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4882" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":285,"skipped":5368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:58.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9571
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jul  5 17:11:58.699: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9571 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 17:11:58.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9571" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":286,"skipped":5390,"failed":0}
S
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:58.799: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename conformance-tests
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in conformance-tests-4721
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jul  5 17:11:58.948: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jul  5 17:11:58.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4721" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":287,"skipped":5391,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:11:58.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6339
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-aa37391e-b9f0-4db5-a6c5-f6f49ee1c229
STEP: Creating a pod to test consume secrets
Jul  5 17:11:59.137: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1" in namespace "projected-6339" to be "Succeeded or Failed"
Jul  5 17:11:59.142: INFO: Pod "pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.312099ms
Jul  5 17:12:01.149: INFO: Pod "pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011584835s
Jul  5 17:12:03.155: INFO: Pod "pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01836211s
STEP: Saw pod success
Jul  5 17:12:03.155: INFO: Pod "pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1" satisfied condition "Succeeded or Failed"
Jul  5 17:12:03.161: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:12:03.230: INFO: Waiting for pod pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1 to disappear
Jul  5 17:12:03.235: INFO: Pod pod-projected-secrets-d4ae18cb-8622-4838-bfe5-f22e6dd9a5e1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 17:12:03.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6339" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":288,"skipped":5402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:03.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1406
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 17:12:03.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef" in namespace "projected-1406" to be "Succeeded or Failed"
Jul  5 17:12:03.420: INFO: Pod "downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.937908ms
Jul  5 17:12:05.428: INFO: Pod "downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013564125s
Jul  5 17:12:07.435: INFO: Pod "downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020088285s
STEP: Saw pod success
Jul  5 17:12:07.435: INFO: Pod "downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef" satisfied condition "Succeeded or Failed"
Jul  5 17:12:07.440: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef container client-container: <nil>
STEP: delete the pod
Jul  5 17:12:07.505: INFO: Waiting for pod downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef to disappear
Jul  5 17:12:07.510: INFO: Pod downwardapi-volume-467a9530-66c3-4ce1-aeb4-2e9870accfef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 17:12:07.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1406" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":289,"skipped":5448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:07.526: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9858
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 17:12:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9858" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":290,"skipped":5480,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:20.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5690
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-75beecd3-f3ad-4c06-87b1-9850be552706
STEP: Creating a pod to test consume secrets
Jul  5 17:12:20.952: INFO: Waiting up to 5m0s for pod "pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401" in namespace "secrets-5690" to be "Succeeded or Failed"
Jul  5 17:12:20.958: INFO: Pod "pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401": Phase="Pending", Reason="", readiness=false. Elapsed: 5.748073ms
Jul  5 17:12:22.965: INFO: Pod "pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012988018s
Jul  5 17:12:24.973: INFO: Pod "pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020300097s
STEP: Saw pod success
Jul  5 17:12:24.973: INFO: Pod "pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401" satisfied condition "Succeeded or Failed"
Jul  5 17:12:24.978: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:12:25.003: INFO: Waiting for pod pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401 to disappear
Jul  5 17:12:25.008: INFO: Pod pod-secrets-6bb1b346-aa85-4771-b0aa-2c09ab759401 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 17:12:25.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5690" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":291,"skipped":5492,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:25.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4967
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 17:12:25.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 17:12:28.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:12:28.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4967" for this suite.
STEP: Destroying namespace "webhook-4967-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":292,"skipped":5494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:28.938: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1919
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jul  5 17:12:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1919" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":293,"skipped":5526,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:29.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1611
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jul  5 17:12:29.319: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:12:31.327: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jul  5 17:12:31.348: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:12:33.355: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  5 17:12:33.361: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:33.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:33.362: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:33.362: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul  5 17:12:33.610: INFO: Exec stderr: ""
Jul  5 17:12:33.610: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:33.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:33.611: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:33.611: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul  5 17:12:33.902: INFO: Exec stderr: ""
Jul  5 17:12:33.902: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:33.902: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:33.902: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:33.903: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul  5 17:12:34.182: INFO: Exec stderr: ""
Jul  5 17:12:34.182: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:34.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:34.183: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:34.183: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul  5 17:12:34.424: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  5 17:12:34.424: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:34.424: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:34.424: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:34.425: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul  5 17:12:34.648: INFO: Exec stderr: ""
Jul  5 17:12:34.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:34.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:34.649: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:34.649: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul  5 17:12:34.917: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  5 17:12:34.917: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:34.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:34.918: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:34.918: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul  5 17:12:35.172: INFO: Exec stderr: ""
Jul  5 17:12:35.172: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:35.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:35.173: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:35.173: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul  5 17:12:35.476: INFO: Exec stderr: ""
Jul  5 17:12:35.477: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:35.477: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:35.477: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:35.477: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul  5 17:12:35.729: INFO: Exec stderr: ""
Jul  5 17:12:35.729: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:12:35.729: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:12:35.730: INFO: ExecWithOptions: Clientset creation
Jul  5 17:12:35.730: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/e2e-kubelet-etc-hosts-1611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul  5 17:12:35.964: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jul  5 17:12:35.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1611" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":294,"skipped":5544,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:12:35.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7208
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jul  5 17:14:00.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7208" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":295,"skipped":5545,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:14:00.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8694
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-f44cd51e-de57-433f-845e-f1eeab98fdbf in namespace container-probe-8694
Jul  5 17:14:02.345: INFO: Started pod busybox-f44cd51e-de57-433f-845e-f1eeab98fdbf in namespace container-probe-8694
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 17:14:02.350: INFO: Initial restart count of pod busybox-f44cd51e-de57-433f-845e-f1eeab98fdbf is 0
Jul  5 17:14:52.541: INFO: Restart count of pod container-probe-8694/busybox-f44cd51e-de57-433f-845e-f1eeab98fdbf is now 1 (50.191162478s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 17:14:52.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8694" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":296,"skipped":5555,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:14:52.567: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7140
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jul  5 17:14:52.721: INFO: Creating simple deployment test-deployment-b9vg9
Jul  5 17:14:52.738: INFO: deployment "test-deployment-b9vg9" doesn't have the required revision set
STEP: Getting /status
Jul  5 17:14:54.767: INFO: Deployment test-deployment-b9vg9 has Conditions: [{Available True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jul  5 17:14:54.780: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 17, 14, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 17, 14, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 5, 17, 14, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 5, 17, 14, 52, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b9vg9-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jul  5 17:14:54.785: INFO: Observed &Deployment event: ADDED
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b9vg9-688c4d6789"}
Jul  5 17:14:54.785: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b9vg9-688c4d6789"}
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul  5 17:14:54.785: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b9vg9-688c4d6789" is progressing.}
Jul  5 17:14:54.785: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul  5 17:14:54.785: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.}
Jul  5 17:14:54.786: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.786: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul  5 17:14:54.786: INFO: Observed Deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.}
Jul  5 17:14:54.786: INFO: Found Deployment test-deployment-b9vg9 in namespace deployment-7140 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul  5 17:14:54.786: INFO: Deployment test-deployment-b9vg9 has an updated status
STEP: patching the Statefulset Status
Jul  5 17:14:54.786: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul  5 17:14:54.793: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jul  5 17:14:54.798: INFO: Observed &Deployment event: ADDED
Jul  5 17:14:54.798: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b9vg9-688c4d6789"}
Jul  5 17:14:54.798: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.798: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b9vg9-688c4d6789"}
Jul  5 17:14:54.798: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul  5 17:14:54.798: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.798: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul  5 17:14:54.798: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:52 +0000 UTC 2022-07-05 17:14:52 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b9vg9-688c4d6789" is progressing.}
Jul  5 17:14:54.798: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.799: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul  5 17:14:54.799: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.}
Jul  5 17:14:54.799: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.799: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul  5 17:14:54.799: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-05 17:14:53 +0000 UTC 2022-07-05 17:14:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.}
Jul  5 17:14:54.799: INFO: Observed deployment test-deployment-b9vg9 in namespace deployment-7140 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul  5 17:14:54.799: INFO: Observed &Deployment event: MODIFIED
Jul  5 17:14:54.799: INFO: Found deployment test-deployment-b9vg9 in namespace deployment-7140 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jul  5 17:14:54.799: INFO: Deployment test-deployment-b9vg9 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 17:14:54.804: INFO: Deployment "test-deployment-b9vg9":
&Deployment{ObjectMeta:{test-deployment-b9vg9  deployment-7140  d5cdfed2-ba84-4af5-9692-d329db3b98b7 40949 1 2022-07-05 17:14:52 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-07-05 17:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-07-05 17:14:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-07-05 17:14:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ade108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-05 17:14:54 +0000 UTC,LastTransitionTime:2022-07-05 17:14:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-b9vg9-688c4d6789" has successfully progressed.,LastUpdateTime:2022-07-05 17:14:54 +0000 UTC,LastTransitionTime:2022-07-05 17:14:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 17:14:54.810: INFO: New ReplicaSet "test-deployment-b9vg9-688c4d6789" of Deployment "test-deployment-b9vg9":
&ReplicaSet{ObjectMeta:{test-deployment-b9vg9-688c4d6789  deployment-7140  a0425773-5e84-424e-87aa-80e98e071df5 40944 1 2022-07-05 17:14:52 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b9vg9 d5cdfed2-ba84-4af5-9692-d329db3b98b7 0xc004ade510 0xc004ade511}] []  [{kube-controller-manager Update apps/v1 2022-07-05 17:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5cdfed2-ba84-4af5-9692-d329db3b98b7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 17:14:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ade5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  5 17:14:54.817: INFO: Pod "test-deployment-b9vg9-688c4d6789-4cwhb" is available:
&Pod{ObjectMeta:{test-deployment-b9vg9-688c4d6789-4cwhb test-deployment-b9vg9-688c4d6789- deployment-7140  adeca136-7c24-4232-95a1-6e5021c00cd8 40943 0 2022-07-05 17:14:52 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:6cd8a44ac08220a6be17cb491768af72fe97f75c954dd8f729dfe60316f09d86 cni.projectcalico.org/podIP:172.16.1.208/32 cni.projectcalico.org/podIPs:172.16.1.208/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-b9vg9-688c4d6789 a0425773-5e84-424e-87aa-80e98e071df5 0xc00319b0e0 0xc00319b0e1}] []  [{kube-controller-manager Update v1 2022-07-05 17:14:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0425773-5e84-424e-87aa-80e98e071df5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 17:14:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 17:14:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7j7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7j7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:14:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:14:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:14:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:14:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.208,StartTime:2022-07-05 17:14:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 17:14:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7b98f19c52feaec7aac3fc807c4f0469f1046648673de2935bcc3094762890e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 17:14:54.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7140" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":297,"skipped":5565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:14:54.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-a92b0efb-d66d-4bba-87bf-afcaee63e80d
STEP: Creating a pod to test consume secrets
Jul  5 17:14:54.997: INFO: Waiting up to 5m0s for pod "pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd" in namespace "secrets-7593" to be "Succeeded or Failed"
Jul  5 17:14:55.002: INFO: Pod "pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178909ms
Jul  5 17:14:57.008: INFO: Pod "pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011823706s
Jul  5 17:14:59.015: INFO: Pod "pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018442573s
STEP: Saw pod success
Jul  5 17:14:59.015: INFO: Pod "pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd" satisfied condition "Succeeded or Failed"
Jul  5 17:14:59.021: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:14:59.045: INFO: Waiting for pod pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd to disappear
Jul  5 17:14:59.051: INFO: Pod pod-secrets-a37a0dc0-181f-4b00-bdba-489c09ece1bd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jul  5 17:14:59.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7593" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":298,"skipped":5638,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:14:59.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1286
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-0e8f1de1-a279-4605-aa1b-11bc06a256e5
STEP: Creating a pod to test consume secrets
Jul  5 17:14:59.232: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62" in namespace "projected-1286" to be "Succeeded or Failed"
Jul  5 17:14:59.237: INFO: Pod "pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.22369ms
Jul  5 17:15:01.245: INFO: Pod "pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012360174s
Jul  5 17:15:03.252: INFO: Pod "pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019358764s
STEP: Saw pod success
Jul  5 17:15:03.252: INFO: Pod "pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62" satisfied condition "Succeeded or Failed"
Jul  5 17:15:03.257: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:15:03.283: INFO: Waiting for pod pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62 to disappear
Jul  5 17:15:03.288: INFO: Pod pod-projected-secrets-51ce2f1e-7f5e-4ac3-9c56-6ce005248b62 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 17:15:03.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1286" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5641,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:03.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6838
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-zhpg
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 17:15:03.477: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zhpg" in namespace "subpath-6838" to be "Succeeded or Failed"
Jul  5 17:15:03.482: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.450395ms
Jul  5 17:15:05.489: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.012542741s
Jul  5 17:15:07.496: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 4.019683776s
Jul  5 17:15:09.503: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 6.026501438s
Jul  5 17:15:11.511: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 8.033870188s
Jul  5 17:15:13.518: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 10.040907896s
Jul  5 17:15:15.524: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 12.047674741s
Jul  5 17:15:17.531: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 14.054082871s
Jul  5 17:15:19.538: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 16.061168663s
Jul  5 17:15:21.544: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 18.067713749s
Jul  5 17:15:23.551: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=true. Elapsed: 20.074389624s
Jul  5 17:15:25.558: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Running", Reason="", readiness=false. Elapsed: 22.081005351s
Jul  5 17:15:27.565: INFO: Pod "pod-subpath-test-projected-zhpg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.087773445s
STEP: Saw pod success
Jul  5 17:15:27.565: INFO: Pod "pod-subpath-test-projected-zhpg" satisfied condition "Succeeded or Failed"
Jul  5 17:15:27.570: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-subpath-test-projected-zhpg container test-container-subpath-projected-zhpg: <nil>
STEP: delete the pod
Jul  5 17:15:27.595: INFO: Waiting for pod pod-subpath-test-projected-zhpg to disappear
Jul  5 17:15:27.601: INFO: Pod pod-subpath-test-projected-zhpg no longer exists
STEP: Deleting pod pod-subpath-test-projected-zhpg
Jul  5 17:15:27.601: INFO: Deleting pod "pod-subpath-test-projected-zhpg" in namespace "subpath-6838"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jul  5 17:15:27.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6838" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":300,"skipped":5655,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:27.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1536
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:15:27.773: INFO: Creating simple deployment test-new-deployment
Jul  5 17:15:27.790: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 17:15:29.853: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1536  a7a57a9c-09ff-4527-9f33-f7f9513a5e90 41225 3 2022-07-05 17:15:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-07-05 17:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 17:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00761c738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-07-05 17:15:28 +0000 UTC,LastTransitionTime:2022-07-05 17:15:27 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-07-05 17:15:29 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 17:15:29.859: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-1536  1e8700e8-9501-429a-9742-66f67f40d0a2 41229 3 2022-07-05 17:15:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a7a57a9c-09ff-4527-9f33-f7f9513a5e90 0xc00761cb57 0xc00761cb58}] []  [{kube-controller-manager Update apps/v1 2022-07-05 17:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7a57a9c-09ff-4527-9f33-f7f9513a5e90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-05 17:15:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00761cbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul  5 17:15:29.866: INFO: Pod "test-new-deployment-55df494869-52fr2" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-52fr2 test-new-deployment-55df494869- deployment-1536  cad16b96-bb37-4d70-8fb4-25b8a5e9e7b7 41232 0 2022-07-05 17:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 1e8700e8-9501-429a-9742-66f67f40d0a2 0xc001a653b7 0xc001a653b8}] []  [{kube-controller-manager Update v1 2022-07-05 17:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e8700e8-9501-429a-9742-66f67f40d0a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-05 17:15:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwlss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwlss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw85sex2ooqi4ztetrj0z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.206,PodIP:,StartTime:2022-07-05 17:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 17:15:29.866: INFO: Pod "test-new-deployment-55df494869-77k7w" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-77k7w test-new-deployment-55df494869- deployment-1536  647fd04a-fb5a-489e-88ca-86047974f3fa 41233 0 2022-07-05 17:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 1e8700e8-9501-429a-9742-66f67f40d0a2 0xc001a65577 0xc001a65578}] []  [{kube-controller-manager Update v1 2022-07-05 17:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e8700e8-9501-429a-9742-66f67f40d0a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqp55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqp55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 17:15:29.866: INFO: Pod "test-new-deployment-55df494869-msrdp" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-msrdp test-new-deployment-55df494869- deployment-1536  d8efae70-c48c-40ff-9e0f-f64b213cb8d6 41235 0 2022-07-05 17:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 1e8700e8-9501-429a-9742-66f67f40d0a2 0xc001a656d0 0xc001a656d1}] []  [{kube-controller-manager Update v1 2022-07-05 17:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e8700e8-9501-429a-9742-66f67f40d0a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55pd9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55pd9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul  5 17:15:29.866: INFO: Pod "test-new-deployment-55df494869-n2dn5" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-n2dn5 test-new-deployment-55df494869- deployment-1536  86573064-9352-4be6-a9fb-38190dd4f42c 41211 0 2022-07-05 17:15:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:9667e8e7874aba3853d7247fea242c047351c2ca2a2e7e117c62321537e6b5c8 cni.projectcalico.org/podIP:172.16.1.212/32 cni.projectcalico.org/podIPs:172.16.1.212/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 1e8700e8-9501-429a-9742-66f67f40d0a2 0xc001a65827 0xc001a65828}] []  [{kube-controller-manager Update v1 2022-07-05 17:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e8700e8-9501-429a-9742-66f67f40d0a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-07-05 17:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-07-05 17:15:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.1.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jbf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tms5g-6sg.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jbf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8bazids4c4cxzuus22z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-05 17:15:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.25.207,PodIP:172.16.1.212,StartTime:2022-07-05 17:15:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-05 17:15:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a0c14534aa0bdf6a52132705550e9bb6bc87024ea8e34b576af5c0a94eddf59f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.1.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 17:15:29.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1536" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":301,"skipped":5659,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:29.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9618
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jul  5 17:15:38.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9618" for this suite.
•{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":302,"skipped":5661,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:38.067: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7752
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jul  5 17:15:38.233: INFO: Waiting up to 5m0s for pod "client-containers-f079643b-2242-4978-bebc-b2c6730390cc" in namespace "containers-7752" to be "Succeeded or Failed"
Jul  5 17:15:38.239: INFO: Pod "client-containers-f079643b-2242-4978-bebc-b2c6730390cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57769ms
Jul  5 17:15:40.247: INFO: Pod "client-containers-f079643b-2242-4978-bebc-b2c6730390cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01347151s
Jul  5 17:15:42.254: INFO: Pod "client-containers-f079643b-2242-4978-bebc-b2c6730390cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020625759s
STEP: Saw pod success
Jul  5 17:15:42.254: INFO: Pod "client-containers-f079643b-2242-4978-bebc-b2c6730390cc" satisfied condition "Succeeded or Failed"
Jul  5 17:15:42.260: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod client-containers-f079643b-2242-4978-bebc-b2c6730390cc container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:15:42.284: INFO: Waiting for pod client-containers-f079643b-2242-4978-bebc-b2c6730390cc to disappear
Jul  5 17:15:42.293: INFO: Pod client-containers-f079643b-2242-4978-bebc-b2c6730390cc no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jul  5 17:15:42.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7752" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5678,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:42.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jul  5 17:15:42.480: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jul  5 17:15:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-160" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":304,"skipped":5684,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:44.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-695
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 17:15:44.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152" in namespace "projected-695" to be "Succeeded or Failed"
Jul  5 17:15:44.693: INFO: Pod "downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152": Phase="Pending", Reason="", readiness=false. Elapsed: 5.36585ms
Jul  5 17:15:46.700: INFO: Pod "downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254257s
Jul  5 17:15:48.714: INFO: Pod "downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026456373s
STEP: Saw pod success
Jul  5 17:15:48.714: INFO: Pod "downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152" satisfied condition "Succeeded or Failed"
Jul  5 17:15:48.720: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152 container client-container: <nil>
STEP: delete the pod
Jul  5 17:15:48.746: INFO: Waiting for pod downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152 to disappear
Jul  5 17:15:48.751: INFO: Pod downwardapi-volume-75fe2d5d-da95-4f97-af2e-689518467152 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 17:15:48.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-695" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":305,"skipped":5696,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:48.767: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6118
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jul  5 17:15:48.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6118" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":306,"skipped":5706,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:48.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2338
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jul  5 17:15:49.104: INFO: Waiting up to 5m0s for pod "client-containers-7cb82bd4-481f-429c-854f-3790cf998967" in namespace "containers-2338" to be "Succeeded or Failed"
Jul  5 17:15:49.110: INFO: Pod "client-containers-7cb82bd4-481f-429c-854f-3790cf998967": Phase="Pending", Reason="", readiness=false. Elapsed: 6.287048ms
Jul  5 17:15:51.116: INFO: Pod "client-containers-7cb82bd4-481f-429c-854f-3790cf998967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012215837s
Jul  5 17:15:53.125: INFO: Pod "client-containers-7cb82bd4-481f-429c-854f-3790cf998967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021273361s
STEP: Saw pod success
Jul  5 17:15:53.126: INFO: Pod "client-containers-7cb82bd4-481f-429c-854f-3790cf998967" satisfied condition "Succeeded or Failed"
Jul  5 17:15:53.131: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod client-containers-7cb82bd4-481f-429c-854f-3790cf998967 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:15:53.158: INFO: Waiting for pod client-containers-7cb82bd4-481f-429c-854f-3790cf998967 to disappear
Jul  5 17:15:53.163: INFO: Pod client-containers-7cb82bd4-481f-429c-854f-3790cf998967 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jul  5 17:15:53.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2338" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5719,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:15:53.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6168
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-6168
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6168 to expose endpoints map[]
Jul  5 17:15:53.351: INFO: successfully validated that service multi-endpoint-test in namespace services-6168 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6168
Jul  5 17:15:53.370: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:15:55.377: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6168 to expose endpoints map[pod1:[100]]
Jul  5 17:15:55.411: INFO: successfully validated that service multi-endpoint-test in namespace services-6168 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6168
Jul  5 17:15:55.428: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:15:57.435: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6168 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  5 17:15:57.468: INFO: successfully validated that service multi-endpoint-test in namespace services-6168 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jul  5 17:15:57.468: INFO: Creating new exec pod
Jul  5 17:16:00.494: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6168 exec execpodgr98m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jul  5 17:16:00.827: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jul  5 17:16:00.827: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:16:00.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6168 exec execpodgr98m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.192.233 80'
Jul  5 17:16:01.185: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 172.26.192.233 80\nConnection to 172.26.192.233 80 port [tcp/http] succeeded!\n"
Jul  5 17:16:01.185: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:16:01.186: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6168 exec execpodgr98m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jul  5 17:16:01.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jul  5 17:16:01.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul  5 17:16:01.603: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6168 exec execpodgr98m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.26.192.233 81'
Jul  5 17:16:01.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.26.192.233 81\nConnection to 172.26.192.233 81 port [tcp/*] succeeded!\n"
Jul  5 17:16:01.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6168
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6168 to expose endpoints map[pod2:[101]]
Jul  5 17:16:02.971: INFO: successfully validated that service multi-endpoint-test in namespace services-6168 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6168
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6168 to expose endpoints map[]
Jul  5 17:16:02.994: INFO: successfully validated that service multi-endpoint-test in namespace services-6168 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:16:03.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6168" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":308,"skipped":5720,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:03.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3712
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jul  5 17:16:03.192: INFO: Found Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul  5 17:16:03.192: INFO: Service test-service-t5cl6 created
STEP: Getting /status
Jul  5 17:16:03.198: INFO: Service test-service-t5cl6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jul  5 17:16:03.210: INFO: observed Service test-service-t5cl6 in namespace services-3712 with annotations: map[] & LoadBalancer: {[]}
Jul  5 17:16:03.210: INFO: Found Service test-service-t5cl6 in namespace services-3712 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul  5 17:16:03.210: INFO: Service test-service-t5cl6 has service status patched
STEP: updating the ServiceStatus
Jul  5 17:16:03.224: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jul  5 17:16:03.229: INFO: Observed Service test-service-t5cl6 in namespace services-3712 with annotations: map[] & Conditions: {[]}
Jul  5 17:16:03.229: INFO: Observed event: &Service{ObjectMeta:{test-service-t5cl6  services-3712  b193ec59-bdb1-4fea-b820-64aefe4b47be 41610 0 2022-07-05 17:16:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-07-05 17:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-07-05 17:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.28.9.152,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.28.9.152],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul  5 17:16:03.229: INFO: Found Service test-service-t5cl6 in namespace services-3712 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul  5 17:16:03.229: INFO: Service test-service-t5cl6 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jul  5 17:16:03.241: INFO: observed Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service-static:true]
Jul  5 17:16:03.241: INFO: observed Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service-static:true]
Jul  5 17:16:03.241: INFO: observed Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service-static:true]
Jul  5 17:16:03.241: INFO: Found Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service:patched test-service-static:true]
Jul  5 17:16:03.241: INFO: Service test-service-t5cl6 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jul  5 17:16:03.256: INFO: Observed event: ADDED
Jul  5 17:16:03.256: INFO: Observed event: MODIFIED
Jul  5 17:16:03.256: INFO: Observed event: MODIFIED
Jul  5 17:16:03.256: INFO: Observed event: MODIFIED
Jul  5 17:16:03.256: INFO: Found Service test-service-t5cl6 in namespace services-3712 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul  5 17:16:03.256: INFO: Service test-service-t5cl6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:16:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3712" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":309,"skipped":5734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:03.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1384
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:16:03.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1384" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":310,"skipped":5747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:03.485: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8505
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:16:03.656: INFO: created pod pod-service-account-defaultsa
Jul  5 17:16:03.656: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  5 17:16:03.666: INFO: created pod pod-service-account-mountsa
Jul  5 17:16:03.666: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  5 17:16:03.674: INFO: created pod pod-service-account-nomountsa
Jul  5 17:16:03.674: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  5 17:16:03.683: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  5 17:16:03.683: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  5 17:16:03.692: INFO: created pod pod-service-account-mountsa-mountspec
Jul  5 17:16:03.692: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  5 17:16:03.701: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  5 17:16:03.701: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  5 17:16:03.710: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  5 17:16:03.710: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  5 17:16:03.718: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  5 17:16:03.718: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  5 17:16:03.726: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  5 17:16:03.726: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 17:16:03.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8505" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":311,"skipped":5783,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:03.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1649
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:16:03.903: INFO: created pod
Jul  5 17:16:03.903: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1649" to be "Succeeded or Failed"
Jul  5 17:16:03.921: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.105145ms
Jul  5 17:16:05.928: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024668417s
Jul  5 17:16:07.936: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032322731s
STEP: Saw pod success
Jul  5 17:16:07.936: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul  5 17:16:37.936: INFO: polling logs
Jul  5 17:16:37.953: INFO: Pod logs: 
I0705 17:16:04.534986       1 log.go:195] OK: Got token
I0705 17:16:04.535014       1 log.go:195] validating with in-cluster discovery
I0705 17:16:04.535374       1 log.go:195] OK: got issuer https://api.tms5g-6sg.it.internal.staging.k8s.ondemand.com
I0705 17:16:04.535412       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tms5g-6sg.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-1649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1657041964, NotBefore:1657041364, IssuedAt:1657041364, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2dd294a3-ef21-4ee3-8c52-b9820e31e552"}}}
I0705 17:16:04.554930       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.tms5g-6sg.it.internal.staging.k8s.ondemand.com
I0705 17:16:04.559577       1 log.go:195] OK: Validated signature on JWT
I0705 17:16:04.559650       1 log.go:195] OK: Got valid claims from token!
I0705 17:16:04.559698       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tms5g-6sg.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-1649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1657041964, NotBefore:1657041364, IssuedAt:1657041364, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2dd294a3-ef21-4ee3-8c52-b9820e31e552"}}}

Jul  5 17:16:37.953: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jul  5 17:16:37.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1649" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":312,"skipped":5801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:37.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9428
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jul  5 17:16:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9428" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":313,"skipped":5829,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:49.191: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8969
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jul  5 17:16:49.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 create -f -'
Jul  5 17:16:50.198: INFO: stderr: ""
Jul  5 17:16:50.198: INFO: stdout: "pod/pause created\n"
Jul  5 17:16:50.198: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  5 17:16:50.198: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8969" to be "running and ready"
Jul  5 17:16:50.204: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.534203ms
Jul  5 17:16:52.211: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012398039s
Jul  5 17:16:52.211: INFO: Pod "pause" satisfied condition "running and ready"
Jul  5 17:16:52.211: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  5 17:16:52.211: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 label pods pause testing-label=testing-label-value'
Jul  5 17:16:52.323: INFO: stderr: ""
Jul  5 17:16:52.323: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  5 17:16:52.323: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 get pod pause -L testing-label'
Jul  5 17:16:52.418: INFO: stderr: ""
Jul  5 17:16:52.418: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  5 17:16:52.418: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 label pods pause testing-label-'
Jul  5 17:16:52.510: INFO: stderr: ""
Jul  5 17:16:52.510: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  5 17:16:52.510: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 get pod pause -L testing-label'
Jul  5 17:16:52.604: INFO: stderr: ""
Jul  5 17:16:52.604: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jul  5 17:16:52.604: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 delete --grace-period=0 --force -f -'
Jul  5 17:16:52.706: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 17:16:52.706: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  5 17:16:52.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 get rc,svc -l name=pause --no-headers'
Jul  5 17:16:52.786: INFO: stderr: "No resources found in kubectl-8969 namespace.\n"
Jul  5 17:16:52.786: INFO: stdout: ""
Jul  5 17:16:52.786: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8969 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 17:16:52.873: INFO: stderr: ""
Jul  5 17:16:52.873: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 17:16:52.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8969" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":314,"skipped":5851,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:52.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1245
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-1245/configmap-test-b929f67f-4348-46d2-b63b-19c422bce096
STEP: Creating a pod to test consume configMaps
Jul  5 17:16:53.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f" in namespace "configmap-1245" to be "Succeeded or Failed"
Jul  5 17:16:53.059: INFO: Pod "pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.322612ms
Jul  5 17:16:55.066: INFO: Pod "pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012617587s
Jul  5 17:16:57.074: INFO: Pod "pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020198164s
STEP: Saw pod success
Jul  5 17:16:57.074: INFO: Pod "pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f" satisfied condition "Succeeded or Failed"
Jul  5 17:16:57.080: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f container env-test: <nil>
STEP: delete the pod
Jul  5 17:16:57.104: INFO: Waiting for pod pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f to disappear
Jul  5 17:16:57.110: INFO: Pod pod-configmaps-624fd366-a8d1-4680-908a-6e6c54e1e89f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:16:57.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1245" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5866,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:16:57.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7584
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  5 17:16:57.297: INFO: Waiting up to 5m0s for pod "pod-19d7343a-0ab8-43c5-bafe-d516910f11b5" in namespace "emptydir-7584" to be "Succeeded or Failed"
Jul  5 17:16:57.304: INFO: Pod "pod-19d7343a-0ab8-43c5-bafe-d516910f11b5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.550594ms
Jul  5 17:16:59.312: INFO: Pod "pod-19d7343a-0ab8-43c5-bafe-d516910f11b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014860151s
Jul  5 17:17:01.318: INFO: Pod "pod-19d7343a-0ab8-43c5-bafe-d516910f11b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021562845s
STEP: Saw pod success
Jul  5 17:17:01.318: INFO: Pod "pod-19d7343a-0ab8-43c5-bafe-d516910f11b5" satisfied condition "Succeeded or Failed"
Jul  5 17:17:01.324: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-19d7343a-0ab8-43c5-bafe-d516910f11b5 container test-container: <nil>
STEP: delete the pod
Jul  5 17:17:01.358: INFO: Waiting for pod pod-19d7343a-0ab8-43c5-bafe-d516910f11b5 to disappear
Jul  5 17:17:01.363: INFO: Pod pod-19d7343a-0ab8-43c5-bafe-d516910f11b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 17:17:01.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7584" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5873,"failed":0}
SSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:01.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-6416
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jul  5 17:17:01.554: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:17:03.561: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.25.206 on the node which pod1 resides and expect scheduled
Jul  5 17:17:03.577: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:17:05.584: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.25.206 but use UDP protocol on the node which pod2 resides
Jul  5 17:17:05.602: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:17:07.609: INFO: The status of Pod pod3 is Running (Ready = true)
Jul  5 17:17:07.625: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:17:09.632: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jul  5 17:17:09.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.25.206 http://127.0.0.1:54323/hostname] Namespace:hostport-6416 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:17:09.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:17:09.638: INFO: ExecWithOptions: Clientset creation
Jul  5 17:17:09.638: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6416/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.25.206+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.25.206, port: 54323
Jul  5 17:17:09.885: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.25.206:54323/hostname] Namespace:hostport-6416 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:17:09.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:17:09.886: INFO: ExecWithOptions: Clientset creation
Jul  5 17:17:09.886: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6416/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.25.206%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.25.206, port: 54323 UDP
Jul  5 17:17:10.161: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.25.206 54323] Namespace:hostport-6416 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul  5 17:17:10.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul  5 17:17:10.162: INFO: ExecWithOptions: Clientset creation
Jul  5 17:17:10.162: INFO: ExecWithOptions: execute(POST https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/hostport-6416/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.250.25.206+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jul  5 17:17:15.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6416" for this suite.
•{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":317,"skipped":5879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:15.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2193
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:17:15.612: INFO: Create a RollingUpdate DaemonSet
Jul  5 17:17:15.619: INFO: Check that daemon pods launch on every node of the cluster
Jul  5 17:17:15.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:15.630: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:16.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:17:16.646: INFO: Node izgw8bazids4c4cxzuus22z is running 0 daemon pod, expected 1
Jul  5 17:17:17.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul  5 17:17:17.647: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jul  5 17:17:17.647: INFO: Update the DaemonSet to trigger a rollout
Jul  5 17:17:17.659: INFO: Updating DaemonSet daemon-set
Jul  5 17:17:19.687: INFO: Roll back the DaemonSet before rollout is complete
Jul  5 17:17:19.700: INFO: Updating DaemonSet daemon-set
Jul  5 17:17:19.700: INFO: Make sure DaemonSet rollback is complete
Jul  5 17:17:19.705: INFO: Wrong image for pod: daemon-set-6dfx8. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jul  5 17:17:19.705: INFO: Pod daemon-set-6dfx8 is not available
Jul  5 17:17:22.719: INFO: Pod daemon-set-964l8 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2193, will wait for the garbage collector to delete the pods
Jul  5 17:17:22.803: INFO: Deleting DaemonSet.extensions daemon-set took: 6.404811ms
Jul  5 17:17:22.904: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.177079ms
Jul  5 17:17:24.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:24.511: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 17:17:24.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42299"},"items":null}

Jul  5 17:17:24.521: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42299"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 17:17:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2193" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":318,"skipped":5893,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:24.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2327
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-3b07d0a9-f535-449e-b046-9d8ce1cb8a9b
STEP: Creating a pod to test consume configMaps
Jul  5 17:17:24.720: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2" in namespace "projected-2327" to be "Succeeded or Failed"
Jul  5 17:17:24.725: INFO: Pod "pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02328ms
Jul  5 17:17:26.732: INFO: Pod "pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011948328s
Jul  5 17:17:28.740: INFO: Pod "pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01953432s
STEP: Saw pod success
Jul  5 17:17:28.740: INFO: Pod "pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2" satisfied condition "Succeeded or Failed"
Jul  5 17:17:28.746: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:17:28.772: INFO: Waiting for pod pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2 to disappear
Jul  5 17:17:28.777: INFO: Pod pod-projected-configmaps-2184d944-25af-4637-b06b-a57cd40515b2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 17:17:28.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2327" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":319,"skipped":5899,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:28.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6496
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:17:28.966: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  5 17:17:28.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:28.977: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jul  5 17:17:29.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:29.011: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:30.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:30.018: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:31.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:17:31.018: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  5 17:17:31.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:17:31.053: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jul  5 17:17:32.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:32.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  5 17:17:32.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:32.073: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:33.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:33.079: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:34.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:34.079: INFO: Node izgw85sex2ooqi4ztetrj0z is running 0 daemon pod, expected 1
Jul  5 17:17:35.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul  5 17:17:35.079: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6496, will wait for the garbage collector to delete the pods
Jul  5 17:17:35.154: INFO: Deleting DaemonSet.extensions daemon-set took: 7.236319ms
Jul  5 17:17:35.255: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.213394ms
Jul  5 17:17:37.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul  5 17:17:37.462: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul  5 17:17:37.468: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42450"},"items":null}

Jul  5 17:17:37.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42450"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jul  5 17:17:37.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6496" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":320,"skipped":5903,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:37.522: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8717
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8717
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8717
STEP: creating replication controller externalsvc in namespace services-8717
I0705 17:17:37.697468    6089 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8717, replica count: 2
I0705 17:17:40.749294    6089 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul  5 17:17:40.772: INFO: Creating new exec pod
Jul  5 17:17:42.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8717 exec execpods5lch -- /bin/sh -x -c nslookup nodeport-service.services-8717.svc.cluster.local'
Jul  5 17:17:43.187: INFO: stderr: "+ nslookup nodeport-service.services-8717.svc.cluster.local\n"
Jul  5 17:17:43.187: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nnodeport-service.services-8717.svc.cluster.local\tcanonical name = externalsvc.services-8717.svc.cluster.local.\nName:\texternalsvc.services-8717.svc.cluster.local\nAddress: 172.26.217.124\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8717, will wait for the garbage collector to delete the pods
Jul  5 17:17:43.251: INFO: Deleting ReplicationController externalsvc took: 7.252862ms
Jul  5 17:17:43.352: INFO: Terminating ReplicationController externalsvc pods took: 100.964208ms
Jul  5 17:17:45.566: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:17:45.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8717" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":321,"skipped":5915,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:45.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9279
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul  5 17:17:45.789: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul  5 17:17:45.800: INFO: starting watch
STEP: patching
STEP: updating
Jul  5 17:17:45.824: INFO: waiting for watch events with expected annotations
Jul  5 17:17:45.824: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jul  5 17:17:45.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9279" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":322,"skipped":5923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:45.870: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4305
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jul  5 17:17:46.032: INFO: Waiting up to 5m0s for pod "client-containers-f3247223-7084-4195-a3b4-6bf56b88df53" in namespace "containers-4305" to be "Succeeded or Failed"
Jul  5 17:17:46.038: INFO: Pod "client-containers-f3247223-7084-4195-a3b4-6bf56b88df53": Phase="Pending", Reason="", readiness=false. Elapsed: 5.955035ms
Jul  5 17:17:48.046: INFO: Pod "client-containers-f3247223-7084-4195-a3b4-6bf56b88df53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014090427s
Jul  5 17:17:50.054: INFO: Pod "client-containers-f3247223-7084-4195-a3b4-6bf56b88df53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021533572s
STEP: Saw pod success
Jul  5 17:17:50.054: INFO: Pod "client-containers-f3247223-7084-4195-a3b4-6bf56b88df53" satisfied condition "Succeeded or Failed"
Jul  5 17:17:50.060: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod client-containers-f3247223-7084-4195-a3b4-6bf56b88df53 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:17:50.130: INFO: Waiting for pod client-containers-f3247223-7084-4195-a3b4-6bf56b88df53 to disappear
Jul  5 17:17:50.135: INFO: Pod client-containers-f3247223-7084-4195-a3b4-6bf56b88df53 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jul  5 17:17:50.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4305" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":5988,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:50.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-788
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 17:17:50.658: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 17:17:53.687: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul  5 17:17:53.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:17:53.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-788" for this suite.
STEP: Destroying namespace "webhook-788-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":324,"skipped":5989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:53.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5484
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul  5 17:17:54.090: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5484 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul  5 17:17:54.176: INFO: stderr: ""
Jul  5 17:17:54.176: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul  5 17:17:54.176: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5484 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jul  5 17:17:54.407: INFO: stderr: ""
Jul  5 17:17:54.407: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul  5 17:17:54.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5484 delete pods e2e-test-httpd-pod'
Jul  5 17:17:55.823: INFO: stderr: ""
Jul  5 17:17:55.823: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 17:17:55.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5484" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":325,"skipped":6045,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:17:55.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3416
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  5 17:17:56.022: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42719 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 17:17:56.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42720 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 17:17:56.022: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42721 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  5 17:18:06.071: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42779 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 17:18:06.071: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42780 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul  5 17:18:06.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3416  a975f8ec-1985-4ff7-9dad-1a5cbe0d6af6 42781 0 2022-07-05 17:17:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-05 17:17:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jul  5 17:18:06.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3416" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":326,"skipped":6053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:06.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1070
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 17:18:06.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4" in namespace "projected-1070" to be "Succeeded or Failed"
Jul  5 17:18:06.255: INFO: Pod "downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617179ms
Jul  5 17:18:08.262: INFO: Pod "downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013092939s
Jul  5 17:18:10.269: INFO: Pod "downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020410747s
STEP: Saw pod success
Jul  5 17:18:10.269: INFO: Pod "downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4" satisfied condition "Succeeded or Failed"
Jul  5 17:18:10.275: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4 container client-container: <nil>
STEP: delete the pod
Jul  5 17:18:10.345: INFO: Waiting for pod downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4 to disappear
Jul  5 17:18:10.351: INFO: Pod downwardapi-volume-5a94998d-f7f3-40f7-a354-54d95c7d9fe4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 17:18:10.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1070" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":327,"skipped":6101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:10.367: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4512
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 17:18:12.700: INFO: DNS probes using dns-4512/dns-test-11be622c-4c05-41fd-9076-b810b93c07c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 17:18:12.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4512" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":328,"skipped":6136,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:12.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-5bbc6f40-a360-4458-bb31-29da273a6965
STEP: Creating a pod to test consume secrets
Jul  5 17:18:12.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91" in namespace "projected-6671" to be "Succeeded or Failed"
Jul  5 17:18:12.906: INFO: Pod "pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.767192ms
Jul  5 17:18:14.913: INFO: Pod "pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016078188s
Jul  5 17:18:16.919: INFO: Pod "pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022389576s
STEP: Saw pod success
Jul  5 17:18:16.919: INFO: Pod "pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91" satisfied condition "Succeeded or Failed"
Jul  5 17:18:16.925: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:18:16.993: INFO: Waiting for pod pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91 to disappear
Jul  5 17:18:16.998: INFO: Pod pod-projected-secrets-a6dbc31e-be75-454c-801a-b3c8690dbd91 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 17:18:16.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6671" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":329,"skipped":6137,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:17.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2368
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jul  5 17:18:17.183: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul  5 17:18:19.267: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jul  5 17:18:21.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2368" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":330,"skipped":6150,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:21.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5332
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul  5 17:18:21.496: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.496: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.496: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.496: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.499: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.499: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.518: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:21.518: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul  5 17:18:22.572: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul  5 17:18:22.572: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul  5 17:18:22.886: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul  5 17:18:22.897: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 0
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.902: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.903: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.903: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.903: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.903: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.904: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.904: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:22.909: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:22.909: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:22.920: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:22.920: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:23.892: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:23.892: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:23.904: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
STEP: listing Deployments
Jul  5 17:18:23.911: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul  5 17:18:23.928: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul  5 17:18:23.940: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:23.940: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:23.940: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:23.944: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:23.947: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:23.951: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:24.901: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:24.907: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:24.914: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:24.921: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul  5 17:18:26.599: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul  5 17:18:26.634: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.634: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.634: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.634: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 1
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 2
Jul  5 17:18:26.635: INFO: observed Deployment test-deployment in namespace deployment-5332 with ReadyReplicas 3
STEP: deleting the Deployment
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.647: INFO: observed event type MODIFIED
Jul  5 17:18:26.648: INFO: observed event type MODIFIED
Jul  5 17:18:26.648: INFO: observed event type MODIFIED
Jul  5 17:18:26.648: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul  5 17:18:26.653: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jul  5 17:18:26.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5332" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":331,"skipped":6151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:26.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2611
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  5 17:18:26.831: INFO: Waiting up to 5m0s for pod "pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8" in namespace "emptydir-2611" to be "Succeeded or Failed"
Jul  5 17:18:26.836: INFO: Pod "pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386817ms
Jul  5 17:18:28.844: INFO: Pod "pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012768603s
Jul  5 17:18:30.851: INFO: Pod "pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02025211s
STEP: Saw pod success
Jul  5 17:18:30.851: INFO: Pod "pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8" satisfied condition "Succeeded or Failed"
Jul  5 17:18:30.857: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8 container test-container: <nil>
STEP: delete the pod
Jul  5 17:18:30.882: INFO: Waiting for pod pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8 to disappear
Jul  5 17:18:30.887: INFO: Pod pod-ba1af74b-1a4f-4f7d-898c-e90a032d32d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 17:18:30.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2611" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":332,"skipped":6174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:30.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3551
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  5 17:18:31.064: INFO: Waiting up to 5m0s for pod "pod-0b9cc755-c69e-492e-9017-024fde0dbfb9" in namespace "emptydir-3551" to be "Succeeded or Failed"
Jul  5 17:18:31.069: INFO: Pod "pod-0b9cc755-c69e-492e-9017-024fde0dbfb9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.197677ms
Jul  5 17:18:33.076: INFO: Pod "pod-0b9cc755-c69e-492e-9017-024fde0dbfb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012030794s
Jul  5 17:18:35.083: INFO: Pod "pod-0b9cc755-c69e-492e-9017-024fde0dbfb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019340621s
STEP: Saw pod success
Jul  5 17:18:35.083: INFO: Pod "pod-0b9cc755-c69e-492e-9017-024fde0dbfb9" satisfied condition "Succeeded or Failed"
Jul  5 17:18:35.089: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-0b9cc755-c69e-492e-9017-024fde0dbfb9 container test-container: <nil>
STEP: delete the pod
Jul  5 17:18:35.115: INFO: Waiting for pod pod-0b9cc755-c69e-492e-9017-024fde0dbfb9 to disappear
Jul  5 17:18:35.120: INFO: Pod pod-0b9cc755-c69e-492e-9017-024fde0dbfb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jul  5 17:18:35.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3551" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":333,"skipped":6235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:35.137: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7987
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 17:18:35.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e" in namespace "projected-7987" to be "Succeeded or Failed"
Jul  5 17:18:35.303: INFO: Pod "downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382653ms
Jul  5 17:18:37.311: INFO: Pod "downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013212402s
Jul  5 17:18:39.318: INFO: Pod "downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020182036s
STEP: Saw pod success
Jul  5 17:18:39.318: INFO: Pod "downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e" satisfied condition "Succeeded or Failed"
Jul  5 17:18:39.324: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e container client-container: <nil>
STEP: delete the pod
Jul  5 17:18:39.349: INFO: Waiting for pod downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e to disappear
Jul  5 17:18:39.354: INFO: Pod downwardapi-volume-214be3ff-1252-4edd-862a-52bd99170e9e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jul  5 17:18:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7987" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":334,"skipped":6290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:39.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8237
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:18:39.521: INFO: Creating ReplicaSet my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0
Jul  5 17:18:39.533: INFO: Pod name my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0: Found 0 pods out of 1
Jul  5 17:18:44.540: INFO: Pod name my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0: Found 1 pods out of 1
Jul  5 17:18:44.540: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0" is running
Jul  5 17:18:44.546: INFO: Pod "my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0-wqtwp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-05 17:18:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-05 17:18:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-05 17:18:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-05 17:18:39 +0000 UTC Reason: Message:}])
Jul  5 17:18:44.546: INFO: Trying to dial the pod
Jul  5 17:18:49.673: INFO: Controller my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0: Got expected result from replica 1 [my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0-wqtwp]: "my-hostname-basic-4283e0ba-1c15-4fb7-98d9-2394b3010aa0-wqtwp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jul  5 17:18:49.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8237" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":335,"skipped":6312,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:49.690: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8386
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jul  5 17:18:49.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8386" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":336,"skipped":6318,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:49.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-9532
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul  5 17:18:50.053: INFO: Waiting up to 5m0s for pod "security-context-016f3732-cd74-47dd-99d4-27e36175863d" in namespace "security-context-9532" to be "Succeeded or Failed"
Jul  5 17:18:50.059: INFO: Pod "security-context-016f3732-cd74-47dd-99d4-27e36175863d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555448ms
Jul  5 17:18:52.065: INFO: Pod "security-context-016f3732-cd74-47dd-99d4-27e36175863d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011848643s
Jul  5 17:18:54.072: INFO: Pod "security-context-016f3732-cd74-47dd-99d4-27e36175863d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018484908s
STEP: Saw pod success
Jul  5 17:18:54.072: INFO: Pod "security-context-016f3732-cd74-47dd-99d4-27e36175863d" satisfied condition "Succeeded or Failed"
Jul  5 17:18:54.077: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod security-context-016f3732-cd74-47dd-99d4-27e36175863d container test-container: <nil>
STEP: delete the pod
Jul  5 17:18:54.102: INFO: Waiting for pod security-context-016f3732-cd74-47dd-99d4-27e36175863d to disappear
Jul  5 17:18:54.107: INFO: Pod security-context-016f3732-cd74-47dd-99d4-27e36175863d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jul  5 17:18:54.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9532" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":337,"skipped":6319,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:18:54.124: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4914
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b in namespace container-probe-4914
Jul  5 17:18:56.296: INFO: Started pod liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b in namespace container-probe-4914
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 17:18:56.302: INFO: Initial restart count of pod liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is 0
Jul  5 17:19:16.379: INFO: Restart count of pod container-probe-4914/liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is now 1 (20.077036822s elapsed)
Jul  5 17:19:36.451: INFO: Restart count of pod container-probe-4914/liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is now 2 (40.148465649s elapsed)
Jul  5 17:19:56.519: INFO: Restart count of pod container-probe-4914/liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is now 3 (1m0.217214623s elapsed)
Jul  5 17:20:16.592: INFO: Restart count of pod container-probe-4914/liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is now 4 (1m20.290332555s elapsed)
Jul  5 17:21:26.854: INFO: Restart count of pod container-probe-4914/liveness-8f2dbde0-799c-4e3c-b991-7de332a9f22b is now 5 (2m30.552039748s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 17:21:26.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4914" for this suite.
•{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:26.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-619
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3728
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5795
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jul  5 17:21:33.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-619" for this suite.
STEP: Destroying namespace "nsdeletetest-3728" for this suite.
Jul  5 17:21:33.345: INFO: Namespace nsdeletetest-3728 was already deleted
STEP: Destroying namespace "nsdeletetest-5795" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":339,"skipped":6401,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:33.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1905
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul  5 17:21:33.523: INFO: The status of Pod pod-update-7bbc66cc-b746-4463-841a-5252482eee3e is Pending, waiting for it to be Running (with Ready = true)
Jul  5 17:21:35.531: INFO: The status of Pod pod-update-7bbc66cc-b746-4463-841a-5252482eee3e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  5 17:21:36.059: INFO: Successfully updated pod "pod-update-7bbc66cc-b746-4463-841a-5252482eee3e"
STEP: verifying the updated pod is in kubernetes
Jul  5 17:21:36.070: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 17:21:36.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1905" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":340,"skipped":6419,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:36.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8817
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul  5 17:21:36.253: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jul  5 17:21:40.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8817" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6424,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:40.299: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2857
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-8dda8b19-55cc-4da1-9f1f-509f9ab37d2f
STEP: Creating a pod to test consume configMaps
Jul  5 17:21:40.465: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042" in namespace "projected-2857" to be "Succeeded or Failed"
Jul  5 17:21:40.471: INFO: Pod "pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042": Phase="Pending", Reason="", readiness=false. Elapsed: 5.585432ms
Jul  5 17:21:42.479: INFO: Pod "pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013586756s
Jul  5 17:21:44.486: INFO: Pod "pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020942097s
STEP: Saw pod success
Jul  5 17:21:44.486: INFO: Pod "pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042" satisfied condition "Succeeded or Failed"
Jul  5 17:21:44.492: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042 container agnhost-container: <nil>
STEP: delete the pod
Jul  5 17:21:44.517: INFO: Waiting for pod pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042 to disappear
Jul  5 17:21:44.522: INFO: Pod pod-projected-configmaps-d57b629d-ec21-4e2d-935c-2be91da64042 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jul  5 17:21:44.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2857" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":342,"skipped":6441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:44.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4105
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jul  5 17:21:44.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4105" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":343,"skipped":6474,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:44.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul  5 17:21:44.942: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jul  5 17:21:44.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2374" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":344,"skipped":6478,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:44.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1120
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jul  5 17:21:45.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e" in namespace "downward-api-1120" to be "Succeeded or Failed"
Jul  5 17:21:45.139: INFO: Pod "downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.60441ms
Jul  5 17:21:47.145: INFO: Pod "downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012110401s
Jul  5 17:21:49.152: INFO: Pod "downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018899457s
STEP: Saw pod success
Jul  5 17:21:49.152: INFO: Pod "downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e" satisfied condition "Succeeded or Failed"
Jul  5 17:21:49.158: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e container client-container: <nil>
STEP: delete the pod
Jul  5 17:21:49.182: INFO: Waiting for pod downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e to disappear
Jul  5 17:21:49.188: INFO: Pod downwardapi-volume-acf4daba-7faf-49aa-b412-e129657cf12e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jul  5 17:21:49.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1120" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":345,"skipped":6479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:21:49.204: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5028
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-73795b49-8995-465a-97f9-481bada75348 in namespace container-probe-5028
Jul  5 17:21:51.378: INFO: Started pod busybox-73795b49-8995-465a-97f9-481bada75348 in namespace container-probe-5028
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 17:21:51.384: INFO: Initial restart count of pod busybox-73795b49-8995-465a-97f9-481bada75348 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jul  5 17:25:52.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5028" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6502,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:25:52.280: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-4766
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-4766-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jul  5 17:25:52.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4766" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":347,"skipped":6504,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:25:52.466: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8507
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-b26ab8d4-f8cf-43da-99f4-35947f51fe0b
STEP: Creating a pod to test consume configMaps
Jul  5 17:25:52.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc" in namespace "configmap-8507" to be "Succeeded or Failed"
Jul  5 17:25:52.646: INFO: Pod "pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.76517ms
Jul  5 17:25:54.653: INFO: Pod "pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013237892s
Jul  5 17:25:56.660: INFO: Pod "pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020326031s
STEP: Saw pod success
Jul  5 17:25:56.660: INFO: Pod "pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc" satisfied condition "Succeeded or Failed"
Jul  5 17:25:56.666: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 17:25:56.700: INFO: Waiting for pod pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc to disappear
Jul  5 17:25:56.705: INFO: Pod pod-configmaps-a89e6eb3-6fd3-4ad9-bd26-a9dd2f835ffc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jul  5 17:25:56.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":348,"skipped":6522,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:25:56.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1692
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul  5 17:25:57.667: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul  5 17:26:00.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:26:11.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1692" for this suite.
STEP: Destroying namespace "webhook-1692-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":349,"skipped":6533,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:26:11.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1092
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-c58f1e52-2b95-442d-9df0-6bf33d86c183
STEP: Creating a pod to test consume secrets
Jul  5 17:26:11.369: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9" in namespace "projected-1092" to be "Succeeded or Failed"
Jul  5 17:26:11.374: INFO: Pod "pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.443189ms
Jul  5 17:26:13.382: INFO: Pod "pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01293299s
Jul  5 17:26:15.389: INFO: Pod "pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020044784s
STEP: Saw pod success
Jul  5 17:26:15.389: INFO: Pod "pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9" satisfied condition "Succeeded or Failed"
Jul  5 17:26:15.394: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 17:26:15.419: INFO: Waiting for pod pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9 to disappear
Jul  5 17:26:15.424: INFO: Pod pod-projected-secrets-6cc72d5a-34ef-45ee-9963-3fcbfc069ed9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jul  5 17:26:15.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1092" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":350,"skipped":6535,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:26:15.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1690
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 17:26:17.764: INFO: DNS probes using dns-test-29ef7caf-fdd4-4d47-ac36-4a1d33a48066 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 17:26:19.880: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:19.896: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:19.896: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:24.956: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:25.013: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:25.013: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:29.908: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:29.957: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains '' instead of 'bar.example.com.'
Jul  5 17:26:29.957: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:34.908: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:34.920: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:34.920: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:39.909: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:39.921: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:39.921: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:44.909: INFO: File wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:44.921: INFO: File jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local from pod  dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 17:26:44.921: INFO: Lookups using dns-1690/dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 failed for: [wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local]

Jul  5 17:26:49.918: INFO: DNS probes using dns-test-6afba9e8-6f7c-4b30-969a-9e789e1c1102 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1690.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1690.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 17:26:52.064: INFO: DNS probes using dns-test-8172197e-b843-4fa1-b118-33d9d2a18062 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jul  5 17:26:52.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1690" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":351,"skipped":6544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:26:52.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5105
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jul  5 17:26:52.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5105 create -f -'
Jul  5 17:26:52.595: INFO: stderr: ""
Jul  5 17:26:52.595: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul  5 17:26:52.595: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5105 diff -f -'
Jul  5 17:26:52.949: INFO: rc: 1
Jul  5 17:26:52.949: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5105 delete -f -'
Jul  5 17:26:53.075: INFO: stderr: ""
Jul  5 17:26:53.075: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 17:26:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5105" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":352,"skipped":6568,"failed":0}

------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:26:53.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4990
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jul  5 17:27:20.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4990" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":353,"skipped":6568,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:27:20.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1899
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul  5 17:27:20.764: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1899 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul  5 17:27:20.874: INFO: stderr: ""
Jul  5 17:27:20.874: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul  5 17:27:25.925: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1899 get pod e2e-test-httpd-pod -o json'
Jul  5 17:27:26.044: INFO: stderr: ""
Jul  5 17:27:26.044: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"d0636bc17c055f8ae48cbfe550b42a48677feba942955642e4f79b705bb26004\",\n            \"cni.projectcalico.org/podIP\": \"172.16.1.14/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.1.14/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2022-07-05T17:27:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1899\",\n        \"resourceVersion\": \"46204\",\n        \"uid\": \"32232bd6-8f17-4859-9c62-fb48bf275296\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tms5g-6sg.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nnxtf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"izgw8bazids4c4cxzuus22z\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nnxtf\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-05T17:27:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-05T17:27:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-05T17:27:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-05T17:27:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://eae769b964313254ca22600b092c82be8397e8e5f95e3ff0e770d0f8797b5754\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-07-05T17:27:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.25.207\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.1.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.1.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-07-05T17:27:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  5 17:27:26.044: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1899 replace -f -'
Jul  5 17:27:26.292: INFO: stderr: ""
Jul  5 17:27:26.292: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jul  5 17:27:26.297: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1899 delete pods e2e-test-httpd-pod'
Jul  5 17:27:27.939: INFO: stderr: ""
Jul  5 17:27:27.939: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jul  5 17:27:27.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1899" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":354,"skipped":6586,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:27:27.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7642
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jul  5 17:27:28.115: INFO: Waiting up to 5m0s for pod "var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b" in namespace "var-expansion-7642" to be "Succeeded or Failed"
Jul  5 17:27:28.121: INFO: Pod "var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337575ms
Jul  5 17:27:30.136: INFO: Pod "var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02068972s
Jul  5 17:27:32.149: INFO: Pod "var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033999339s
STEP: Saw pod success
Jul  5 17:27:32.150: INFO: Pod "var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b" satisfied condition "Succeeded or Failed"
Jul  5 17:27:32.156: INFO: Trying to get logs from node izgw8bazids4c4cxzuus22z pod var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b container dapi-container: <nil>
STEP: delete the pod
Jul  5 17:27:32.182: INFO: Waiting for pod var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b to disappear
Jul  5 17:27:32.187: INFO: Pod var-expansion-6747489c-5877-4f3b-bde2-bf95cdc0ae9b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jul  5 17:27:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7642" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":355,"skipped":6596,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jul  5 17:27:32.204: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6355
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jul  5 17:27:32.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jul  5 17:27:35.266: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-6355 --namespace=crd-publish-openapi-6355 create -f -'
Jul  5 17:27:36.704: INFO: stderr: ""
Jul  5 17:27:36.704: INFO: stdout: "e2e-test-crd-publish-openapi-3370-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul  5 17:27:36.704: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-6355 --namespace=crd-publish-openapi-6355 delete e2e-test-crd-publish-openapi-3370-crds test-cr'
Jul  5 17:27:36.893: INFO: stderr: ""
Jul  5 17:27:36.893: INFO: stdout: "e2e-test-crd-publish-openapi-3370-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul  5 17:27:36.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-6355 --namespace=crd-publish-openapi-6355 apply -f -'
Jul  5 17:27:37.231: INFO: stderr: ""
Jul  5 17:27:37.231: INFO: stdout: "e2e-test-crd-publish-openapi-3370-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul  5 17:27:37.231: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-6355 --namespace=crd-publish-openapi-6355 delete e2e-test-crd-publish-openapi-3370-crds test-cr'
Jul  5 17:27:37.339: INFO: stderr: ""
Jul  5 17:27:37.339: INFO: stdout: "e2e-test-crd-publish-openapi-3370-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul  5 17:27:37.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tms5g-6sg.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-6355 explain e2e-test-crd-publish-openapi-3370-crds'
Jul  5 17:27:37.600: INFO: stderr: ""
Jul  5 17:27:37.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3370-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jul  5 17:27:41.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6355" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":356,"skipped":6610,"failed":0}
SSSSSJul  5 17:27:41.589: INFO: Running AfterSuite actions on all nodes
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jul  5 17:27:41.589: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jul  5 17:27:41.589: INFO: Running AfterSuite actions on node 1
Jul  5 17:27:41.589: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/e2e/artifacts/1657036604/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6615,"failed":0}

Ran 356 of 6971 Specs in 5453.905 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Flaked | 0 Pending | 6615 Skipped
PASS

Ginkgo ran 1 suite in 1h30m56.15916243s
Test Suite Passed
