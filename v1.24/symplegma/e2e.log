I0611 13:53:36.553898      19 e2e.go:129] Starting e2e run "904d3f1f-cad0-493e-932b-892432cc0485" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1654955616 - Will randomize all specs
Will run 356 of 6965 specs

Jun 11 13:53:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
E0611 13:53:38.794956      19 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 11 13:53:38.795: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 11 13:53:38.817: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 11 13:53:38.851: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 11 13:53:38.851: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun 11 13:53:38.851: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 11 13:53:38.859: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 11 13:53:38.859: INFO: e2e test version: v1.24.1
Jun 11 13:53:38.862: INFO: kube-apiserver version: v1.24.1
Jun 11 13:53:38.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 13:53:38.868: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:53:38.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename init-container
Jun 11 13:53:38.912: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
W0611 13:53:38.912044      19 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jun 11 13:53:38.929: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jun 11 13:53:38.938: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 13:53:47.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7588" for this suite.

â€¢ [SLOW TEST:8.495 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":1,"skipped":27,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:53:47.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jun 11 13:53:47.403: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 11 13:53:47.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:49.185: INFO: stderr: ""
Jun 11 13:53:49.185: INFO: stdout: "service/agnhost-replica created\n"
Jun 11 13:53:49.185: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 11 13:53:49.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:50.652: INFO: stderr: ""
Jun 11 13:53:50.652: INFO: stdout: "service/agnhost-primary created\n"
Jun 11 13:53:50.652: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 11 13:53:50.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:52.112: INFO: stderr: ""
Jun 11 13:53:52.112: INFO: stdout: "service/frontend created\n"
Jun 11 13:53:52.112: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 11 13:53:52.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:52.385: INFO: stderr: ""
Jun 11 13:53:52.385: INFO: stdout: "deployment.apps/frontend created\n"
Jun 11 13:53:52.385: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 11 13:53:52.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:52.658: INFO: stderr: ""
Jun 11 13:53:52.658: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 11 13:53:52.658: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.36
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 11 13:53:52.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 create -f -'
Jun 11 13:53:53.009: INFO: stderr: ""
Jun 11 13:53:53.009: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 11 13:53:53.009: INFO: Waiting for all frontend pods to be Running.
Jun 11 13:54:03.061: INFO: Waiting for frontend to serve content.
Jun 11 13:54:03.087: INFO: Trying to add a new entry to the guestbook.
Jun 11 13:54:03.106: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 11 13:54:03.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:03.257: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:03.257: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 13:54:03.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:03.463: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:03.463: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 13:54:03.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:03.598: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:03.598: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 13:54:03.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:03.712: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:03.713: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 13:54:03.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:03.886: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:03.886: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 11 13:54:03.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3294 delete --grace-period=0 --force -f -'
Jun 11 13:54:04.023: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 13:54:04.023: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 13:54:04.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3294" for this suite.

â€¢ [SLOW TEST:16.685 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":2,"skipped":40,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:04.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 13:54:08.172: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jun 11 13:54:08.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1099" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":3,"skipped":54,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:08.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-2aae3598-e930-47d3-ba43-bd4473c9843a
STEP: Creating a pod to test consume configMaps
Jun 11 13:54:08.268: INFO: Waiting up to 5m0s for pod "pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6" in namespace "configmap-6885" to be "Succeeded or Failed"
Jun 11 13:54:08.273: INFO: Pod "pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.347796ms
Jun 11 13:54:10.280: INFO: Pod "pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011766501s
Jun 11 13:54:12.289: INFO: Pod "pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020483405s
STEP: Saw pod success
Jun 11 13:54:12.289: INFO: Pod "pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6" satisfied condition "Succeeded or Failed"
Jun 11 13:54:12.294: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 13:54:12.345: INFO: Waiting for pod pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6 to disappear
Jun 11 13:54:12.349: INFO: Pod pod-configmaps-63692315-b58b-45bb-9ea5-abd9f7e19ac6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 13:54:12.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6885" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":54,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:12.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 11 13:54:12.419: INFO: The status of Pod pod-update-7ef4a8d9-35e5-4362-ba71-00484f565f2e is Pending, waiting for it to be Running (with Ready = true)
Jun 11 13:54:14.431: INFO: The status of Pod pod-update-7ef4a8d9-35e5-4362-ba71-00484f565f2e is Pending, waiting for it to be Running (with Ready = true)
Jun 11 13:54:16.425: INFO: The status of Pod pod-update-7ef4a8d9-35e5-4362-ba71-00484f565f2e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 11 13:54:16.951: INFO: Successfully updated pod "pod-update-7ef4a8d9-35e5-4362-ba71-00484f565f2e"
STEP: verifying the updated pod is in kubernetes
Jun 11 13:54:16.961: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 13:54:16.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-838" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:16.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jun 11 13:54:28.182: INFO: 69 pods remaining
Jun 11 13:54:28.192: INFO: 69 pods has nil DeletionTimestamp
Jun 11 13:54:28.192: INFO: 
STEP: Gathering metrics
Jun 11 13:54:33.214: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 13:54:33.358: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 11 13:54:33.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-27psj" in namespace "gc-6059"
Jun 11 13:54:33.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qsbj" in namespace "gc-6059"
Jun 11 13:54:33.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-2skz8" in namespace "gc-6059"
Jun 11 13:54:33.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-2sxjx" in namespace "gc-6059"
Jun 11 13:54:33.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-2x5wf" in namespace "gc-6059"
Jun 11 13:54:33.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-4579v" in namespace "gc-6059"
Jun 11 13:54:33.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nbkr" in namespace "gc-6059"
Jun 11 13:54:33.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nfw4" in namespace "gc-6059"
Jun 11 13:54:33.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vbf6" in namespace "gc-6059"
Jun 11 13:54:33.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-52mrs" in namespace "gc-6059"
Jun 11 13:54:33.578: INFO: Deleting pod "simpletest-rc-to-be-deleted-56s26" in namespace "gc-6059"
Jun 11 13:54:33.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-56ww2" in namespace "gc-6059"
Jun 11 13:54:33.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-57lzc" in namespace "gc-6059"
Jun 11 13:54:33.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-59hzg" in namespace "gc-6059"
Jun 11 13:54:33.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gb5r" in namespace "gc-6059"
Jun 11 13:54:33.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k7vq" in namespace "gc-6059"
Jun 11 13:54:33.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-66tvd" in namespace "gc-6059"
Jun 11 13:54:33.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-67zvc" in namespace "gc-6059"
Jun 11 13:54:33.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dqnk" in namespace "gc-6059"
Jun 11 13:54:33.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kdjs" in namespace "gc-6059"
Jun 11 13:54:33.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lrvf" in namespace "gc-6059"
Jun 11 13:54:33.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n7d5" in namespace "gc-6059"
Jun 11 13:54:33.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pmxp" in namespace "gc-6059"
Jun 11 13:54:34.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-772rw" in namespace "gc-6059"
Jun 11 13:54:34.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fdqf" in namespace "gc-6059"
Jun 11 13:54:34.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tdvj" in namespace "gc-6059"
Jun 11 13:54:34.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-97h9v" in namespace "gc-6059"
Jun 11 13:54:34.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kbtg" in namespace "gc-6059"
Jun 11 13:54:34.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nr8w" in namespace "gc-6059"
Jun 11 13:54:34.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sjf4" in namespace "gc-6059"
Jun 11 13:54:34.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z4xv" in namespace "gc-6059"
Jun 11 13:54:34.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-blmfk" in namespace "gc-6059"
Jun 11 13:54:34.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmjvf" in namespace "gc-6059"
Jun 11 13:54:34.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2kg2" in namespace "gc-6059"
Jun 11 13:54:34.300: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7jfr" in namespace "gc-6059"
Jun 11 13:54:34.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccgb9" in namespace "gc-6059"
Jun 11 13:54:34.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-chdlb" in namespace "gc-6059"
Jun 11 13:54:34.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvnsm" in namespace "gc-6059"
Jun 11 13:54:34.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4q6x" in namespace "gc-6059"
Jun 11 13:54:34.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4q9p" in namespace "gc-6059"
Jun 11 13:54:34.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvzwn" in namespace "gc-6059"
Jun 11 13:54:34.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbgwq" in namespace "gc-6059"
Jun 11 13:54:34.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm52b" in namespace "gc-6059"
Jun 11 13:54:34.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp56n" in namespace "gc-6059"
Jun 11 13:54:34.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpkzs" in namespace "gc-6059"
Jun 11 13:54:34.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvzp6" in namespace "gc-6059"
Jun 11 13:54:34.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzgx2" in namespace "gc-6059"
Jun 11 13:54:34.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn886" in namespace "gc-6059"
Jun 11 13:54:34.632: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs5jm" in namespace "gc-6059"
Jun 11 13:54:34.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz559" in namespace "gc-6059"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 13:54:34.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6059" for this suite.

â€¢ [SLOW TEST:17.728 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":6,"skipped":122,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:34.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jun 11 13:54:34.809: INFO: Waiting up to 5m0s for pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88" in namespace "downward-api-6860" to be "Succeeded or Failed"
Jun 11 13:54:34.815: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Pending", Reason="", readiness=false. Elapsed: 5.158828ms
Jun 11 13:54:36.820: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010804823s
Jun 11 13:54:38.825: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015793299s
Jun 11 13:54:40.836: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02640448s
Jun 11 13:54:42.842: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032890499s
Jun 11 13:54:44.849: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Running", Reason="", readiness=true. Elapsed: 10.039810774s
Jun 11 13:54:46.858: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.048720236s
STEP: Saw pod success
Jun 11 13:54:46.858: INFO: Pod "downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88" satisfied condition "Succeeded or Failed"
Jun 11 13:54:46.863: INFO: Trying to get logs from node ip-172-31-41-158 pod downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88 container dapi-container: <nil>
STEP: delete the pod
Jun 11 13:54:46.893: INFO: Waiting for pod downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88 to disappear
Jun 11 13:54:46.897: INFO: Pod downward-api-76bedfe0-7fa9-4f0a-b7c7-5bee96bbad88 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jun 11 13:54:46.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6860" for this suite.

â€¢ [SLOW TEST:12.200 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":7,"skipped":126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:46.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jun 11 13:54:46.971: INFO: Waiting up to 5m0s for pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed" in namespace "containers-7519" to be "Succeeded or Failed"
Jun 11 13:54:46.978: INFO: Pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030613ms
Jun 11 13:54:48.987: INFO: Pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015706563s
Jun 11 13:54:50.994: INFO: Pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed": Phase="Running", Reason="", readiness=true. Elapsed: 4.022149325s
Jun 11 13:54:53.003: INFO: Pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031830747s
STEP: Saw pod success
Jun 11 13:54:53.004: INFO: Pod "client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed" satisfied condition "Succeeded or Failed"
Jun 11 13:54:53.009: INFO: Trying to get logs from node ip-172-31-41-158 pod client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed container agnhost-container: <nil>
STEP: delete the pod
Jun 11 13:54:53.032: INFO: Waiting for pod client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed to disappear
Jun 11 13:54:53.038: INFO: Pod client-containers-1b2882c7-b95f-4134-8555-7834e8c2f8ed no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jun 11 13:54:53.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7519" for this suite.

â€¢ [SLOW TEST:6.144 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":8,"skipped":156,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:53.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 11 13:54:53.158: INFO: starting watch
STEP: patching
STEP: updating
Jun 11 13:54:53.181: INFO: waiting for watch events with expected annotations
Jun 11 13:54:53.181: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jun 11 13:54:53.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8804" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":9,"skipped":162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:53.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 11 13:54:53.343: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 13:54:57.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2441" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":10,"skipped":191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:57.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jun 11 13:54:57.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1532" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":11,"skipped":241,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:54:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 13:54:57.716: INFO: The status of Pod server-envvars-ebea67d2-1483-4eab-9ae7-f29645374329 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 13:54:59.727: INFO: The status of Pod server-envvars-ebea67d2-1483-4eab-9ae7-f29645374329 is Running (Ready = true)
Jun 11 13:54:59.770: INFO: Waiting up to 5m0s for pod "client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22" in namespace "pods-1507" to be "Succeeded or Failed"
Jun 11 13:54:59.783: INFO: Pod "client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22": Phase="Pending", Reason="", readiness=false. Elapsed: 13.298503ms
Jun 11 13:55:01.792: INFO: Pod "client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022125123s
Jun 11 13:55:03.804: INFO: Pod "client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033865402s
STEP: Saw pod success
Jun 11 13:55:03.804: INFO: Pod "client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22" satisfied condition "Succeeded or Failed"
Jun 11 13:55:03.809: INFO: Trying to get logs from node ip-172-31-41-158 pod client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22 container env3cont: <nil>
STEP: delete the pod
Jun 11 13:55:03.840: INFO: Waiting for pod client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22 to disappear
Jun 11 13:55:03.847: INFO: Pod client-envvars-e2763f80-0bfd-427b-a530-d09d02f5fc22 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 13:55:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1507" for this suite.

â€¢ [SLOW TEST:6.213 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":12,"skipped":247,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:55:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 13:55:03.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12" in namespace "downward-api-2431" to be "Succeeded or Failed"
Jun 11 13:55:03.926: INFO: Pod "downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985621ms
Jun 11 13:55:05.936: INFO: Pod "downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013683801s
Jun 11 13:55:07.942: INFO: Pod "downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019894408s
STEP: Saw pod success
Jun 11 13:55:07.942: INFO: Pod "downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12" satisfied condition "Succeeded or Failed"
Jun 11 13:55:07.949: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12 container client-container: <nil>
STEP: delete the pod
Jun 11 13:55:07.993: INFO: Waiting for pod downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12 to disappear
Jun 11 13:55:07.999: INFO: Pod downwardapi-volume-726a8b1c-2b0e-42c7-88b7-ed740549bf12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 13:55:07.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2431" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":13,"skipped":262,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:55:08.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 13:55:08.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7643" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":14,"skipped":269,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:55:08.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-ff8n
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 13:55:08.250: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ff8n" in namespace "subpath-4509" to be "Succeeded or Failed"
Jun 11 13:55:08.258: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106302ms
Jun 11 13:55:10.268: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 2.018249477s
Jun 11 13:55:12.275: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 4.024883142s
Jun 11 13:55:14.288: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 6.037803751s
Jun 11 13:55:16.303: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 8.053222796s
Jun 11 13:55:18.309: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 10.058402929s
Jun 11 13:55:20.314: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 12.063828717s
Jun 11 13:55:22.319: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 14.068865629s
Jun 11 13:55:24.327: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 16.076611523s
Jun 11 13:55:26.333: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 18.083209439s
Jun 11 13:55:28.344: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=true. Elapsed: 20.094096549s
Jun 11 13:55:30.365: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Running", Reason="", readiness=false. Elapsed: 22.114624586s
Jun 11 13:55:32.376: INFO: Pod "pod-subpath-test-configmap-ff8n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.125590186s
STEP: Saw pod success
Jun 11 13:55:32.376: INFO: Pod "pod-subpath-test-configmap-ff8n" satisfied condition "Succeeded or Failed"
Jun 11 13:55:32.382: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-subpath-test-configmap-ff8n container test-container-subpath-configmap-ff8n: <nil>
STEP: delete the pod
Jun 11 13:55:32.410: INFO: Waiting for pod pod-subpath-test-configmap-ff8n to disappear
Jun 11 13:55:32.417: INFO: Pod pod-subpath-test-configmap-ff8n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ff8n
Jun 11 13:55:32.417: INFO: Deleting pod "pod-subpath-test-configmap-ff8n" in namespace "subpath-4509"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jun 11 13:55:32.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4509" for this suite.

â€¢ [SLOW TEST:24.277 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":15,"skipped":272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:55:32.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-efa39ba2-879b-455f-be8d-c7e8e30bda39 in namespace container-probe-7109
Jun 11 13:55:34.545: INFO: Started pod test-webserver-efa39ba2-879b-455f-be8d-c7e8e30bda39 in namespace container-probe-7109
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 13:55:34.550: INFO: Initial restart count of pod test-webserver-efa39ba2-879b-455f-be8d-c7e8e30bda39 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 13:59:35.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7109" for this suite.

â€¢ [SLOW TEST:243.259 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":296,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:59:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 13:59:35.796: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:35.796: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:35.797: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:35.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:35.803: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:36.810: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:36.810: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:36.810: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:36.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:36.815: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:37.811: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:37.811: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:37.811: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:37.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:37.815: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:38.814: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:38.814: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:38.814: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:38.820: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:38.820: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:39.810: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:39.810: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:39.810: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:39.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:39.815: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:40.812: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:40.813: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:40.813: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:40.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:40.822: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:41.810: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.810: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.810: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 13:59:41.814: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 11 13:59:41.839: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.840: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.840: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:41.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 13:59:41.844: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:42.859: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:42.859: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:42.859: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:42.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 13:59:42.864: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:43.851: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:43.851: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:43.851: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:43.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 13:59:43.856: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:44.855: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:44.855: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:44.855: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:44.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 13:59:44.860: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:45.853: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:45.854: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:45.854: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:45.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 13:59:45.858: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:46.851: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:46.851: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:46.851: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:46.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 13:59:46.856: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5395, will wait for the garbage collector to delete the pods
Jun 11 13:59:46.927: INFO: Deleting DaemonSet.extensions daemon-set took: 11.412242ms
Jun 11 13:59:47.028: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.497558ms
Jun 11 13:59:50.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:50.438: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 13:59:50.444: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7878"},"items":null}

Jun 11 13:59:50.449: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7878"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 13:59:50.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5395" for this suite.

â€¢ [SLOW TEST:14.786 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":17,"skipped":307,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:59:50.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun 11 13:59:50.557: INFO: Waiting up to 5m0s for pod "security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366" in namespace "security-context-8486" to be "Succeeded or Failed"
Jun 11 13:59:50.567: INFO: Pod "security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366": Phase="Pending", Reason="", readiness=false. Elapsed: 9.161378ms
Jun 11 13:59:52.573: INFO: Pod "security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015875463s
Jun 11 13:59:54.581: INFO: Pod "security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023304279s
STEP: Saw pod success
Jun 11 13:59:54.581: INFO: Pod "security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366" satisfied condition "Succeeded or Failed"
Jun 11 13:59:54.585: INFO: Trying to get logs from node ip-172-31-41-158 pod security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366 container test-container: <nil>
STEP: delete the pod
Jun 11 13:59:54.621: INFO: Waiting for pod security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366 to disappear
Jun 11 13:59:54.625: INFO: Pod security-context-0451b6b8-4030-4ea0-8f00-84a68bf9a366 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 13:59:54.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8486" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":18,"skipped":323,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:59:54.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 13:59:54.738: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:54.738: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:54.738: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:54.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:54.744: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:55.751: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:55.752: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:55.752: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:55.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:55.757: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 13:59:56.756: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:56.756: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:56.756: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 13:59:56.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 13:59:56.762: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status
Jun 11 13:59:56.771: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jun 11 13:59:56.787: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jun 11 13:59:56.792: INFO: Observed &DaemonSet event: ADDED
Jun 11 13:59:56.792: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.792: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.793: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.793: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.793: INFO: Found daemon set daemon-set in namespace daemonsets-2113 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 11 13:59:56.793: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jun 11 13:59:56.806: INFO: Observed &DaemonSet event: ADDED
Jun 11 13:59:56.806: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.807: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.807: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.807: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.807: INFO: Observed daemon set daemon-set in namespace daemonsets-2113 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 11 13:59:56.808: INFO: Observed &DaemonSet event: MODIFIED
Jun 11 13:59:56.808: INFO: Found daemon set daemon-set in namespace daemonsets-2113 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 11 13:59:56.808: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2113, will wait for the garbage collector to delete the pods
Jun 11 13:59:56.877: INFO: Deleting DaemonSet.extensions daemon-set took: 9.137259ms
Jun 11 13:59:56.977: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.599438ms
Jun 11 13:59:59.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 13:59:59.388: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 13:59:59.393: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8007"},"items":null}

Jun 11 13:59:59.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8007"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 13:59:59.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2113" for this suite.
â€¢{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":19,"skipped":327,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 13:59:59.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 13:59:59.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 11 13:59:59.499: INFO: The status of Pod pod-logs-websocket-74f6e162-7d0e-41d7-b825-4453f6017019 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:00:01.515: INFO: The status of Pod pod-logs-websocket-74f6e162-7d0e-41d7-b825-4453f6017019 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 14:00:01.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6927" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":20,"skipped":348,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:00:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jun 11 14:00:01.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:00:30.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-618" for this suite.

â€¢ [SLOW TEST:29.110 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":21,"skipped":354,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:00:30.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 14:00:30.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5380" for this suite.
â€¢{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":22,"skipped":371,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:00:30.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-1616/configmap-test-6d57e5f3-1734-4998-9d87-8e3cbfd2af99
STEP: Creating a pod to test consume configMaps
Jun 11 14:00:30.820: INFO: Waiting up to 5m0s for pod "pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7" in namespace "configmap-1616" to be "Succeeded or Failed"
Jun 11 14:00:30.824: INFO: Pod "pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.249304ms
Jun 11 14:00:32.831: INFO: Pod "pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010714619s
Jun 11 14:00:34.839: INFO: Pod "pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018408446s
STEP: Saw pod success
Jun 11 14:00:34.839: INFO: Pod "pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7" satisfied condition "Succeeded or Failed"
Jun 11 14:00:34.843: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7 container env-test: <nil>
STEP: delete the pod
Jun 11 14:00:34.879: INFO: Waiting for pod pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7 to disappear
Jun 11 14:00:34.886: INFO: Pod pod-configmaps-54f53f82-4df6-4461-bc62-7798cf6b85e7 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:00:34.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1616" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:00:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:00:34.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Creating first CR 
Jun 11 14:00:37.551: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:00:37Z]] name:name1 resourceVersion:8231 uid:de26ea7d-abf6-42fd-a12f-46c5a40f7990] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 11 14:00:47.571: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:00:47Z]] name:name2 resourceVersion:8272 uid:a40fe86a-8bf0-461f-91ee-69678e409300] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 11 14:00:57.584: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:00:57Z]] name:name1 resourceVersion:8301 uid:de26ea7d-abf6-42fd-a12f-46c5a40f7990] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 11 14:01:07.595: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:01:07Z]] name:name2 resourceVersion:8331 uid:a40fe86a-8bf0-461f-91ee-69678e409300] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 11 14:01:17.606: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:00:57Z]] name:name1 resourceVersion:8360 uid:de26ea7d-abf6-42fd-a12f-46c5a40f7990] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 11 14:01:27.617: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-11T14:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-11T14:01:07Z]] name:name2 resourceVersion:8389 uid:a40fe86a-8bf0-461f-91ee-69678e409300] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:01:38.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2066" for this suite.

â€¢ [SLOW TEST:63.255 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":24,"skipped":415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:01:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:01:38.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a" in namespace "downward-api-1311" to be "Succeeded or Failed"
Jun 11 14:01:38.228: INFO: Pod "downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034526ms
Jun 11 14:01:40.237: INFO: Pod "downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017922659s
Jun 11 14:01:42.243: INFO: Pod "downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024060286s
STEP: Saw pod success
Jun 11 14:01:42.244: INFO: Pod "downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a" satisfied condition "Succeeded or Failed"
Jun 11 14:01:42.248: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a container client-container: <nil>
STEP: delete the pod
Jun 11 14:01:42.274: INFO: Waiting for pod downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a to disappear
Jun 11 14:01:42.278: INFO: Pod downwardapi-volume-940a5383-1969-4336-83e0-0087edebd21a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:01:42.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1311" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":453,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:01:42.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-2138/configmap-test-68b404ea-0511-4247-ab1b-eff9438f4f49
STEP: Creating a pod to test consume configMaps
Jun 11 14:01:42.360: INFO: Waiting up to 5m0s for pod "pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274" in namespace "configmap-2138" to be "Succeeded or Failed"
Jun 11 14:01:42.364: INFO: Pod "pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044331ms
Jun 11 14:01:44.372: INFO: Pod "pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011357059s
Jun 11 14:01:46.379: INFO: Pod "pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018571618s
STEP: Saw pod success
Jun 11 14:01:46.379: INFO: Pod "pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274" satisfied condition "Succeeded or Failed"
Jun 11 14:01:46.384: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274 container env-test: <nil>
STEP: delete the pod
Jun 11 14:01:46.410: INFO: Waiting for pod pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274 to disappear
Jun 11 14:01:46.416: INFO: Pod pod-configmaps-10ee3759-b372-467b-b3cb-6e13c22b8274 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:01:46.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2138" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":26,"skipped":453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:01:46.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:01:47.088: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:01:50.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:01:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1326-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:01:53.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4778" for this suite.
STEP: Destroying namespace "webhook-4778-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:6.973 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":27,"skipped":526,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:01:53.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:01:53.529: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 11 14:01:53.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:53.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jun 11 14:01:53.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:53.582: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:01:54.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:54.589: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:01:55.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 14:01:55.588: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 11 14:01:55.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 14:01:55.616: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun 11 14:01:56.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:56.624: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 11 14:01:56.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:56.642: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:01:57.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:57.650: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:01:58.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:58.649: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:01:59.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:01:59.663: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:02:00.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 14:02:00.649: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6731, will wait for the garbage collector to delete the pods
Jun 11 14:02:00.733: INFO: Deleting DaemonSet.extensions daemon-set took: 8.881884ms
Jun 11 14:02:00.833: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.460627ms
Jun 11 14:02:02.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:02:02.746: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 14:02:02.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8704"},"items":null}

Jun 11 14:02:02.763: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8704"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:02:02.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6731" for this suite.

â€¢ [SLOW TEST:9.393 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":28,"skipped":547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun 11 14:02:02.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-138 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 11 14:02:02.948: INFO: stderr: ""
Jun 11 14:02:02.948: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 11 14:02:02.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-138 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jun 11 14:02:04.470: INFO: stderr: ""
Jun 11 14:02:04.470: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun 11 14:02:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-138 delete pods e2e-test-httpd-pod'
Jun 11 14:02:06.724: INFO: stderr: ""
Jun 11 14:02:06.724: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:02:06.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-138" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":29,"skipped":573,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:06.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:02:06.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jun 11 14:02:13.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-3091 --namespace=crd-publish-openapi-3091 create -f -'
Jun 11 14:02:14.626: INFO: stderr: ""
Jun 11 14:02:14.626: INFO: stdout: "e2e-test-crd-publish-openapi-2891-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 11 14:02:14.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-3091 --namespace=crd-publish-openapi-3091 delete e2e-test-crd-publish-openapi-2891-crds test-cr'
Jun 11 14:02:14.751: INFO: stderr: ""
Jun 11 14:02:14.751: INFO: stdout: "e2e-test-crd-publish-openapi-2891-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 11 14:02:14.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-3091 --namespace=crd-publish-openapi-3091 apply -f -'
Jun 11 14:02:15.843: INFO: stderr: ""
Jun 11 14:02:15.843: INFO: stdout: "e2e-test-crd-publish-openapi-2891-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 11 14:02:15.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-3091 --namespace=crd-publish-openapi-3091 delete e2e-test-crd-publish-openapi-2891-crds test-cr'
Jun 11 14:02:15.924: INFO: stderr: ""
Jun 11 14:02:15.924: INFO: stdout: "e2e-test-crd-publish-openapi-2891-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 11 14:02:15.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-3091 explain e2e-test-crd-publish-openapi-2891-crds'
Jun 11 14:02:16.156: INFO: stderr: ""
Jun 11 14:02:16.156: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2891-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:02:20.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3091" for this suite.

â€¢ [SLOW TEST:13.415 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":30,"skipped":582,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:20.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jun 11 14:02:20.217: INFO: Waiting up to 5m0s for pod "var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25" in namespace "var-expansion-3544" to be "Succeeded or Failed"
Jun 11 14:02:20.223: INFO: Pod "var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.853054ms
Jun 11 14:02:22.232: INFO: Pod "var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015073297s
Jun 11 14:02:24.240: INFO: Pod "var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023196382s
STEP: Saw pod success
Jun 11 14:02:24.240: INFO: Pod "var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25" satisfied condition "Succeeded or Failed"
Jun 11 14:02:24.244: INFO: Trying to get logs from node ip-172-31-41-158 pod var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25 container dapi-container: <nil>
STEP: delete the pod
Jun 11 14:02:24.270: INFO: Waiting for pod var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25 to disappear
Jun 11 14:02:24.273: INFO: Pod var-expansion-d568d819-428f-4c37-ad9d-a451171d8e25 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 14:02:24.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3544" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":586,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:24.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:02:24.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9064" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":32,"skipped":595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:24.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jun 11 14:02:24.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1822" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":619,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-xzt7
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 14:02:24.609: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xzt7" in namespace "subpath-2863" to be "Succeeded or Failed"
Jun 11 14:02:24.615: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.201323ms
Jun 11 14:02:26.622: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012997311s
Jun 11 14:02:28.628: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 4.019221472s
Jun 11 14:02:30.635: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 6.025621052s
Jun 11 14:02:32.640: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031169424s
Jun 11 14:02:34.646: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 10.03720725s
Jun 11 14:02:36.652: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 12.042726684s
Jun 11 14:02:38.657: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 14.047877518s
Jun 11 14:02:40.666: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 16.056529628s
Jun 11 14:02:42.673: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 18.064031973s
Jun 11 14:02:44.684: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=true. Elapsed: 20.074535204s
Jun 11 14:02:46.694: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Running", Reason="", readiness=false. Elapsed: 22.084770356s
Jun 11 14:02:48.704: INFO: Pod "pod-subpath-test-secret-xzt7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.095017529s
STEP: Saw pod success
Jun 11 14:02:48.704: INFO: Pod "pod-subpath-test-secret-xzt7" satisfied condition "Succeeded or Failed"
Jun 11 14:02:48.708: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-subpath-test-secret-xzt7 container test-container-subpath-secret-xzt7: <nil>
STEP: delete the pod
Jun 11 14:02:48.734: INFO: Waiting for pod pod-subpath-test-secret-xzt7 to disappear
Jun 11 14:02:48.738: INFO: Pod pod-subpath-test-secret-xzt7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-xzt7
Jun 11 14:02:48.738: INFO: Deleting pod "pod-subpath-test-secret-xzt7" in namespace "subpath-2863"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jun 11 14:02:48.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2863" for this suite.

â€¢ [SLOW TEST:24.234 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":34,"skipped":625,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:48.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:02:48.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:02:52.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6380" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":35,"skipped":642,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:02:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 11 14:03:32.237: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 14:03:32.339: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 11 14:03:32.339: INFO: Deleting pod "simpletest.rc-28cd9" in namespace "gc-2488"
Jun 11 14:03:32.355: INFO: Deleting pod "simpletest.rc-2b9sr" in namespace "gc-2488"
Jun 11 14:03:32.378: INFO: Deleting pod "simpletest.rc-2htpz" in namespace "gc-2488"
Jun 11 14:03:32.400: INFO: Deleting pod "simpletest.rc-2m79s" in namespace "gc-2488"
Jun 11 14:03:32.414: INFO: Deleting pod "simpletest.rc-2n55r" in namespace "gc-2488"
Jun 11 14:03:32.442: INFO: Deleting pod "simpletest.rc-2rt5g" in namespace "gc-2488"
Jun 11 14:03:32.481: INFO: Deleting pod "simpletest.rc-2wjf9" in namespace "gc-2488"
Jun 11 14:03:32.498: INFO: Deleting pod "simpletest.rc-2x8ll" in namespace "gc-2488"
Jun 11 14:03:32.518: INFO: Deleting pod "simpletest.rc-44q2d" in namespace "gc-2488"
Jun 11 14:03:32.549: INFO: Deleting pod "simpletest.rc-4b9br" in namespace "gc-2488"
Jun 11 14:03:32.570: INFO: Deleting pod "simpletest.rc-4sv6r" in namespace "gc-2488"
Jun 11 14:03:32.584: INFO: Deleting pod "simpletest.rc-4tck7" in namespace "gc-2488"
Jun 11 14:03:32.603: INFO: Deleting pod "simpletest.rc-5s882" in namespace "gc-2488"
Jun 11 14:03:32.707: INFO: Deleting pod "simpletest.rc-5tsmt" in namespace "gc-2488"
Jun 11 14:03:32.741: INFO: Deleting pod "simpletest.rc-6k4k6" in namespace "gc-2488"
Jun 11 14:03:32.755: INFO: Deleting pod "simpletest.rc-6tqth" in namespace "gc-2488"
Jun 11 14:03:32.772: INFO: Deleting pod "simpletest.rc-79pr4" in namespace "gc-2488"
Jun 11 14:03:32.831: INFO: Deleting pod "simpletest.rc-7cpfm" in namespace "gc-2488"
Jun 11 14:03:32.864: INFO: Deleting pod "simpletest.rc-7pcfr" in namespace "gc-2488"
Jun 11 14:03:32.897: INFO: Deleting pod "simpletest.rc-82gd4" in namespace "gc-2488"
Jun 11 14:03:32.927: INFO: Deleting pod "simpletest.rc-8gqff" in namespace "gc-2488"
Jun 11 14:03:32.943: INFO: Deleting pod "simpletest.rc-8lxdx" in namespace "gc-2488"
Jun 11 14:03:33.000: INFO: Deleting pod "simpletest.rc-98zdv" in namespace "gc-2488"
Jun 11 14:03:33.013: INFO: Deleting pod "simpletest.rc-9nd4d" in namespace "gc-2488"
Jun 11 14:03:33.034: INFO: Deleting pod "simpletest.rc-9ndqv" in namespace "gc-2488"
Jun 11 14:03:33.059: INFO: Deleting pod "simpletest.rc-9nh4p" in namespace "gc-2488"
Jun 11 14:03:33.081: INFO: Deleting pod "simpletest.rc-bfq6n" in namespace "gc-2488"
Jun 11 14:03:33.099: INFO: Deleting pod "simpletest.rc-bjdxp" in namespace "gc-2488"
Jun 11 14:03:33.114: INFO: Deleting pod "simpletest.rc-c49f8" in namespace "gc-2488"
Jun 11 14:03:33.129: INFO: Deleting pod "simpletest.rc-c7b98" in namespace "gc-2488"
Jun 11 14:03:33.141: INFO: Deleting pod "simpletest.rc-c7zhq" in namespace "gc-2488"
Jun 11 14:03:33.158: INFO: Deleting pod "simpletest.rc-ccbfx" in namespace "gc-2488"
Jun 11 14:03:33.174: INFO: Deleting pod "simpletest.rc-chhgn" in namespace "gc-2488"
Jun 11 14:03:33.191: INFO: Deleting pod "simpletest.rc-ckk7h" in namespace "gc-2488"
Jun 11 14:03:33.215: INFO: Deleting pod "simpletest.rc-cmrck" in namespace "gc-2488"
Jun 11 14:03:33.244: INFO: Deleting pod "simpletest.rc-d97rb" in namespace "gc-2488"
Jun 11 14:03:33.260: INFO: Deleting pod "simpletest.rc-dd94k" in namespace "gc-2488"
Jun 11 14:03:33.282: INFO: Deleting pod "simpletest.rc-dn8mq" in namespace "gc-2488"
Jun 11 14:03:33.298: INFO: Deleting pod "simpletest.rc-dnhsp" in namespace "gc-2488"
Jun 11 14:03:33.318: INFO: Deleting pod "simpletest.rc-dr2rw" in namespace "gc-2488"
Jun 11 14:03:33.336: INFO: Deleting pod "simpletest.rc-dxfs5" in namespace "gc-2488"
Jun 11 14:03:33.359: INFO: Deleting pod "simpletest.rc-fcxk6" in namespace "gc-2488"
Jun 11 14:03:33.385: INFO: Deleting pod "simpletest.rc-fdp64" in namespace "gc-2488"
Jun 11 14:03:33.400: INFO: Deleting pod "simpletest.rc-fl56f" in namespace "gc-2488"
Jun 11 14:03:33.419: INFO: Deleting pod "simpletest.rc-fnc9n" in namespace "gc-2488"
Jun 11 14:03:33.440: INFO: Deleting pod "simpletest.rc-fwmwv" in namespace "gc-2488"
Jun 11 14:03:33.459: INFO: Deleting pod "simpletest.rc-gkg26" in namespace "gc-2488"
Jun 11 14:03:33.476: INFO: Deleting pod "simpletest.rc-gp4xv" in namespace "gc-2488"
Jun 11 14:03:33.488: INFO: Deleting pod "simpletest.rc-hmpnf" in namespace "gc-2488"
Jun 11 14:03:33.504: INFO: Deleting pod "simpletest.rc-jqs9m" in namespace "gc-2488"
Jun 11 14:03:33.519: INFO: Deleting pod "simpletest.rc-jw6l7" in namespace "gc-2488"
Jun 11 14:03:33.537: INFO: Deleting pod "simpletest.rc-k424n" in namespace "gc-2488"
Jun 11 14:03:33.549: INFO: Deleting pod "simpletest.rc-k4gdj" in namespace "gc-2488"
Jun 11 14:03:33.569: INFO: Deleting pod "simpletest.rc-k6rld" in namespace "gc-2488"
Jun 11 14:03:33.582: INFO: Deleting pod "simpletest.rc-k8df7" in namespace "gc-2488"
Jun 11 14:03:33.602: INFO: Deleting pod "simpletest.rc-k9ffz" in namespace "gc-2488"
Jun 11 14:03:33.617: INFO: Deleting pod "simpletest.rc-kdmbv" in namespace "gc-2488"
Jun 11 14:03:33.635: INFO: Deleting pod "simpletest.rc-kw4lg" in namespace "gc-2488"
Jun 11 14:03:33.650: INFO: Deleting pod "simpletest.rc-lpxdc" in namespace "gc-2488"
Jun 11 14:03:33.680: INFO: Deleting pod "simpletest.rc-lsjt7" in namespace "gc-2488"
Jun 11 14:03:33.696: INFO: Deleting pod "simpletest.rc-m7s5z" in namespace "gc-2488"
Jun 11 14:03:33.719: INFO: Deleting pod "simpletest.rc-mf5ht" in namespace "gc-2488"
Jun 11 14:03:33.742: INFO: Deleting pod "simpletest.rc-mkdpj" in namespace "gc-2488"
Jun 11 14:03:33.763: INFO: Deleting pod "simpletest.rc-mqcs2" in namespace "gc-2488"
Jun 11 14:03:33.783: INFO: Deleting pod "simpletest.rc-mzn6n" in namespace "gc-2488"
Jun 11 14:03:33.799: INFO: Deleting pod "simpletest.rc-n5h7g" in namespace "gc-2488"
Jun 11 14:03:33.812: INFO: Deleting pod "simpletest.rc-n64sb" in namespace "gc-2488"
Jun 11 14:03:33.828: INFO: Deleting pod "simpletest.rc-nkvgs" in namespace "gc-2488"
Jun 11 14:03:33.842: INFO: Deleting pod "simpletest.rc-nm8sd" in namespace "gc-2488"
Jun 11 14:03:33.856: INFO: Deleting pod "simpletest.rc-nndw9" in namespace "gc-2488"
Jun 11 14:03:33.879: INFO: Deleting pod "simpletest.rc-nntjf" in namespace "gc-2488"
Jun 11 14:03:33.895: INFO: Deleting pod "simpletest.rc-nqhf8" in namespace "gc-2488"
Jun 11 14:03:33.912: INFO: Deleting pod "simpletest.rc-p582z" in namespace "gc-2488"
Jun 11 14:03:33.934: INFO: Deleting pod "simpletest.rc-pt8cx" in namespace "gc-2488"
Jun 11 14:03:33.956: INFO: Deleting pod "simpletest.rc-qgvj5" in namespace "gc-2488"
Jun 11 14:03:33.969: INFO: Deleting pod "simpletest.rc-r2rk8" in namespace "gc-2488"
Jun 11 14:03:33.983: INFO: Deleting pod "simpletest.rc-rcqgn" in namespace "gc-2488"
Jun 11 14:03:33.998: INFO: Deleting pod "simpletest.rc-rvc9w" in namespace "gc-2488"
Jun 11 14:03:34.019: INFO: Deleting pod "simpletest.rc-s9h69" in namespace "gc-2488"
Jun 11 14:03:34.038: INFO: Deleting pod "simpletest.rc-sdwr6" in namespace "gc-2488"
Jun 11 14:03:34.056: INFO: Deleting pod "simpletest.rc-sqjc2" in namespace "gc-2488"
Jun 11 14:03:34.074: INFO: Deleting pod "simpletest.rc-szkgp" in namespace "gc-2488"
Jun 11 14:03:34.088: INFO: Deleting pod "simpletest.rc-szzdx" in namespace "gc-2488"
Jun 11 14:03:34.106: INFO: Deleting pod "simpletest.rc-t89zb" in namespace "gc-2488"
Jun 11 14:03:34.140: INFO: Deleting pod "simpletest.rc-tmwkk" in namespace "gc-2488"
Jun 11 14:03:34.156: INFO: Deleting pod "simpletest.rc-v2t7z" in namespace "gc-2488"
Jun 11 14:03:34.174: INFO: Deleting pod "simpletest.rc-vd7bx" in namespace "gc-2488"
Jun 11 14:03:34.209: INFO: Deleting pod "simpletest.rc-vgtp9" in namespace "gc-2488"
Jun 11 14:03:34.256: INFO: Deleting pod "simpletest.rc-vwfhd" in namespace "gc-2488"
Jun 11 14:03:34.315: INFO: Deleting pod "simpletest.rc-wdzfr" in namespace "gc-2488"
Jun 11 14:03:34.360: INFO: Deleting pod "simpletest.rc-wk2l5" in namespace "gc-2488"
Jun 11 14:03:34.423: INFO: Deleting pod "simpletest.rc-wks4m" in namespace "gc-2488"
Jun 11 14:03:34.460: INFO: Deleting pod "simpletest.rc-x7rqg" in namespace "gc-2488"
Jun 11 14:03:34.504: INFO: Deleting pod "simpletest.rc-xrsvw" in namespace "gc-2488"
Jun 11 14:03:34.564: INFO: Deleting pod "simpletest.rc-xwbpt" in namespace "gc-2488"
Jun 11 14:03:34.614: INFO: Deleting pod "simpletest.rc-xwpgc" in namespace "gc-2488"
Jun 11 14:03:34.657: INFO: Deleting pod "simpletest.rc-z6b9w" in namespace "gc-2488"
Jun 11 14:03:34.707: INFO: Deleting pod "simpletest.rc-z9vcc" in namespace "gc-2488"
Jun 11 14:03:34.756: INFO: Deleting pod "simpletest.rc-zf7hw" in namespace "gc-2488"
Jun 11 14:03:34.807: INFO: Deleting pod "simpletest.rc-zljk4" in namespace "gc-2488"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 14:03:34.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2488" for this suite.

â€¢ [SLOW TEST:42.910 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":36,"skipped":648,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:03:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jun 11 14:03:35.011: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:37.017: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:39.017: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:41.016: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:43.023: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:45.019: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:47.016: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jun 11 14:03:47.032: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:49.041: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:51.040: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:53.040: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:55.043: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:03:57.041: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 11 14:03:57.065: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 11 14:03:57.071: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 11 14:03:59.072: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 11 14:03:59.077: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jun 11 14:03:59.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7999" for this suite.

â€¢ [SLOW TEST:24.138 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":649,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:03:59.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-8c95bff2-1696-4a16-b171-739eeefad124
STEP: Creating a pod to test consume configMaps
Jun 11 14:03:59.152: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e" in namespace "configmap-2950" to be "Succeeded or Failed"
Jun 11 14:03:59.157: INFO: Pod "pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119592ms
Jun 11 14:04:01.164: INFO: Pod "pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e": Phase="Running", Reason="", readiness=false. Elapsed: 2.011453779s
Jun 11 14:04:03.170: INFO: Pod "pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017748319s
STEP: Saw pod success
Jun 11 14:04:03.171: INFO: Pod "pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e" satisfied condition "Succeeded or Failed"
Jun 11 14:04:03.174: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:04:03.195: INFO: Waiting for pod pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e to disappear
Jun 11 14:04:03.199: INFO: Pod pod-configmaps-5b1d75a2-a8b0-495f-9c52-418baad2d39e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:04:03.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2950" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:03.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:04:03.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588" in namespace "projected-275" to be "Succeeded or Failed"
Jun 11 14:04:03.282: INFO: Pod "downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588": Phase="Pending", Reason="", readiness=false. Elapsed: 5.96522ms
Jun 11 14:04:05.292: INFO: Pod "downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016120209s
Jun 11 14:04:07.299: INFO: Pod "downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023590179s
STEP: Saw pod success
Jun 11 14:04:07.300: INFO: Pod "downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588" satisfied condition "Succeeded or Failed"
Jun 11 14:04:07.304: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588 container client-container: <nil>
STEP: delete the pod
Jun 11 14:04:07.326: INFO: Waiting for pod downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588 to disappear
Jun 11 14:04:07.331: INFO: Pod downwardapi-volume-96032013-b1a2-4ee3-860a-9174aa773588 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:04:07.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-275" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":39,"skipped":686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:07.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-112bc8a1-a5b8-4171-9f22-beb0db18365e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:04:09.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1175" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":40,"skipped":753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:09.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 11 14:04:09.495: INFO: Waiting up to 5m0s for pod "pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30" in namespace "emptydir-5762" to be "Succeeded or Failed"
Jun 11 14:04:09.499: INFO: Pod "pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640006ms
Jun 11 14:04:11.509: INFO: Pod "pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013347506s
Jun 11 14:04:13.515: INFO: Pod "pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019572782s
STEP: Saw pod success
Jun 11 14:04:13.515: INFO: Pod "pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30" satisfied condition "Succeeded or Failed"
Jun 11 14:04:13.519: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30 container test-container: <nil>
STEP: delete the pod
Jun 11 14:04:13.539: INFO: Waiting for pod pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30 to disappear
Jun 11 14:04:13.543: INFO: Pod pod-0277cd43-6e7d-4b31-9e7e-173e98ccdd30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:04:13.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5762" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":41,"skipped":794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:13.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jun 11 14:04:13.594: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-9153 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:04:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9153" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":42,"skipped":861,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:13.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:04:13.747: INFO: created pod
Jun 11 14:04:13.747: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1717" to be "Succeeded or Failed"
Jun 11 14:04:13.752: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135148ms
Jun 11 14:04:15.760: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013226162s
Jun 11 14:04:17.770: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02280159s
STEP: Saw pod success
Jun 11 14:04:17.770: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 11 14:04:47.771: INFO: polling logs
Jun 11 14:04:47.780: INFO: Pod logs: 
I0611 14:04:14.658637       1 log.go:195] OK: Got token
I0611 14:04:14.658693       1 log.go:195] validating with in-cluster discovery
I0611 14:04:14.659009       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0611 14:04:14.659044       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1717:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654956854, NotBefore:1654956254, IssuedAt:1654956254, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1717", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cf1d229-99a7-4dfc-8470-5ecbfe36dad2"}}}
I0611 14:04:14.719227       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0611 14:04:14.733338       1 log.go:195] OK: Validated signature on JWT
I0611 14:04:14.733588       1 log.go:195] OK: Got valid claims from token!
I0611 14:04:14.733698       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1717:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654956854, NotBefore:1654956254, IssuedAt:1654956254, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1717", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cf1d229-99a7-4dfc-8470-5ecbfe36dad2"}}}

Jun 11 14:04:47.780: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 14:04:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1717" for this suite.

â€¢ [SLOW TEST:34.107 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":43,"skipped":861,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:47.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun 11 14:04:49.900: INFO: running pods: 0 < 3
Jun 11 14:04:51.906: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jun 11 14:04:53.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3259" for this suite.

â€¢ [SLOW TEST:6.119 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":44,"skipped":868,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:04:53.990: INFO: created pod pod-service-account-defaultsa
Jun 11 14:04:53.990: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 11 14:04:54.000: INFO: created pod pod-service-account-mountsa
Jun 11 14:04:54.000: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 11 14:04:54.008: INFO: created pod pod-service-account-nomountsa
Jun 11 14:04:54.009: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 11 14:04:54.017: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 11 14:04:54.017: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 11 14:04:54.029: INFO: created pod pod-service-account-mountsa-mountspec
Jun 11 14:04:54.029: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 11 14:04:54.036: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 11 14:04:54.037: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 11 14:04:54.043: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 11 14:04:54.043: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 11 14:04:54.052: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 11 14:04:54.053: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 11 14:04:54.062: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 11 14:04:54.062: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 14:04:54.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7453" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":45,"skipped":879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:54.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun 11 14:04:54.250: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun 11 14:04:54.300: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jun 11 14:04:54.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5387" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":46,"skipped":910,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:04:54.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:04:54.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba" in namespace "downward-api-8752" to be "Succeeded or Failed"
Jun 11 14:04:54.426: INFO: Pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.639156ms
Jun 11 14:04:56.436: INFO: Pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017357953s
Jun 11 14:04:58.445: INFO: Pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026455237s
Jun 11 14:05:00.451: INFO: Pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031772816s
STEP: Saw pod success
Jun 11 14:05:00.451: INFO: Pod "downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba" satisfied condition "Succeeded or Failed"
Jun 11 14:05:00.456: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba container client-container: <nil>
STEP: delete the pod
Jun 11 14:05:00.483: INFO: Waiting for pod downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba to disappear
Jun 11 14:05:00.487: INFO: Pod downwardapi-volume-a1742205-99da-4ddb-b321-7b40800aa1ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:05:00.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8752" for this suite.

â€¢ [SLOW TEST:6.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:05:00.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4859
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-4859
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4859
Jun 11 14:05:00.577: INFO: Found 0 stateful pods, waiting for 1
Jun 11 14:05:10.584: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 11 14:05:10.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:05:10.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:05:10.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:05:10.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 14:05:10.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 11 14:05:20.825: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 14:05:20.825: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 14:05:20.857: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 11 14:05:20.859: INFO: ss-0  ip-172-31-27-214  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:00 +0000 UTC  }]
Jun 11 14:05:20.860: INFO: 
Jun 11 14:05:20.861: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 11 14:05:21.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985061176s
Jun 11 14:05:22.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979906218s
Jun 11 14:05:23.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97064928s
Jun 11 14:05:24.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962797704s
Jun 11 14:05:25.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954131036s
Jun 11 14:05:26.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.945557768s
Jun 11 14:05:27.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938213639s
Jun 11 14:05:28.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932925691s
Jun 11 14:05:29.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.232479ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4859
Jun 11 14:05:30.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 14:05:31.134: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 14:05:31.134: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 14:05:31.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 14:05:31.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 14:05:31.319: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 11 14:05:31.319: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 14:05:31.319: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 14:05:31.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 14:05:31.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 11 14:05:31.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 14:05:31.539: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 14:05:31.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 11 14:05:41.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:05:41.559: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:05:41.559: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 11 14:05:41.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:05:41.790: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:05:41.790: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:05:41.790: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 14:05:41.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:05:41.988: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:05:41.988: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:05:41.988: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 14:05:41.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-4859 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:05:42.167: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:05:42.167: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:05:42.167: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 14:05:42.167: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 14:05:42.172: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 11 14:05:52.182: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 14:05:52.182: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 14:05:52.182: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 14:05:52.199: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 11 14:05:52.199: INFO: ss-0  ip-172-31-27-214  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:00 +0000 UTC  }]
Jun 11 14:05:52.199: INFO: ss-1  ip-172-31-41-158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  }]
Jun 11 14:05:52.199: INFO: ss-2  ip-172-31-41-158  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  }]
Jun 11 14:05:52.199: INFO: 
Jun 11 14:05:52.199: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 11 14:05:53.205: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 11 14:05:53.205: INFO: ss-1  ip-172-31-41-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  }]
Jun 11 14:05:53.205: INFO: ss-2  ip-172-31-41-158  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 14:05:20 +0000 UTC  }]
Jun 11 14:05:53.205: INFO: 
Jun 11 14:05:53.205: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 11 14:05:54.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988094514s
Jun 11 14:05:55.216: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982078104s
Jun 11 14:05:56.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.976657105s
Jun 11 14:05:57.228: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.970415135s
Jun 11 14:05:58.235: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964347977s
Jun 11 14:05:59.245: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.957638849s
Jun 11 14:06:00.251: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.948041088s
Jun 11 14:06:01.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 941.620331ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4859
Jun 11 14:06:02.269: INFO: Scaling statefulset ss to 0
Jun 11 14:06:02.282: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 14:06:02.289: INFO: Deleting all statefulset in ns statefulset-4859
Jun 11 14:06:02.295: INFO: Scaling statefulset ss to 0
Jun 11 14:06:02.308: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 14:06:02.311: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 14:06:02.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4859" for this suite.

â€¢ [SLOW TEST:61.836 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":48,"skipped":946,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:02.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:06:04.399: INFO: Deleting pod "var-expansion-200dc3a8-6b53-49d0-9809-1027624e7568" in namespace "var-expansion-3537"
Jun 11 14:06:04.408: INFO: Wait up to 5m0s for pod "var-expansion-200dc3a8-6b53-49d0-9809-1027624e7568" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 14:06:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3537" for this suite.

â€¢ [SLOW TEST:6.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":49,"skipped":954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:08.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 14:06:12.522: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jun 11 14:06:12.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2380" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":50,"skipped":978,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:12.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jun 11 14:06:12.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 create -f -'
Jun 11 14:06:14.383: INFO: stderr: ""
Jun 11 14:06:14.383: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 14:06:14.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 14:06:14.505: INFO: stderr: ""
Jun 11 14:06:14.505: INFO: stdout: "update-demo-nautilus-26qzc update-demo-nautilus-lzf4g "
Jun 11 14:06:14.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods update-demo-nautilus-26qzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 14:06:14.609: INFO: stderr: ""
Jun 11 14:06:14.609: INFO: stdout: ""
Jun 11 14:06:14.609: INFO: update-demo-nautilus-26qzc is created but not running
Jun 11 14:06:19.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 14:06:19.695: INFO: stderr: ""
Jun 11 14:06:19.695: INFO: stdout: "update-demo-nautilus-26qzc update-demo-nautilus-lzf4g "
Jun 11 14:06:19.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods update-demo-nautilus-26qzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 14:06:19.786: INFO: stderr: ""
Jun 11 14:06:19.786: INFO: stdout: "true"
Jun 11 14:06:19.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods update-demo-nautilus-26qzc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 14:06:19.878: INFO: stderr: ""
Jun 11 14:06:19.878: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 14:06:19.878: INFO: validating pod update-demo-nautilus-26qzc
Jun 11 14:06:19.887: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 14:06:19.888: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 14:06:19.888: INFO: update-demo-nautilus-26qzc is verified up and running
Jun 11 14:06:19.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods update-demo-nautilus-lzf4g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 14:06:19.976: INFO: stderr: ""
Jun 11 14:06:19.976: INFO: stdout: "true"
Jun 11 14:06:19.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods update-demo-nautilus-lzf4g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 14:06:20.064: INFO: stderr: ""
Jun 11 14:06:20.064: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 14:06:20.064: INFO: validating pod update-demo-nautilus-lzf4g
Jun 11 14:06:20.071: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 14:06:20.071: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 14:06:20.071: INFO: update-demo-nautilus-lzf4g is verified up and running
STEP: using delete to clean up resources
Jun 11 14:06:20.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 delete --grace-period=0 --force -f -'
Jun 11 14:06:20.174: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 14:06:20.174: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 11 14:06:20.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get rc,svc -l name=update-demo --no-headers'
Jun 11 14:06:20.283: INFO: stderr: "No resources found in kubectl-7701 namespace.\n"
Jun 11 14:06:20.283: INFO: stdout: ""
Jun 11 14:06:20.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-7701 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 14:06:20.372: INFO: stderr: ""
Jun 11 14:06:20.372: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:06:20.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7701" for this suite.

â€¢ [SLOW TEST:7.828 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":51,"skipped":991,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:20.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-807007a4-3ffe-463b-a31c-40d17fd38c40
STEP: Creating a pod to test consume secrets
Jun 11 14:06:20.451: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85" in namespace "projected-32" to be "Succeeded or Failed"
Jun 11 14:06:20.456: INFO: Pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574499ms
Jun 11 14:06:22.462: INFO: Pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85": Phase="Running", Reason="", readiness=true. Elapsed: 2.010504489s
Jun 11 14:06:24.468: INFO: Pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85": Phase="Running", Reason="", readiness=false. Elapsed: 4.016779473s
Jun 11 14:06:26.474: INFO: Pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023054898s
STEP: Saw pod success
Jun 11 14:06:26.474: INFO: Pod "pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85" satisfied condition "Succeeded or Failed"
Jun 11 14:06:26.479: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:06:26.500: INFO: Waiting for pod pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85 to disappear
Jun 11 14:06:26.505: INFO: Pod pod-projected-secrets-f84e8baf-64d3-462b-9a71-101c34880e85 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 14:06:26.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-32" for this suite.

â€¢ [SLOW TEST:6.131 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:26.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-1db4fca1-cfa6-4e10-bd72-18ce20d6074d
STEP: Creating the pod
Jun 11 14:06:26.587: INFO: The status of Pod pod-configmaps-505e43e3-5b45-40c9-83b9-8ed604f78a7c is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:06:28.599: INFO: The status of Pod pod-configmaps-505e43e3-5b45-40c9-83b9-8ed604f78a7c is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-1db4fca1-cfa6-4e10-bd72-18ce20d6074d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:06:30.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9690" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:30.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:06:30.703: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 11 14:06:35.713: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 14:06:35.713: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 11 14:06:37.720: INFO: Creating deployment "test-rollover-deployment"
Jun 11 14:06:37.734: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 11 14:06:39.748: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 11 14:06:39.756: INFO: Ensure that both replica sets have 1 created replica
Jun 11 14:06:39.766: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 11 14:06:39.780: INFO: Updating deployment test-rollover-deployment
Jun 11 14:06:39.780: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 11 14:06:41.791: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 11 14:06:41.802: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 11 14:06:41.810: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 14:06:41.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:43.827: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 14:06:43.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:45.825: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 14:06:45.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:47.820: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 14:06:47.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:49.824: INFO: all replica sets need to contain the pod-template-hash label
Jun 11 14:06:49.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:51.836: INFO: 
Jun 11 14:06:51.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 6, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 6, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77745f886c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:06:53.821: INFO: 
Jun 11 14:06:53.821: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:06:53.832: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7032  877008e7-8da8-41d4-9c4c-c6432008313e 12897 2 2022-06-11 14:06:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-11 14:06:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ebbac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-11 14:06:37 +0000 UTC,LastTransitionTime:2022-06-11 14:06:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77745f886c" has successfully progressed.,LastUpdateTime:2022-06-11 14:06:51 +0000 UTC,LastTransitionTime:2022-06-11 14:06:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 14:06:53.837: INFO: New ReplicaSet "test-rollover-deployment-77745f886c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77745f886c  deployment-7032  d27e2621-3ffb-45c6-ad06-0362164e4a7a 12887 2 2022-06-11 14:06:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 877008e7-8da8-41d4-9c4c-c6432008313e 0xc003071477 0xc003071478}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:06:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877008e7-8da8-41d4-9c4c-c6432008313e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:06:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77745f886c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003071528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:06:53.837: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 11 14:06:53.838: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7032  7a3115d9-0c24-47a1-8a7d-6bdecc2bca53 12896 2 2022-06-11 14:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 877008e7-8da8-41d4-9c4c-c6432008313e 0xc003071347 0xc003071348}] []  [{e2e.test Update apps/v1 2022-06-11 14:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877008e7-8da8-41d4-9c4c-c6432008313e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:06:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003071408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:06:53.838: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-7032  bac783a5-7b52-42b6-be41-ade1c36078f9 12837 2 2022-06-11 14:06:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 877008e7-8da8-41d4-9c4c-c6432008313e 0xc003071590 0xc003071591}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:06:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877008e7-8da8-41d4-9c4c-c6432008313e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:06:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003071638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:06:53.843: INFO: Pod "test-rollover-deployment-77745f886c-ktk27" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77745f886c-ktk27 test-rollover-deployment-77745f886c- deployment-7032  012029bf-4efc-45f9-90b1-bc66f3bf7e22 12854 0 2022-06-11 14:06:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77745f886c] map[cni.projectcalico.org/containerID:1b4151246fe1fe02fcd97275b767c7abe9e1acb970c209d5859304ce72f9e081 cni.projectcalico.org/podIP:192.168.203.23/32 cni.projectcalico.org/podIPs:192.168.203.23/32] [{apps/v1 ReplicaSet test-rollover-deployment-77745f886c d27e2621-3ffb-45c6-ad06-0362164e4a7a 0xc003071b97 0xc003071b98}] []  [{kube-controller-manager Update v1 2022-06-11 14:06:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d27e2621-3ffb-45c6-ad06-0362164e4a7a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:06:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:06:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7rc8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rc8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:06:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:06:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:06:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:06:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.23,StartTime:2022-06-11 14:06:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:06:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8,ContainerID:containerd://7f848775adbfd0bac5dcea2ae57bb53f1d6baec5b3ee8675fb22b7cb8722add9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:06:53.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7032" for this suite.

â€¢ [SLOW TEST:23.207 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":54,"skipped":1074,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:53.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-13c86770-010f-4980-bfa2-e97cf782db4e
STEP: Creating a pod to test consume configMaps
Jun 11 14:06:53.928: INFO: Waiting up to 5m0s for pod "pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0" in namespace "configmap-9850" to be "Succeeded or Failed"
Jun 11 14:06:53.934: INFO: Pod "pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149663ms
Jun 11 14:06:55.948: INFO: Pod "pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019315314s
Jun 11 14:06:57.956: INFO: Pod "pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027899967s
STEP: Saw pod success
Jun 11 14:06:57.957: INFO: Pod "pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0" satisfied condition "Succeeded or Failed"
Jun 11 14:06:57.964: INFO: Trying to get logs from node ip-172-31-27-214 pod pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:06:58.001: INFO: Waiting for pod pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0 to disappear
Jun 11 14:06:58.005: INFO: Pod pod-configmaps-34f901a8-c61a-432a-8c98-cba1519690d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:06:58.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9850" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":1079,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:06:58.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:07:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2936" for this suite.

â€¢ [SLOW TEST:7.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":56,"skipped":1085,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:05.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-a27ff243-e2f0-4f6b-80d9-a32c20e21b05
STEP: Creating configMap with name cm-test-opt-upd-f33b664c-af17-4ca1-adae-86bddd261fcf
STEP: Creating the pod
Jun 11 14:07:05.175: INFO: The status of Pod pod-configmaps-3986e422-2d48-4f28-b260-cfb9d6ffb653 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:07:07.184: INFO: The status of Pod pod-configmaps-3986e422-2d48-4f28-b260-cfb9d6ffb653 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-a27ff243-e2f0-4f6b-80d9-a32c20e21b05
STEP: Updating configmap cm-test-opt-upd-f33b664c-af17-4ca1-adae-86bddd261fcf
STEP: Creating configMap with name cm-test-opt-create-bdebc548-d103-44bc-aa92-f8cec73c8456
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:07:09.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4643" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":57,"skipped":1096,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:09.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 11 14:07:09.363: INFO: Waiting up to 5m0s for pod "pod-abcc923c-c227-4825-8fc1-431f4d8dbf35" in namespace "emptydir-6304" to be "Succeeded or Failed"
Jun 11 14:07:09.368: INFO: Pod "pod-abcc923c-c227-4825-8fc1-431f4d8dbf35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786973ms
Jun 11 14:07:11.374: INFO: Pod "pod-abcc923c-c227-4825-8fc1-431f4d8dbf35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01084028s
Jun 11 14:07:13.382: INFO: Pod "pod-abcc923c-c227-4825-8fc1-431f4d8dbf35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018810248s
STEP: Saw pod success
Jun 11 14:07:13.383: INFO: Pod "pod-abcc923c-c227-4825-8fc1-431f4d8dbf35" satisfied condition "Succeeded or Failed"
Jun 11 14:07:13.388: INFO: Trying to get logs from node ip-172-31-27-214 pod pod-abcc923c-c227-4825-8fc1-431f4d8dbf35 container test-container: <nil>
STEP: delete the pod
Jun 11 14:07:13.408: INFO: Waiting for pod pod-abcc923c-c227-4825-8fc1-431f4d8dbf35 to disappear
Jun 11 14:07:13.416: INFO: Pod pod-abcc923c-c227-4825-8fc1-431f4d8dbf35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:07:13.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6304" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":58,"skipped":1097,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:13.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:07:13.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5" in namespace "projected-3008" to be "Succeeded or Failed"
Jun 11 14:07:13.489: INFO: Pod "downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.534724ms
Jun 11 14:07:15.495: INFO: Pod "downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013739479s
Jun 11 14:07:17.501: INFO: Pod "downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019049134s
STEP: Saw pod success
Jun 11 14:07:17.501: INFO: Pod "downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5" satisfied condition "Succeeded or Failed"
Jun 11 14:07:17.505: INFO: Trying to get logs from node ip-172-31-27-214 pod downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5 container client-container: <nil>
STEP: delete the pod
Jun 11 14:07:17.524: INFO: Waiting for pod downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5 to disappear
Jun 11 14:07:17.528: INFO: Pod downwardapi-volume-661e13d4-f010-4eb2-ab07-2ebdd76daab5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:07:17.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3008" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":59,"skipped":1098,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:07:17.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3" in namespace "projected-1358" to be "Succeeded or Failed"
Jun 11 14:07:17.596: INFO: Pod "downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024241ms
Jun 11 14:07:19.602: INFO: Pod "downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010131878s
Jun 11 14:07:21.610: INFO: Pod "downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018488828s
STEP: Saw pod success
Jun 11 14:07:21.610: INFO: Pod "downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3" satisfied condition "Succeeded or Failed"
Jun 11 14:07:21.614: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3 container client-container: <nil>
STEP: delete the pod
Jun 11 14:07:21.641: INFO: Waiting for pod downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3 to disappear
Jun 11 14:07:21.645: INFO: Pod downwardapi-volume-89904160-4e24-4f5e-a38d-8be8b47694e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:07:21.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1358" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":1105,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:21.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-884ada79-6ae4-43d1-a122-23f1c1a978dd
STEP: Creating a pod to test consume configMaps
Jun 11 14:07:21.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2" in namespace "configmap-9146" to be "Succeeded or Failed"
Jun 11 14:07:21.737: INFO: Pod "pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789272ms
Jun 11 14:07:23.744: INFO: Pod "pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011656596s
Jun 11 14:07:25.751: INFO: Pod "pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018379892s
STEP: Saw pod success
Jun 11 14:07:25.751: INFO: Pod "pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2" satisfied condition "Succeeded or Failed"
Jun 11 14:07:25.758: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:07:25.783: INFO: Waiting for pod pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2 to disappear
Jun 11 14:07:25.788: INFO: Pod pod-configmaps-d1c09e5f-a7cf-4f77-addc-e94c0c8d14e2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 14:07:25.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9146" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1111,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:25.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 11 14:07:26.972: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 14:07:27.098: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 14:07:27.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-20" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":62,"skipped":1129,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:27.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:07:28.425: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:07:31.457: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:07:31.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:07:34.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9377" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

â€¢ [SLOW TEST:7.628 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":63,"skipped":1133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-b0520265-18bf-4c7a-b853-063aff38dab4
STEP: Creating a pod to test consume secrets
Jun 11 14:07:34.853: INFO: Waiting up to 5m0s for pod "pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1" in namespace "secrets-8775" to be "Succeeded or Failed"
Jun 11 14:07:34.859: INFO: Pod "pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.597344ms
Jun 11 14:07:36.864: INFO: Pod "pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010684432s
Jun 11 14:07:38.871: INFO: Pod "pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017123794s
STEP: Saw pod success
Jun 11 14:07:38.871: INFO: Pod "pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1" satisfied condition "Succeeded or Failed"
Jun 11 14:07:38.875: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:07:38.899: INFO: Waiting for pod pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1 to disappear
Jun 11 14:07:38.903: INFO: Pod pod-secrets-532aa2b6-6f30-4835-9ea5-9b4bb16e3bd1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:07:38.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8775" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":1158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:07:38.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 11 14:07:38.976: INFO: The status of Pod pod-exec-websocket-38235dab-18a2-4bda-9639-6f513e01ea98 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:07:40.981: INFO: The status of Pod pod-exec-websocket-38235dab-18a2-4bda-9639-6f513e01ea98 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 14:07:41.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-605" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":65,"skipped":1190,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:41.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jun 11 14:07:41.124: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:07:43.133: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jun 11 14:07:43.162: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:07:45.170: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun 11 14:07:45.185: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 14:07:45.190: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 14:07:47.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 14:07:47.198: INFO: Pod pod-with-prestop-http-hook still exists
Jun 11 14:07:49.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 11 14:07:49.196: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jun 11 14:07:49.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9205" for this suite.

â€¢ [SLOW TEST:8.156 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":66,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:07:49.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 11 14:07:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:07:53.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:08:10.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1296" for this suite.

â€¢ [SLOW TEST:21.046 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":67,"skipped":1226,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:08:10.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8484
STEP: creating service affinity-nodeport-transition in namespace services-8484
STEP: creating replication controller affinity-nodeport-transition in namespace services-8484
I0611 14:08:10.336758      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8484, replica count: 3
I0611 14:08:13.387292      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 14:08:13.406: INFO: Creating new exec pod
Jun 11 14:08:16.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun 11 14:08:19.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 11 14:08:19.237: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:08:19.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.251.88 80'
Jun 11 14:08:19.446: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.251.88 80\nConnection to 10.107.251.88 80 port [tcp/http] succeeded!\n"
Jun 11 14:08:19.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:08:19.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 32064'
Jun 11 14:08:19.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.214 32064\nConnection to 172.31.27.214 32064 port [tcp/*] succeeded!\n"
Jun 11 14:08:19.642: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:08:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 32064'
Jun 11 14:08:19.829: INFO: stderr: "+ + nc -v -t -w 2 172.31.41.158 32064\necho hostName\nConnection to 172.31.41.158 32064 port [tcp/*] succeeded!\n"
Jun 11 14:08:19.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:08:19.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.214:32064/ ; done'
Jun 11 14:08:20.176: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n"
Jun 11 14:08:20.176: INFO: stdout: "\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-tbtcp\naffinity-nodeport-transition-tbtcp\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-wsz4h\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-tbtcp\naffinity-nodeport-transition-wsz4h"
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-tbtcp
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-tbtcp
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-tbtcp
Jun 11 14:08:20.176: INFO: Received response from host: affinity-nodeport-transition-wsz4h
Jun 11 14:08:20.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-8484 exec execpod-affinity2w4kd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.214:32064/ ; done'
Jun 11 14:08:20.547: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:32064/\n"
Jun 11 14:08:20.547: INFO: stdout: "\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576\naffinity-nodeport-transition-dc576"
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Received response from host: affinity-nodeport-transition-dc576
Jun 11 14:08:20.548: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8484, will wait for the garbage collector to delete the pods
Jun 11 14:08:20.638: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.905215ms
Jun 11 14:08:20.738: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.680725ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:08:23.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8484" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:13.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":68,"skipped":1246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:08:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-c88dab76-f2bd-4850-99b4-4ce830293fe3
STEP: Creating a pod to test consume configMaps
Jun 11 14:08:23.349: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa" in namespace "projected-8694" to be "Succeeded or Failed"
Jun 11 14:08:23.357: INFO: Pod "pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.928265ms
Jun 11 14:08:25.365: INFO: Pod "pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015257756s
Jun 11 14:08:27.375: INFO: Pod "pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024782053s
STEP: Saw pod success
Jun 11 14:08:27.375: INFO: Pod "pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa" satisfied condition "Succeeded or Failed"
Jun 11 14:08:27.379: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:08:27.415: INFO: Waiting for pod pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa to disappear
Jun 11 14:08:27.419: INFO: Pod pod-projected-configmaps-73b61abc-cecd-4ce1-ac78-378614f6d7aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 14:08:27.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8694" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":69,"skipped":1276,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:08:27.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 11 14:08:27.492: INFO: Waiting up to 5m0s for pod "pod-08621ef9-bb31-4575-b43d-7bafece5d5c1" in namespace "emptydir-814" to be "Succeeded or Failed"
Jun 11 14:08:27.506: INFO: Pod "pod-08621ef9-bb31-4575-b43d-7bafece5d5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.097009ms
Jun 11 14:08:29.518: INFO: Pod "pod-08621ef9-bb31-4575-b43d-7bafece5d5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025191734s
Jun 11 14:08:31.530: INFO: Pod "pod-08621ef9-bb31-4575-b43d-7bafece5d5c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037654536s
STEP: Saw pod success
Jun 11 14:08:31.530: INFO: Pod "pod-08621ef9-bb31-4575-b43d-7bafece5d5c1" satisfied condition "Succeeded or Failed"
Jun 11 14:08:31.536: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-08621ef9-bb31-4575-b43d-7bafece5d5c1 container test-container: <nil>
STEP: delete the pod
Jun 11 14:08:31.569: INFO: Waiting for pod pod-08621ef9-bb31-4575-b43d-7bafece5d5c1 to disappear
Jun 11 14:08:31.574: INFO: Pod pod-08621ef9-bb31-4575-b43d-7bafece5d5c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:08:31.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-814" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":70,"skipped":1283,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:08:31.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jun 11 14:08:33.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8719" for this suite.
â€¢{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":71,"skipped":1286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:08:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 11 14:08:33.763: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 11 14:08:33.777: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 14:08:33.783: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-27-214 before test
Jun 11 14:08:33.794: INFO: calico-kube-controllers-68884f975d-h49sk from calico-system started at 2022-06-11 13:45:50 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.794: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 11 14:08:33.794: INFO: calico-node-6kzfb from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.794: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:08:33.794: INFO: calico-typha-7bb48cc99f-25j2w from calico-system started at 2022-06-11 13:45:39 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.794: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:08:33.794: INFO: kube-proxy-9l4fz from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.794: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:08:33.794: INFO: nginx-proxy-ip-172-31-27-214 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.794: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:08:33.794: INFO: sonobuoy-e2e-job-f40c26145eda47fd from sonobuoy started at 2022-06-11 13:53:19 +0000 UTC (2 container statuses recorded)
Jun 11 14:08:33.795: INFO: 	Container e2e ready: true, restart count 0
Jun 11 14:08:33.795: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:08:33.795: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-pjdgw from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:08:33.795: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:08:33.795: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 14:08:33.795: INFO: tigera-operator-5fb55776df-zr59s from tigera-operator started at 2022-06-11 13:45:20 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.795: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 11 14:08:33.795: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-41-158 before test
Jun 11 14:08:33.806: INFO: calico-apiserver-697d674bb5-dzsj2 from calico-apiserver started at 2022-06-11 13:46:33 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container calico-apiserver ready: true, restart count 0
Jun 11 14:08:33.806: INFO: calico-node-7hz6q from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:08:33.806: INFO: calico-typha-7bb48cc99f-nt5gz from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:08:33.806: INFO: suspend-false-to-true-7tdsb from job-8719 started at 2022-06-11 14:08:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container c ready: true, restart count 0
Jun 11 14:08:33.806: INFO: suspend-false-to-true-9r9mz from job-8719 started at 2022-06-11 14:08:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container c ready: true, restart count 0
Jun 11 14:08:33.806: INFO: kube-proxy-ljlz6 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:08:33.806: INFO: nginx-proxy-ip-172-31-41-158 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:08:33.806: INFO: sonobuoy from sonobuoy started at 2022-06-11 13:53:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 14:08:33.806: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-gzshn from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:08:33.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:08:33.806: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-80ca5a60-8ab2-4611-8f17-fd8851fa5652 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.27.214 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-80ca5a60-8ab2-4611-8f17-fd8851fa5652 off the node ip-172-31-27-214
STEP: verifying the node doesn't have the label kubernetes.io/e2e-80ca5a60-8ab2-4611-8f17-fd8851fa5652
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:13:37.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2382" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

â€¢ [SLOW TEST:304.277 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":72,"skipped":1308,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:13:37.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:13:38.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jun 11 14:13:42.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-1750 --namespace=crd-publish-openapi-1750 create -f -'
Jun 11 14:13:43.155: INFO: stderr: ""
Jun 11 14:13:43.155: INFO: stdout: "e2e-test-crd-publish-openapi-8452-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 11 14:13:43.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-1750 --namespace=crd-publish-openapi-1750 delete e2e-test-crd-publish-openapi-8452-crds test-cr'
Jun 11 14:13:43.416: INFO: stderr: ""
Jun 11 14:13:43.416: INFO: stdout: "e2e-test-crd-publish-openapi-8452-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 11 14:13:43.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-1750 --namespace=crd-publish-openapi-1750 apply -f -'
Jun 11 14:13:44.511: INFO: stderr: ""
Jun 11 14:13:44.511: INFO: stdout: "e2e-test-crd-publish-openapi-8452-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 11 14:13:44.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-1750 --namespace=crd-publish-openapi-1750 delete e2e-test-crd-publish-openapi-8452-crds test-cr'
Jun 11 14:13:44.607: INFO: stderr: ""
Jun 11 14:13:44.607: INFO: stdout: "e2e-test-crd-publish-openapi-8452-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 11 14:13:44.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-1750 explain e2e-test-crd-publish-openapi-8452-crds'
Jun 11 14:13:44.847: INFO: stderr: ""
Jun 11 14:13:44.847: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8452-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:13:50.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1750" for this suite.

â€¢ [SLOW TEST:12.608 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":73,"skipped":1312,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:13:50.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:13:51.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:13:54.278: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:13:54.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7228" for this suite.
STEP: Destroying namespace "webhook-7228-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":74,"skipped":1316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:13:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 11 14:14:04.636: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 14:14:04.757: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 14:14:04.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3094" for this suite.

â€¢ [SLOW TEST:10.297 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":75,"skipped":1338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:14:04.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 14:14:14.864: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.869: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.878: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.883: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.889: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.898: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.904: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.910: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:14.910: INFO: Lookups using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local]

Jun 11 14:14:19.916: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.921: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.927: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.938: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.943: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.949: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.954: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:19.954: INFO: Lookups using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local]

Jun 11 14:14:24.916: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.924: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.934: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.940: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.945: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.951: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.956: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.962: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:24.962: INFO: Lookups using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local]

Jun 11 14:14:29.916: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.922: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.928: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.934: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.940: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.948: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.954: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.960: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:29.960: INFO: Lookups using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local]

Jun 11 14:14:34.922: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.935: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.941: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.947: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.952: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.958: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.964: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local from pod dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a: the server could not find the requested resource (get pods dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a)
Jun 11 14:14:34.964: INFO: Lookups using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6398.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6398.svc.cluster.local jessie_udp@dns-test-service-2.dns-6398.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6398.svc.cluster.local]

Jun 11 14:14:39.968: INFO: DNS probes using dns-6398/dns-test-8a45045f-68ed-4370-8ff5-a2a144dd161a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 14:14:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6398" for this suite.

â€¢ [SLOW TEST:35.258 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":76,"skipped":1398,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:14:40.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-5048
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5048 to expose endpoints map[]
Jun 11 14:14:40.113: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun 11 14:14:41.126: INFO: successfully validated that service multi-endpoint-test in namespace services-5048 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5048
Jun 11 14:14:41.140: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:14:43.150: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:14:45.148: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5048 to expose endpoints map[pod1:[100]]
Jun 11 14:14:45.169: INFO: successfully validated that service multi-endpoint-test in namespace services-5048 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5048
Jun 11 14:14:45.184: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:14:47.190: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5048 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 11 14:14:47.213: INFO: successfully validated that service multi-endpoint-test in namespace services-5048 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jun 11 14:14:47.213: INFO: Creating new exec pod
Jun 11 14:14:50.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5048 exec execpodqd6fw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun 11 14:14:50.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 11 14:14:50.416: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:14:50.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5048 exec execpodqd6fw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.56.54 80'
Jun 11 14:14:50.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.56.54 80\nConnection to 10.97.56.54 80 port [tcp/http] succeeded!\n"
Jun 11 14:14:50.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:14:50.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5048 exec execpodqd6fw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun 11 14:14:50.742: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 11 14:14:50.742: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:14:50.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5048 exec execpodqd6fw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.56.54 81'
Jun 11 14:14:50.967: INFO: stderr: "+ nc -v -t -w 2 10.97.56.54 81\n+ echo hostName\nConnection to 10.97.56.54 81 port [tcp/*] succeeded!\n"
Jun 11 14:14:50.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5048
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5048 to expose endpoints map[pod2:[101]]
Jun 11 14:14:52.012: INFO: successfully validated that service multi-endpoint-test in namespace services-5048 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5048
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5048 to expose endpoints map[]
Jun 11 14:14:53.046: INFO: successfully validated that service multi-endpoint-test in namespace services-5048 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:14:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5048" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:13.065 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":77,"skipped":1412,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:14:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 11 14:14:53.169: INFO: Waiting up to 5m0s for pod "pod-fe646f68-03ab-43fb-89a9-df368446b2c6" in namespace "emptydir-5339" to be "Succeeded or Failed"
Jun 11 14:14:53.174: INFO: Pod "pod-fe646f68-03ab-43fb-89a9-df368446b2c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.038686ms
Jun 11 14:14:55.180: INFO: Pod "pod-fe646f68-03ab-43fb-89a9-df368446b2c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.010691952s
Jun 11 14:14:57.189: INFO: Pod "pod-fe646f68-03ab-43fb-89a9-df368446b2c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019298618s
STEP: Saw pod success
Jun 11 14:14:57.189: INFO: Pod "pod-fe646f68-03ab-43fb-89a9-df368446b2c6" satisfied condition "Succeeded or Failed"
Jun 11 14:14:57.194: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-fe646f68-03ab-43fb-89a9-df368446b2c6 container test-container: <nil>
STEP: delete the pod
Jun 11 14:14:57.250: INFO: Waiting for pod pod-fe646f68-03ab-43fb-89a9-df368446b2c6 to disappear
Jun 11 14:14:57.255: INFO: Pod pod-fe646f68-03ab-43fb-89a9-df368446b2c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:14:57.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5339" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":78,"skipped":1414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:14:57.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:15:13.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5558" for this suite.

â€¢ [SLOW TEST:16.278 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":79,"skipped":1438,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:13.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jun 11 14:15:15.636: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jun 11 14:15:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7353" for this suite.

â€¢ [SLOW TEST:8.293 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":80,"skipped":1452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-1168
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1168
STEP: Deleting pre-stop pod
Jun 11 14:15:31.021: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jun 11 14:15:31.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1168" for this suite.

â€¢ [SLOW TEST:9.213 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":81,"skipped":1492,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:31.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 11 14:15:32.181: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 14:15:32.257: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 14:15:32.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9144" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":82,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:32.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-28125b3f-f013-489c-9da3-760d8474548f
STEP: Creating a pod to test consume secrets
Jun 11 14:15:32.323: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba" in namespace "projected-2388" to be "Succeeded or Failed"
Jun 11 14:15:32.328: INFO: Pod "pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321505ms
Jun 11 14:15:34.334: INFO: Pod "pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010657619s
Jun 11 14:15:36.345: INFO: Pod "pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021207131s
STEP: Saw pod success
Jun 11 14:15:36.345: INFO: Pod "pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba" satisfied condition "Succeeded or Failed"
Jun 11 14:15:36.350: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:15:36.380: INFO: Waiting for pod pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba to disappear
Jun 11 14:15:36.385: INFO: Pod pod-projected-secrets-72f4a6a1-74e5-480d-a800-47a2d7b9f8ba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 14:15:36.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2388" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1539,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:36.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:15:36.913: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:15:39.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:15:40.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3281" for this suite.
STEP: Destroying namespace "webhook-3281-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":84,"skipped":1547,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:15:40.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3871
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jun 11 14:15:40.272: INFO: Found 0 stateful pods, waiting for 3
Jun 11 14:15:50.279: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:15:50.279: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:15:50.279: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:15:50.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-3871 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:15:50.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:15:50.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:15:50.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jun 11 14:16:00.565: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 11 14:16:10.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-3871 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 14:16:10.863: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 14:16:10.863: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 14:16:10.863: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jun 11 14:16:30.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-3871 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 14:16:31.102: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 14:16:31.102: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 14:16:31.102: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 14:16:41.145: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 11 14:16:51.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-3871 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 14:16:51.367: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 14:16:51.367: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 14:16:51.367: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 14:17:01.426: INFO: Deleting all statefulset in ns statefulset-3871
Jun 11 14:17:01.452: INFO: Scaling statefulset ss2 to 0
Jun 11 14:17:11.500: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 14:17:11.508: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 14:17:11.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3871" for this suite.

â€¢ [SLOW TEST:91.375 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":85,"skipped":1556,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:11.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 11 14:17:11.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:17:15.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:17:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1127" for this suite.

â€¢ [SLOW TEST:23.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":86,"skipped":1565,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:35.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:17:35.582: INFO: Endpoints addresses: [172.31.0.57 172.31.15.213 172.31.30.216] , ports: [6443]
Jun 11 14:17:35.582: INFO: EndpointSlices addresses: [172.31.0.57 172.31.15.213 172.31.30.216] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jun 11 14:17:35.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2586" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":87,"skipped":1566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:35.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 11 14:17:35.654: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jun 11 14:17:35.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9040" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":88,"skipped":1612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:35.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:17:35.723: INFO: Creating pod...
Jun 11 14:17:37.748: INFO: Creating service...
Jun 11 14:17:37.769: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/DELETE
Jun 11 14:17:37.778: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 11 14:17:37.778: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/GET
Jun 11 14:17:37.790: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 11 14:17:37.790: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/HEAD
Jun 11 14:17:37.796: INFO: http.Client request:HEAD | StatusCode:200
Jun 11 14:17:37.797: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 11 14:17:37.803: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 11 14:17:37.804: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/PATCH
Jun 11 14:17:37.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 11 14:17:37.815: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/POST
Jun 11 14:17:37.822: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 11 14:17:37.822: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/pods/agnhost/proxy/some/path/with/PUT
Jun 11 14:17:37.831: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 11 14:17:37.831: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/DELETE
Jun 11 14:17:37.844: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 11 14:17:37.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/GET
Jun 11 14:17:37.853: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 11 14:17:37.853: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/HEAD
Jun 11 14:17:37.864: INFO: http.Client request:HEAD | StatusCode:200
Jun 11 14:17:37.864: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/OPTIONS
Jun 11 14:17:37.874: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 11 14:17:37.874: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/PATCH
Jun 11 14:17:37.885: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 11 14:17:37.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/POST
Jun 11 14:17:37.895: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 11 14:17:37.895: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1757/services/test-service/proxy/some/path/with/PUT
Jun 11 14:17:37.907: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jun 11 14:17:37.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1757" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":89,"skipped":1640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:37.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:17:38.535: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:17:41.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:17:41.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:17:44.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6941" for this suite.
STEP: Destroying namespace "webhook-6941-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:7.064 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":90,"skipped":1663,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:44.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jun 11 14:17:45.069: INFO: Waiting up to 5m0s for pod "var-expansion-33289eb2-43b1-443c-a6a8-acba07827772" in namespace "var-expansion-7802" to be "Succeeded or Failed"
Jun 11 14:17:45.075: INFO: Pod "var-expansion-33289eb2-43b1-443c-a6a8-acba07827772": Phase="Pending", Reason="", readiness=false. Elapsed: 5.388521ms
Jun 11 14:17:47.082: INFO: Pod "var-expansion-33289eb2-43b1-443c-a6a8-acba07827772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012684191s
Jun 11 14:17:49.089: INFO: Pod "var-expansion-33289eb2-43b1-443c-a6a8-acba07827772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019608238s
STEP: Saw pod success
Jun 11 14:17:49.090: INFO: Pod "var-expansion-33289eb2-43b1-443c-a6a8-acba07827772" satisfied condition "Succeeded or Failed"
Jun 11 14:17:49.095: INFO: Trying to get logs from node ip-172-31-41-158 pod var-expansion-33289eb2-43b1-443c-a6a8-acba07827772 container dapi-container: <nil>
STEP: delete the pod
Jun 11 14:17:49.140: INFO: Waiting for pod var-expansion-33289eb2-43b1-443c-a6a8-acba07827772 to disappear
Jun 11 14:17:49.144: INFO: Pod var-expansion-33289eb2-43b1-443c-a6a8-acba07827772 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 14:17:49.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7802" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":91,"skipped":1680,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:49.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:17:49.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082" in namespace "projected-712" to be "Succeeded or Failed"
Jun 11 14:17:49.216: INFO: Pod "downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082": Phase="Pending", Reason="", readiness=false. Elapsed: 6.505748ms
Jun 11 14:17:51.232: INFO: Pod "downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022221718s
Jun 11 14:17:53.241: INFO: Pod "downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031883824s
STEP: Saw pod success
Jun 11 14:17:53.241: INFO: Pod "downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082" satisfied condition "Succeeded or Failed"
Jun 11 14:17:53.247: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082 container client-container: <nil>
STEP: delete the pod
Jun 11 14:17:53.275: INFO: Waiting for pod downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082 to disappear
Jun 11 14:17:53.279: INFO: Pod downwardapi-volume-eaf96e18-00f0-4019-a716-81c5200bb082 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:17:53.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-712" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1699,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:17:53.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 11 14:17:53.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 11 14:18:14.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:18:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:18:38.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9212" for this suite.

â€¢ [SLOW TEST:45.214 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":93,"skipped":1707,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:18:38.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jun 11 14:18:42.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5319" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":94,"skipped":1720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:18:42.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 11 14:18:43.124: INFO: Pod name wrapped-volume-race-ff5fb4e8-8231-4a9b-8ef8-0b6781706f56: Found 0 pods out of 5
Jun 11 14:18:48.139: INFO: Pod name wrapped-volume-race-ff5fb4e8-8231-4a9b-8ef8-0b6781706f56: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ff5fb4e8-8231-4a9b-8ef8-0b6781706f56 in namespace emptydir-wrapper-9651, will wait for the garbage collector to delete the pods
Jun 11 14:18:58.253: INFO: Deleting ReplicationController wrapped-volume-race-ff5fb4e8-8231-4a9b-8ef8-0b6781706f56 took: 13.566195ms
Jun 11 14:18:58.355: INFO: Terminating ReplicationController wrapped-volume-race-ff5fb4e8-8231-4a9b-8ef8-0b6781706f56 pods took: 102.225096ms
STEP: Creating RC which spawns configmap-volume pods
Jun 11 14:19:01.893: INFO: Pod name wrapped-volume-race-ed9c3300-f9f7-4d82-8708-00850801a899: Found 0 pods out of 5
Jun 11 14:19:06.905: INFO: Pod name wrapped-volume-race-ed9c3300-f9f7-4d82-8708-00850801a899: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ed9c3300-f9f7-4d82-8708-00850801a899 in namespace emptydir-wrapper-9651, will wait for the garbage collector to delete the pods
Jun 11 14:19:17.027: INFO: Deleting ReplicationController wrapped-volume-race-ed9c3300-f9f7-4d82-8708-00850801a899 took: 14.981446ms
Jun 11 14:19:17.228: INFO: Terminating ReplicationController wrapped-volume-race-ed9c3300-f9f7-4d82-8708-00850801a899 pods took: 201.002102ms
STEP: Creating RC which spawns configmap-volume pods
Jun 11 14:19:21.167: INFO: Pod name wrapped-volume-race-c46e0d3d-38f0-4353-bd5e-477218467aa7: Found 0 pods out of 5
Jun 11 14:19:26.185: INFO: Pod name wrapped-volume-race-c46e0d3d-38f0-4353-bd5e-477218467aa7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c46e0d3d-38f0-4353-bd5e-477218467aa7 in namespace emptydir-wrapper-9651, will wait for the garbage collector to delete the pods
Jun 11 14:19:38.301: INFO: Deleting ReplicationController wrapped-volume-race-c46e0d3d-38f0-4353-bd5e-477218467aa7 took: 12.294536ms
Jun 11 14:19:38.403: INFO: Terminating ReplicationController wrapped-volume-race-c46e0d3d-38f0-4353-bd5e-477218467aa7 pods took: 102.20216ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jun 11 14:19:42.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9651" for this suite.

â€¢ [SLOW TEST:59.516 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":95,"skipped":1773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:19:42.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 11 14:19:42.205: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 14:20:42.266: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jun 11 14:20:42.316: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 11 14:20:42.326: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 11 14:20:42.365: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 11 14:20:42.376: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:20:54.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1586" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

â€¢ [SLOW TEST:72.416 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":96,"skipped":1815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:20:54.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 11 14:20:54.588: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 11 14:20:54.599: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 11 14:20:54.599: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 11 14:20:54.617: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 11 14:20:54.617: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 11 14:20:54.634: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 11 14:20:54.634: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 11 14:21:01.711: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jun 11 14:21:01.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1503" for this suite.

â€¢ [SLOW TEST:7.214 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":97,"skipped":1858,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-057fc303-07e6-4345-9216-7df959037b93
STEP: Creating a pod to test consume secrets
Jun 11 14:21:01.815: INFO: Waiting up to 5m0s for pod "pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a" in namespace "secrets-1740" to be "Succeeded or Failed"
Jun 11 14:21:01.826: INFO: Pod "pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.21406ms
Jun 11 14:21:03.835: INFO: Pod "pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019915724s
Jun 11 14:21:05.846: INFO: Pod "pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031013171s
STEP: Saw pod success
Jun 11 14:21:05.846: INFO: Pod "pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a" satisfied condition "Succeeded or Failed"
Jun 11 14:21:05.851: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:21:05.894: INFO: Waiting for pod pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a to disappear
Jun 11 14:21:05.899: INFO: Pod pod-secrets-136332c2-b207-4b38-a24f-498b1552b66a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:21:05.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1740" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:05.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jun 11 14:21:05.997: INFO: Found Service test-service-z2s7x in namespace services-9749 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 11 14:21:05.998: INFO: Service test-service-z2s7x created
STEP: Getting /status
Jun 11 14:21:06.011: INFO: Service test-service-z2s7x has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jun 11 14:21:06.029: INFO: observed Service test-service-z2s7x in namespace services-9749 with annotations: map[] & LoadBalancer: {[]}
Jun 11 14:21:06.030: INFO: Found Service test-service-z2s7x in namespace services-9749 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 11 14:21:06.030: INFO: Service test-service-z2s7x has service status patched
STEP: updating the ServiceStatus
Jun 11 14:21:06.078: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jun 11 14:21:06.081: INFO: Observed Service test-service-z2s7x in namespace services-9749 with annotations: map[] & Conditions: {[]}
Jun 11 14:21:06.081: INFO: Observed event: &Service{ObjectMeta:{test-service-z2s7x  services-9749  16c50056-4afb-4f45-944d-657a355d20bf 18837 0 2022-06-11 14:21:05 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-06-11 14:21:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-06-11 14:21:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.106.63.197,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.106.63.197],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 11 14:21:06.082: INFO: Found Service test-service-z2s7x in namespace services-9749 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 11 14:21:06.082: INFO: Service test-service-z2s7x has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jun 11 14:21:06.107: INFO: observed Service test-service-z2s7x in namespace services-9749 with labels: map[test-service-static:true]
Jun 11 14:21:06.108: INFO: observed Service test-service-z2s7x in namespace services-9749 with labels: map[test-service-static:true]
Jun 11 14:21:06.108: INFO: observed Service test-service-z2s7x in namespace services-9749 with labels: map[test-service-static:true]
Jun 11 14:21:06.108: INFO: Found Service test-service-z2s7x in namespace services-9749 with labels: map[test-service:patched test-service-static:true]
Jun 11 14:21:06.108: INFO: Service test-service-z2s7x patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jun 11 14:21:06.158: INFO: Observed event: ADDED
Jun 11 14:21:06.158: INFO: Observed event: MODIFIED
Jun 11 14:21:06.158: INFO: Observed event: MODIFIED
Jun 11 14:21:06.159: INFO: Observed event: MODIFIED
Jun 11 14:21:06.159: INFO: Found Service test-service-z2s7x in namespace services-9749 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 11 14:21:06.159: INFO: Service test-service-z2s7x deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:21:06.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9749" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
â€¢{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":99,"skipped":1879,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:06.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:21:17.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7152" for this suite.

â€¢ [SLOW TEST:11.158 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":100,"skipped":1883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-2ab7eaaf-ac79-47b3-9407-d013de2073e8
STEP: Creating a pod to test consume secrets
Jun 11 14:21:17.420: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703" in namespace "projected-6730" to be "Succeeded or Failed"
Jun 11 14:21:17.427: INFO: Pod "pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703": Phase="Pending", Reason="", readiness=false. Elapsed: 6.795723ms
Jun 11 14:21:19.436: INFO: Pod "pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015340706s
Jun 11 14:21:21.443: INFO: Pod "pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023214378s
STEP: Saw pod success
Jun 11 14:21:21.444: INFO: Pod "pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703" satisfied condition "Succeeded or Failed"
Jun 11 14:21:21.448: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:21:21.492: INFO: Waiting for pod pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703 to disappear
Jun 11 14:21:21.499: INFO: Pod pod-projected-secrets-cabf82a6-ff19-404b-85aa-fa68f490c703 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 14:21:21.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6730" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":101,"skipped":1907,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jun 11 14:21:21.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:21:46.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3718" for this suite.

â€¢ [SLOW TEST:24.789 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":102,"skipped":1920,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:46.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-1718246f-3f4f-4cc4-80dd-4208ed7589e6
STEP: Creating secret with name secret-projected-all-test-volume-f15f6885-763e-4554-8209-f4272c2e62be
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 11 14:21:46.400: INFO: Waiting up to 5m0s for pod "projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0" in namespace "projected-1769" to be "Succeeded or Failed"
Jun 11 14:21:46.408: INFO: Pod "projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.672176ms
Jun 11 14:21:48.422: INFO: Pod "projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021881704s
Jun 11 14:21:50.431: INFO: Pod "projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031139741s
STEP: Saw pod success
Jun 11 14:21:50.432: INFO: Pod "projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0" satisfied condition "Succeeded or Failed"
Jun 11 14:21:50.437: INFO: Trying to get logs from node ip-172-31-41-158 pod projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 11 14:21:50.478: INFO: Waiting for pod projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0 to disappear
Jun 11 14:21:50.483: INFO: Pod projected-volume-0522baee-d1a6-4413-9c4f-0424c5f0a0b0 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jun 11 14:21:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1769" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":103,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:50.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jun 11 14:21:50.584: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jun 11 14:21:50.623: INFO: waiting for watch events with expected annotations in namespace
Jun 11 14:21:50.624: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jun 11 14:21:50.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5941" for this suite.
â€¢{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":104,"skipped":1959,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:50.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-df3cd1d6-e5fc-4335-b6e1-f7db23f28f7b
STEP: Creating a pod to test consume secrets
Jun 11 14:21:50.772: INFO: Waiting up to 5m0s for pod "pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f" in namespace "secrets-3025" to be "Succeeded or Failed"
Jun 11 14:21:50.780: INFO: Pod "pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.091827ms
Jun 11 14:21:52.785: INFO: Pod "pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012377691s
Jun 11 14:21:54.794: INFO: Pod "pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021735399s
STEP: Saw pod success
Jun 11 14:21:54.794: INFO: Pod "pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f" satisfied condition "Succeeded or Failed"
Jun 11 14:21:54.799: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:21:54.832: INFO: Waiting for pod pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f to disappear
Jun 11 14:21:54.839: INFO: Pod pod-secrets-41598eee-7ab9-4496-ba84-72a6c0cf1b2f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:21:54.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3025" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":1967,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:54.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 11 14:21:54.897: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 11 14:21:54.909: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 14:21:54.915: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-27-214 before test
Jun 11 14:21:54.927: INFO: calico-kube-controllers-68884f975d-h49sk from calico-system started at 2022-06-11 13:45:50 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.927: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 11 14:21:54.927: INFO: calico-node-6kzfb from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.927: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:21:54.927: INFO: calico-typha-7bb48cc99f-25j2w from calico-system started at 2022-06-11 13:45:39 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.927: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:21:54.927: INFO: kube-proxy-9l4fz from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.928: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:21:54.928: INFO: nginx-proxy-ip-172-31-27-214 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.928: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:21:54.928: INFO: sonobuoy-e2e-job-f40c26145eda47fd from sonobuoy started at 2022-06-11 13:53:19 +0000 UTC (2 container statuses recorded)
Jun 11 14:21:54.929: INFO: 	Container e2e ready: true, restart count 0
Jun 11 14:21:54.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:21:54.929: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-pjdgw from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:21:54.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:21:54.929: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 14:21:54.929: INFO: tigera-operator-5fb55776df-zr59s from tigera-operator started at 2022-06-11 13:45:20 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.929: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 11 14:21:54.929: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-41-158 before test
Jun 11 14:21:54.942: INFO: calico-apiserver-697d674bb5-dzsj2 from calico-apiserver started at 2022-06-11 13:46:33 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.942: INFO: 	Container calico-apiserver ready: true, restart count 0
Jun 11 14:21:54.943: INFO: calico-node-7hz6q from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.944: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:21:54.944: INFO: calico-typha-7bb48cc99f-nt5gz from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.945: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:21:54.945: INFO: kube-proxy-ljlz6 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.945: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:21:54.946: INFO: nginx-proxy-ip-172-31-41-158 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.946: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:21:54.946: INFO: sonobuoy from sonobuoy started at 2022-06-11 13:53:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:21:54.947: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 14:21:54.947: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-gzshn from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:21:54.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:21:54.947: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f796e8de7c6e34], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:21:55.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5418" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":106,"skipped":1967,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:21:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 11 14:21:56.088: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19167 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:21:56.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19167 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 11 14:21:56.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19169 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:21:56.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19169 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 11 14:21:56.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19171 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:21:56.121: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19171 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 11 14:21:56.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19173 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:21:56.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4074  07fb5141-1062-4f43-8301-92657bb02b94 19173 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 11 14:21:56.144: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4074  31aabbf5-1735-4932-a429-eba6b81970c7 19174 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:21:56.144: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4074  31aabbf5-1735-4932-a429-eba6b81970c7 19174 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 11 14:22:06.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4074  31aabbf5-1735-4932-a429-eba6b81970c7 19219 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 14:22:06.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4074  31aabbf5-1735-4932-a429-eba6b81970c7 19219 0 2022-06-11 14:21:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-11 14:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jun 11 14:22:16.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4074" for this suite.

â€¢ [SLOW TEST:20.169 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":107,"skipped":1981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:22:16.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jun 11 14:22:36.512: INFO: EndpointSlice for Service endpointslice-625/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jun 11 14:22:46.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-625" for this suite.

â€¢ [SLOW TEST:30.376 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":108,"skipped":2034,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:22:46.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 11 14:22:46.645: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 14:23:46.707: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:23:46.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 11 14:23:48.829: INFO: found a healthy node: ip-172-31-41-158
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:23:56.978: INFO: pods created so far: [1 1 1]
Jun 11 14:23:56.978: INFO: length of pods created so far: 3
Jun 11 14:23:59.008: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jun 11 14:24:06.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9580" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:24:06.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3503" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

â€¢ [SLOW TEST:79.638 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":109,"skipped":2034,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:06.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:24:06.282: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-17923079-bacc-4afd-b617-a21ef35173d9" in namespace "security-context-test-5467" to be "Succeeded or Failed"
Jun 11 14:24:06.322: INFO: Pod "busybox-readonly-false-17923079-bacc-4afd-b617-a21ef35173d9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.397782ms
Jun 11 14:24:08.330: INFO: Pod "busybox-readonly-false-17923079-bacc-4afd-b617-a21ef35173d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047871589s
Jun 11 14:24:10.345: INFO: Pod "busybox-readonly-false-17923079-bacc-4afd-b617-a21ef35173d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062870319s
Jun 11 14:24:10.345: INFO: Pod "busybox-readonly-false-17923079-bacc-4afd-b617-a21ef35173d9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 14:24:10.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5467" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":2041,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:10.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:24:21.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3720" for this suite.

â€¢ [SLOW TEST:11.160 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":111,"skipped":2053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:21.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:24:22.374: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:24:25.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:24:25.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6377" for this suite.
STEP: Destroying namespace "webhook-6377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":112,"skipped":2080,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:25.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:24:25.934: INFO: Got root ca configmap in namespace "svcaccounts-3128"
Jun 11 14:24:25.942: INFO: Deleted root ca configmap in namespace "svcaccounts-3128"
STEP: waiting for a new root ca configmap created
Jun 11 14:24:26.449: INFO: Recreated root ca configmap in namespace "svcaccounts-3128"
Jun 11 14:24:26.457: INFO: Updated root ca configmap in namespace "svcaccounts-3128"
STEP: waiting for the root ca configmap reconciled
Jun 11 14:24:26.970: INFO: Reconciled root ca configmap in namespace "svcaccounts-3128"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 14:24:26.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3128" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":113,"skipped":2084,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:26.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 11 14:24:27.058: INFO: Waiting up to 5m0s for pod "pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb" in namespace "emptydir-1285" to be "Succeeded or Failed"
Jun 11 14:24:27.063: INFO: Pod "pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877084ms
Jun 11 14:24:29.071: INFO: Pod "pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013222716s
Jun 11 14:24:31.077: INFO: Pod "pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018888809s
STEP: Saw pod success
Jun 11 14:24:31.077: INFO: Pod "pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb" satisfied condition "Succeeded or Failed"
Jun 11 14:24:31.084: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb container test-container: <nil>
STEP: delete the pod
Jun 11 14:24:31.118: INFO: Waiting for pod pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb to disappear
Jun 11 14:24:31.122: INFO: Pod pod-4f569f73-e62f-491c-b8bd-9e72c1f5edeb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:24:31.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1285" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":2097,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:31.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:24:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8835" for this suite.
STEP: Destroying namespace "nspatchtest-2fde1848-05e0-4e2d-a959-b08a8c726f99-8592" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":115,"skipped":2114,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:31.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:24:31.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 create -f -'
Jun 11 14:24:32.361: INFO: stderr: ""
Jun 11 14:24:32.361: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 11 14:24:32.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 create -f -'
Jun 11 14:24:33.640: INFO: stderr: ""
Jun 11 14:24:33.640: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 11 14:24:34.648: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 14:24:34.648: INFO: Found 1 / 1
Jun 11 14:24:34.648: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 11 14:24:34.653: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 14:24:34.653: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 14:24:34.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 describe pod agnhost-primary-lk8l8'
Jun 11 14:24:34.755: INFO: stderr: ""
Jun 11 14:24:34.755: INFO: stdout: "Name:         agnhost-primary-lk8l8\nNamespace:    kubectl-5833\nPriority:     0\nNode:         ip-172-31-41-158/172.31.41.158\nStart Time:   Sat, 11 Jun 2022 14:24:32 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 449ced670a4a2fcaba253a0c73c6509eac86a9aa85fc0ee732914679ce956b2a\n              cni.projectcalico.org/podIP: 192.168.203.47/32\n              cni.projectcalico.org/podIPs: 192.168.203.47/32\nStatus:       Running\nIP:           192.168.203.47\nIPs:\n  IP:           192.168.203.47\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://238d87f94aae0e9417a80e62264ee9f68f3b94b4e65ce9efa1fc5593430ca9f6\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 11 Jun 2022 14:24:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4v68j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4v68j:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5833/agnhost-primary-lk8l8 to ip-172-31-41-158\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.36\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun 11 14:24:34.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 describe rc agnhost-primary'
Jun 11 14:24:34.854: INFO: stderr: ""
Jun 11 14:24:34.854: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5833\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.36\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lk8l8\n"
Jun 11 14:24:34.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 describe service agnhost-primary'
Jun 11 14:24:34.944: INFO: stderr: ""
Jun 11 14:24:34.944: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5833\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.82.13\nIPs:               10.110.82.13\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.203.47:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 11 14:24:34.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 describe node ip-172-31-0-57'
Jun 11 14:24:35.103: INFO: stderr: ""
Jun 11 14:24:35.103: INFO: stdout: "Name:               ip-172-31-0-57\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-0-57\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.0.57/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.231.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 11 Jun 2022 13:31:14 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-0-57\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 11 Jun 2022 14:24:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 11 Jun 2022 13:45:56 +0000   Sat, 11 Jun 2022 13:45:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 11 Jun 2022 14:19:32 +0000   Sat, 11 Jun 2022 13:31:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 11 Jun 2022 14:19:32 +0000   Sat, 11 Jun 2022 13:31:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 11 Jun 2022 14:19:32 +0000   Sat, 11 Jun 2022 13:31:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 11 Jun 2022 14:19:32 +0000   Sat, 11 Jun 2022 13:45:51 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.0.57\n  Hostname:    ip-172-31-0-57\nCapacity:\n  cpu:                2\n  ephemeral-storage:  7950756Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3970708Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  7327416718\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3868308Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec218521815f638768f649cce461e42c\n  System UUID:                ec218521-815f-6387-68f6-49cce461e42c\n  Boot ID:                    183629e0-5664-41a7-8f6a-60594e64d8d6\n  Kernel Version:             5.15.0-1011-aws\n  OS Image:                   Ubuntu 22.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.24.1\n  Kube-Proxy Version:         v1.24.1\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-grxgx                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  kube-system                 etcd-ip-172-31-0-57                                        100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         53m\n  kube-system                 kube-apiserver-ip-172-31-0-57                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-controller-manager-ip-172-31-0-57                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-proxy-lvqh9                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-scheduler-ip-172-31-0-57                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         53m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-v5zv5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             100Mi (2%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 52m                kube-proxy       \n  Normal   NodeHasNoDiskPressure    54m (x7 over 54m)  kubelet          Node ip-172-31-0-57 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     54m (x7 over 54m)  kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientPID\n  Normal   NodeHasSufficientMemory  53m (x8 over 54m)  kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientMemory\n  Normal   Starting                 53m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      53m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  53m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    53m                kubelet          Node ip-172-31-0-57 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     53m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  53m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           53m                node-controller  Node ip-172-31-0-57 event: Registered Node ip-172-31-0-57 in Controller\n  Normal   Starting                 44m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      44m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasNoDiskPressure\n  Normal   NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientPID\n  Normal   Starting                 44m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      44m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 44m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      44m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     44m                kubelet          Node ip-172-31-0-57 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                38m                kubelet          Node ip-172-31-0-57 status is now: NodeReady\n"
Jun 11 14:24:35.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5833 describe namespace kubectl-5833'
Jun 11 14:24:35.192: INFO: stderr: ""
Jun 11 14:24:35.192: INFO: stdout: "Name:         kubectl-5833\nLabels:       e2e-framework=kubectl\n              e2e-run=904d3f1f-cad0-493e-932b-892432cc0485\n              kubernetes.io/metadata.name=kubectl-5833\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:24:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5833" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":116,"skipped":2122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:35.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun 11 14:24:35.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1551 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 11 14:24:35.332: INFO: stderr: ""
Jun 11 14:24:35.332: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 11 14:24:40.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1551 get pod e2e-test-httpd-pod -o json'
Jun 11 14:24:40.479: INFO: stderr: ""
Jun 11 14:24:40.479: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"b4c33511f50ee0b6e354828ad3881574c227a86319b16fd1f5f51698f377904e\",\n            \"cni.projectcalico.org/podIP\": \"192.168.203.23/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.203.23/32\"\n        },\n        \"creationTimestamp\": \"2022-06-11T14:24:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1551\",\n        \"resourceVersion\": \"20177\",\n        \"uid\": \"3164c395-cf5e-408a-aa34-8462ea0140c1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vtk28\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-41-158\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vtk28\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-11T14:24:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-11T14:24:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-11T14:24:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-11T14:24:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://53078993c1a1fee1540475481ff8ac5ecafbc621beddc83664ef6c32804769fa\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-06-11T14:24:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.41.158\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.203.23\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.203.23\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-06-11T14:24:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 11 14:24:40.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1551 replace -f -'
Jun 11 14:24:41.593: INFO: stderr: ""
Jun 11 14:24:41.593: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jun 11 14:24:41.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1551 delete pods e2e-test-httpd-pod'
Jun 11 14:24:43.731: INFO: stderr: ""
Jun 11 14:24:43.731: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:24:43.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1551" for this suite.

â€¢ [SLOW TEST:8.550 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":117,"skipped":2169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:43.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-8818d0f6-8bba-4bb4-8fa4-a21da82a81ec
STEP: Creating a pod to test consume secrets
Jun 11 14:24:43.816: INFO: Waiting up to 5m0s for pod "pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d" in namespace "secrets-3196" to be "Succeeded or Failed"
Jun 11 14:24:43.820: INFO: Pod "pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693271ms
Jun 11 14:24:45.829: INFO: Pod "pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013709475s
Jun 11 14:24:47.836: INFO: Pod "pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020348158s
STEP: Saw pod success
Jun 11 14:24:47.836: INFO: Pod "pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d" satisfied condition "Succeeded or Failed"
Jun 11 14:24:47.840: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:24:47.868: INFO: Waiting for pod pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d to disappear
Jun 11 14:24:47.874: INFO: Pod pod-secrets-9325105f-2e9f-421f-b40d-e603f60a093d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:24:47.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3196" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":118,"skipped":2200,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:47.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:47.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1262
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jun 11 14:24:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-465" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jun 11 14:24:54.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1262" for this suite.

â€¢ [SLOW TEST:6.237 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":119,"skipped":2221,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:24:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-7081
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 14:24:54.219: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 11 14:24:54.280: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:24:56.321: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:24:58.295: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:00.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:02.287: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:04.289: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:06.291: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:08.290: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:10.296: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:12.301: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:14.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:16.305: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 11 14:25:16.318: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jun 11 14:25:18.394: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jun 11 14:25:18.395: INFO: Going to poll 192.168.149.74 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jun 11 14:25:18.399: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.149.74:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7081 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:25:18.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:25:18.400: INFO: ExecWithOptions: Clientset creation
Jun 11 14:25:18.401: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7081/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.149.74%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 11 14:25:18.495: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 11 14:25:18.496: INFO: Going to poll 192.168.203.38 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jun 11 14:25:18.502: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.203.38:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7081 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:25:18.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:25:18.503: INFO: ExecWithOptions: Clientset creation
Jun 11 14:25:18.503: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7081/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.203.38%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 11 14:25:18.587: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jun 11 14:25:18.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7081" for this suite.

â€¢ [SLOW TEST:24.477 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2239,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:18.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 11 14:25:19.881: INFO: starting watch
STEP: patching
STEP: updating
Jun 11 14:25:19.899: INFO: waiting for watch events with expected annotations
Jun 11 14:25:19.899: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:25:19.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5030" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":121,"skipped":2250,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:20.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-7939
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 14:25:20.058: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 11 14:25:20.110: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:25:22.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:24.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:26.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:28.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:30.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:32.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:34.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:36.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:38.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:40.119: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:25:42.117: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 11 14:25:42.128: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jun 11 14:25:44.171: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jun 11 14:25:44.171: INFO: Breadth first check of 192.168.149.109 on host 172.31.27.214...
Jun 11 14:25:44.175: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.203.61:9080/dial?request=hostname&protocol=udp&host=192.168.149.109&port=8081&tries=1'] Namespace:pod-network-test-7939 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:25:44.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:25:44.176: INFO: ExecWithOptions: Clientset creation
Jun 11 14:25:44.176: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.203.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.149.109%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 11 14:25:44.267: INFO: Waiting for responses: map[]
Jun 11 14:25:44.268: INFO: reached 192.168.149.109 after 0/1 tries
Jun 11 14:25:44.268: INFO: Breadth first check of 192.168.203.22 on host 172.31.41.158...
Jun 11 14:25:44.272: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.203.61:9080/dial?request=hostname&protocol=udp&host=192.168.203.22&port=8081&tries=1'] Namespace:pod-network-test-7939 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:25:44.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:25:44.273: INFO: ExecWithOptions: Clientset creation
Jun 11 14:25:44.273: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7939/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.203.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.203.22%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 11 14:25:44.373: INFO: Waiting for responses: map[]
Jun 11 14:25:44.374: INFO: reached 192.168.203.22 after 0/1 tries
Jun 11 14:25:44.374: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jun 11 14:25:44.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7939" for this suite.

â€¢ [SLOW TEST:24.386 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":122,"skipped":2256,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:44.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jun 11 14:25:44.438: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun 11 14:25:49.445: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jun 11 14:25:49.449: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 14:25:49.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1178" for this suite.

â€¢ [SLOW TEST:5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":123,"skipped":2268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:49.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-90e0ed44-f398-4476-82f4-f2a05542f86e
STEP: Creating a pod to test consume secrets
Jun 11 14:25:49.566: INFO: Waiting up to 5m0s for pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56" in namespace "secrets-5131" to be "Succeeded or Failed"
Jun 11 14:25:49.571: INFO: Pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.225929ms
Jun 11 14:25:51.581: INFO: Pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56": Phase="Running", Reason="", readiness=true. Elapsed: 2.014986716s
Jun 11 14:25:53.589: INFO: Pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56": Phase="Running", Reason="", readiness=false. Elapsed: 4.023671491s
Jun 11 14:25:55.596: INFO: Pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030136226s
STEP: Saw pod success
Jun 11 14:25:55.596: INFO: Pod "pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56" satisfied condition "Succeeded or Failed"
Jun 11 14:25:55.600: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56 container secret-env-test: <nil>
STEP: delete the pod
Jun 11 14:25:55.633: INFO: Waiting for pod pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56 to disappear
Jun 11 14:25:55.643: INFO: Pod pod-secrets-f50599a8-4b90-4ebf-9280-dfef792edc56 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:25:55.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5131" for this suite.

â€¢ [SLOW TEST:6.170 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":124,"skipped":2310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:55.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 11 14:25:55.711: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 11 14:25:55.725: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 14:25:55.729: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-27-214 before test
Jun 11 14:25:55.749: INFO: calico-kube-controllers-68884f975d-h49sk from calico-system started at 2022-06-11 13:45:50 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 11 14:25:55.749: INFO: calico-node-6kzfb from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:25:55.749: INFO: calico-typha-7bb48cc99f-25j2w from calico-system started at 2022-06-11 13:45:39 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:25:55.749: INFO: kube-proxy-9l4fz from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:25:55.749: INFO: nginx-proxy-ip-172-31-27-214 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:25:55.749: INFO: sonobuoy-e2e-job-f40c26145eda47fd from sonobuoy started at 2022-06-11 13:53:19 +0000 UTC (2 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container e2e ready: true, restart count 0
Jun 11 14:25:55.749: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:25:55.749: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-pjdgw from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:25:55.749: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 14:25:55.749: INFO: tigera-operator-5fb55776df-zr59s from tigera-operator started at 2022-06-11 13:45:20 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.749: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 11 14:25:55.749: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-41-158 before test
Jun 11 14:25:55.760: INFO: calico-apiserver-697d674bb5-dzsj2 from calico-apiserver started at 2022-06-11 13:46:33 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.760: INFO: 	Container calico-apiserver ready: true, restart count 0
Jun 11 14:25:55.760: INFO: calico-node-7hz6q from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:25:55.761: INFO: calico-typha-7bb48cc99f-nt5gz from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:25:55.761: INFO: kube-proxy-ljlz6 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:25:55.761: INFO: nginx-proxy-ip-172-31-41-158 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:25:55.761: INFO: sonobuoy from sonobuoy started at 2022-06-11 13:53:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 14:25:55.761: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-gzshn from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:25:55.761: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:25:55.761: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e098a180-ada6-456f-926f-f2a3907cd761 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e098a180-ada6-456f-926f-f2a3907cd761 off the node ip-172-31-41-158
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e098a180-ada6-456f-926f-f2a3907cd761
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:25:59.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3190" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":125,"skipped":2358,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:25:59.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-cd3f9847-3e0e-4f9d-a399-c499236e3d4b
STEP: Creating a pod to test consume configMaps
Jun 11 14:25:59.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495" in namespace "projected-5817" to be "Succeeded or Failed"
Jun 11 14:25:59.979: INFO: Pod "pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495": Phase="Pending", Reason="", readiness=false. Elapsed: 8.784484ms
Jun 11 14:26:01.988: INFO: Pod "pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017693134s
Jun 11 14:26:04.007: INFO: Pod "pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036511227s
STEP: Saw pod success
Jun 11 14:26:04.007: INFO: Pod "pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495" satisfied condition "Succeeded or Failed"
Jun 11 14:26:04.024: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 14:26:04.067: INFO: Waiting for pod pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495 to disappear
Jun 11 14:26:04.073: INFO: Pod pod-projected-configmaps-714c25bc-0d76-45aa-9779-f284483f3495 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 14:26:04.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5817" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4071
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4071
STEP: creating replication controller externalsvc in namespace services-4071
I0611 14:26:04.199966      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4071, replica count: 2
I0611 14:26:07.253351      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 11 14:26:07.302: INFO: Creating new exec pod
Jun 11 14:26:09.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4071 exec execpodgd6tf -- /bin/sh -x -c nslookup nodeport-service.services-4071.svc.cluster.local'
Jun 11 14:26:09.574: INFO: stderr: "+ nslookup nodeport-service.services-4071.svc.cluster.local\n"
Jun 11 14:26:09.574: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4071.svc.cluster.local\tcanonical name = externalsvc.services-4071.svc.cluster.local.\nName:\texternalsvc.services-4071.svc.cluster.local\nAddress: 10.98.56.63\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4071, will wait for the garbage collector to delete the pods
Jun 11 14:26:09.641: INFO: Deleting ReplicationController externalsvc took: 9.20174ms
Jun 11 14:26:09.741: INFO: Terminating ReplicationController externalsvc pods took: 100.469549ms
Jun 11 14:26:12.178: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:26:12.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4071" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:8.133 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":127,"skipped":2410,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:12.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:26:12.620: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:26:15.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 11 14:26:17.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=webhook-7312 attach --namespace=webhook-7312 to-be-attached-pod -i -c=container1'
Jun 11 14:26:17.946: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:26:17.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7312" for this suite.
STEP: Destroying namespace "webhook-7312-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:5.934 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":128,"skipped":2411,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:18.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jun 11 14:26:18.271: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:26:20.290: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jun 11 14:26:20.313: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:26:22.321: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 11 14:26:22.384: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 14:26:22.389: INFO: Pod pod-with-poststart-http-hook still exists
Jun 11 14:26:24.390: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 14:26:24.396: INFO: Pod pod-with-poststart-http-hook still exists
Jun 11 14:26:26.391: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 11 14:26:26.408: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jun 11 14:26:26.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-218" for this suite.

â€¢ [SLOW TEST:8.316 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2429,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:26.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jun 11 14:26:29.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7140" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":130,"skipped":2445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:29.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jun 11 14:26:29.831: INFO: Creating simple deployment test-deployment-lpz8p
Jun 11 14:26:29.852: INFO: deployment "test-deployment-lpz8p" doesn't have the required revision set
STEP: Getting /status
Jun 11 14:26:31.883: INFO: Deployment test-deployment-lpz8p has Conditions: [{Available True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpz8p-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jun 11 14:26:31.906: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 26, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 26, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 26, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 26, 29, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lpz8p-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jun 11 14:26:31.910: INFO: Observed &Deployment event: ADDED
Jun 11 14:26:31.910: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpz8p-688c4d6789"}
Jun 11 14:26:31.910: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.910: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpz8p-688c4d6789"}
Jun 11 14:26:31.910: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 11 14:26:31.910: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpz8p-688c4d6789" is progressing.}
Jun 11 14:26:31.911: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpz8p-688c4d6789" has successfully progressed.}
Jun 11 14:26:31.911: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 11 14:26:31.911: INFO: Observed Deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpz8p-688c4d6789" has successfully progressed.}
Jun 11 14:26:31.911: INFO: Found Deployment test-deployment-lpz8p in namespace deployment-9587 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 11 14:26:31.911: INFO: Deployment test-deployment-lpz8p has an updated status
STEP: patching the Statefulset Status
Jun 11 14:26:31.911: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 11 14:26:31.926: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jun 11 14:26:31.931: INFO: Observed &Deployment event: ADDED
Jun 11 14:26:31.931: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpz8p-688c4d6789"}
Jun 11 14:26:31.932: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.932: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lpz8p-688c4d6789"}
Jun 11 14:26:31.932: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 11 14:26:31.932: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.933: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 11 14:26:31.933: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:29 +0000 UTC 2022-06-11 14:26:29 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lpz8p-688c4d6789" is progressing.}
Jun 11 14:26:31.933: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.933: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 11 14:26:31.933: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpz8p-688c4d6789" has successfully progressed.}
Jun 11 14:26:31.934: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.934: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 11 14:26:31.934: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-11 14:26:31 +0000 UTC 2022-06-11 14:26:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lpz8p-688c4d6789" has successfully progressed.}
Jun 11 14:26:31.934: INFO: Observed deployment test-deployment-lpz8p in namespace deployment-9587 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 11 14:26:31.934: INFO: Observed &Deployment event: MODIFIED
Jun 11 14:26:31.934: INFO: Found deployment test-deployment-lpz8p in namespace deployment-9587 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 11 14:26:31.934: INFO: Deployment test-deployment-lpz8p has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:26:31.940: INFO: Deployment "test-deployment-lpz8p":
&Deployment{ObjectMeta:{test-deployment-lpz8p  deployment-9587  75ff01c9-8522-4f37-a391-16660c2cf294 21369 1 2022-06-11 14:26:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-11 14:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-06-11 14:26:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-06-11 14:26:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049f68b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lpz8p-688c4d6789",LastUpdateTime:2022-06-11 14:26:31 +0000 UTC,LastTransitionTime:2022-06-11 14:26:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 14:26:31.949: INFO: New ReplicaSet "test-deployment-lpz8p-688c4d6789" of Deployment "test-deployment-lpz8p":
&ReplicaSet{ObjectMeta:{test-deployment-lpz8p-688c4d6789  deployment-9587  4d6404a9-12d0-44c1-82ed-33b06c99aae5 21348 1 2022-06-11 14:26:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lpz8p 75ff01c9-8522-4f37-a391-16660c2cf294 0xc0049f6c50 0xc0049f6c51}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75ff01c9-8522-4f37-a391-16660c2cf294\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:26:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049f6cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:26:31.960: INFO: Pod "test-deployment-lpz8p-688c4d6789-sl88q" is available:
&Pod{ObjectMeta:{test-deployment-lpz8p-688c4d6789-sl88q test-deployment-lpz8p-688c4d6789- deployment-9587  3d3b68a2-de20-40a5-a209-d30be63235bf 21347 0 2022-06-11 14:26:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:c5aca3add7d86c7da93c4b4d287bfb60eee274ec1647547d791056d5805db77e cni.projectcalico.org/podIP:192.168.203.18/32 cni.projectcalico.org/podIPs:192.168.203.18/32] [{apps/v1 ReplicaSet test-deployment-lpz8p-688c4d6789 4d6404a9-12d0-44c1-82ed-33b06c99aae5 0xc004e63237 0xc004e63238}] []  [{kube-controller-manager Update v1 2022-06-11 14:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d6404a9-12d0-44c1-82ed-33b06c99aae5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:26:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:26:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqb6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqb6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.18,StartTime:2022-06-11 14:26:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:26:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://01d5d345bd56ca3e4d0c56868af50b0d99dab9c7799628e268264301b230b865,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:26:31.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9587" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":131,"skipped":2473,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:31.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jun 11 14:26:32.053: INFO: Waiting up to 5m0s for pod "downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda" in namespace "downward-api-8731" to be "Succeeded or Failed"
Jun 11 14:26:32.065: INFO: Pod "downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda": Phase="Pending", Reason="", readiness=false. Elapsed: 11.629415ms
Jun 11 14:26:34.074: INFO: Pod "downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020466516s
Jun 11 14:26:36.079: INFO: Pod "downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025616962s
STEP: Saw pod success
Jun 11 14:26:36.079: INFO: Pod "downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda" satisfied condition "Succeeded or Failed"
Jun 11 14:26:36.083: INFO: Trying to get logs from node ip-172-31-41-158 pod downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda container dapi-container: <nil>
STEP: delete the pod
Jun 11 14:26:36.125: INFO: Waiting for pod downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda to disappear
Jun 11 14:26:36.130: INFO: Pod downward-api-46d293b7-c8bb-4fe6-8b49-2330b9f03fda no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jun 11 14:26:36.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8731" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2487,"failed":0}
SS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:36.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jun 11 14:26:36.242: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jun 11 14:26:38.268: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jun 11 14:26:40.304: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jun 11 14:26:42.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9969" for this suite.

â€¢ [SLOW TEST:6.182 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":133,"skipped":2489,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:42.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun 11 14:26:42.385: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.385: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.397: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.397: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.419: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.419: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.491: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:42.491: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 11 14:26:43.749: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 11 14:26:43.749: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 11 14:26:44.198: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun 11 14:26:44.224: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun 11 14:26:44.227: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 0
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:44.228: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:44.229: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.229: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.229: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.229: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.243: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.243: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.282: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.282: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:44.300: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:44.300: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:44.335: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:44.335: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:45.216: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:45.216: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:45.248: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
STEP: listing Deployments
Jun 11 14:26:45.255: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun 11 14:26:45.272: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun 11 14:26:45.281: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:45.292: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:45.348: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:45.374: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:45.397: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:46.225: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:46.308: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:46.331: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 11 14:26:47.775: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun 11 14:26:47.830: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 1
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:47.831: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 2
Jun 11 14:26:47.832: INFO: observed Deployment test-deployment in namespace deployment-3767 with ReadyReplicas 3
STEP: deleting the Deployment
Jun 11 14:26:47.845: INFO: observed event type MODIFIED
Jun 11 14:26:47.845: INFO: observed event type MODIFIED
Jun 11 14:26:47.845: INFO: observed event type MODIFIED
Jun 11 14:26:47.845: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
Jun 11 14:26:47.846: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:26:47.850: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 11 14:26:47.855: INFO: ReplicaSet "test-deployment-6bdc46c995":
&ReplicaSet{ObjectMeta:{test-deployment-6bdc46c995  deployment-3767  2893348a-7d94-493b-aa41-adfea0624557 21577 3 2022-06-11 14:26:42 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 233b2d97-0664-4294-97a6-bce6bfb8db8a 0xc004869f77 0xc004869f78}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233b2d97-0664-4294-97a6-bce6bfb8db8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6bdc46c995,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bdc46c995 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002706000 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 11 14:26:47.860: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-3767  f7f89b58-53bb-40d5-9251-8faceec13149 21675 2 2022-06-11 14:26:45 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 233b2d97-0664-4294-97a6-bce6bfb8db8a 0xc002706067 0xc002706068}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233b2d97-0664-4294-97a6-bce6bfb8db8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:26:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027060f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun 11 14:26:47.869: INFO: pod: "test-deployment-74c6dd549b-px82s":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-px82s test-deployment-74c6dd549b- deployment-3767  92e30d68-c3e7-4223-a343-35412c7e7ed9 21618 0 2022-06-11 14:26:45 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:51e048fbe9209e955094e342e02c96e55db62b38a57293bb9bb14fd13de35fbf cni.projectcalico.org/podIP:192.168.203.14/32 cni.projectcalico.org/podIPs:192.168.203.14/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b f7f89b58-53bb-40d5-9251-8faceec13149 0xc0043884e7 0xc0043884e8}] []  [{Go-http-client Update v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7f89b58-53bb-40d5-9251-8faceec13149\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxnbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxnbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.14,StartTime:2022-06-11 14:26:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:26:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://04d185b84c1ac80a0aa00af51978c280b01db98bfd63b8db7c39d0f4233ffd48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 11 14:26:47.869: INFO: pod: "test-deployment-74c6dd549b-tgmnd":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-tgmnd test-deployment-74c6dd549b- deployment-3767  ad314111-8466-4ae5-aa9f-9e1f7d85398a 21674 0 2022-06-11 14:26:46 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:42d24643e8540e9d2c759d392128f28b92d3202cd4ec2e628d91d36afa29d89b cni.projectcalico.org/podIP:192.168.149.116/32 cni.projectcalico.org/podIPs:192.168.149.116/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b f7f89b58-53bb-40d5-9251-8faceec13149 0xc004388707 0xc004388708}] []  [{Go-http-client Update v1 2022-06-11 14:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7f89b58-53bb-40d5-9251-8faceec13149\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:26:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qkst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qkst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:192.168.149.116,StartTime:2022-06-11 14:26:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:26:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5ffa5717346f94071b35b10e4f37278352eb526cbb2b795ca9d3b8ae6df7c37b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 11 14:26:47.869: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-3767  72ab2e41-ca69-47c0-af06-0e0f112fac70 21683 4 2022-06-11 14:26:44 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 233b2d97-0664-4294-97a6-bce6bfb8db8a 0xc002706157 0xc002706158}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233b2d97-0664-4294-97a6-bce6bfb8db8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:26:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027061e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 11 14:26:47.874: INFO: pod: "test-deployment-84b949bdfc-6thtp":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-6thtp test-deployment-84b949bdfc- deployment-3767  2e0b346c-6a51-4ca7-9b2d-eb6c1301b0a6 21653 0 2022-06-11 14:26:45 +0000 UTC 2022-06-11 14:26:47 +0000 UTC 0xc0027066e8 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:aa35a6a660f3697831cff4f0eac33689fa1b3f285273a3f3e84b01219d6a5f6e cni.projectcalico.org/podIP:192.168.149.126/32 cni.projectcalico.org/podIPs:192.168.149.126/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 72ab2e41-ca69-47c0-af06-0e0f112fac70 0xc002706717 0xc002706718}] []  [{Go-http-client Update v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72ab2e41-ca69-47c0-af06-0e0f112fac70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:26:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccclc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccclc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:192.168.149.126,StartTime:2022-06-11 14:26:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:26:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://9f9f265e66f6d91d7e12ebbd1751b0ce1aba54a6a3c6663112618559c762b249,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 11 14:26:47.875: INFO: pod: "test-deployment-84b949bdfc-xzcrr":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-xzcrr test-deployment-84b949bdfc- deployment-3767  a44fe6f8-b4e0-4973-a0c6-b32192c06fe1 21679 0 2022-06-11 14:26:44 +0000 UTC 2022-06-11 14:26:48 +0000 UTC 0xc002706f70 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:e4713c255bd1c261efbf4e50cc82f48e51f2a1a79179aa8e92d88c19c702f799 cni.projectcalico.org/podIP:192.168.203.58/32 cni.projectcalico.org/podIPs:192.168.203.58/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 72ab2e41-ca69-47c0-af06-0e0f112fac70 0xc002706fa7 0xc002706fa8}] []  [{Go-http-client Update v1 2022-06-11 14:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72ab2e41-ca69-47c0-af06-0e0f112fac70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sc22v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sc22v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:26:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.58,StartTime:2022-06-11 14:26:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://6bcb047d860910aaeafa1404f97179608bf4f7d55d9e3538b079b8f909062d83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:26:47.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3767" for this suite.

â€¢ [SLOW TEST:5.564 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":134,"skipped":2500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:26:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-802a0955-5bc2-4545-baa6-0c1f3ed591e7 in namespace container-probe-1861
Jun 11 14:26:49.963: INFO: Started pod liveness-802a0955-5bc2-4545-baa6-0c1f3ed591e7 in namespace container-probe-1861
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 14:26:49.966: INFO: Initial restart count of pod liveness-802a0955-5bc2-4545-baa6-0c1f3ed591e7 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 14:30:51.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1861" for this suite.

â€¢ [SLOW TEST:243.437 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:30:51.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:30:51.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426" in namespace "projected-6370" to be "Succeeded or Failed"
Jun 11 14:30:51.422: INFO: Pod "downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426": Phase="Pending", Reason="", readiness=false. Elapsed: 18.478509ms
Jun 11 14:30:53.428: INFO: Pod "downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024972005s
Jun 11 14:30:55.434: INFO: Pod "downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030733658s
STEP: Saw pod success
Jun 11 14:30:55.434: INFO: Pod "downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426" satisfied condition "Succeeded or Failed"
Jun 11 14:30:55.441: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426 container client-container: <nil>
STEP: delete the pod
Jun 11 14:30:55.501: INFO: Waiting for pod downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426 to disappear
Jun 11 14:30:55.513: INFO: Pod downwardapi-volume-2aee3b21-5b97-4c0e-9a43-1dead5b72426 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:30:55.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6370" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2580,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:30:55.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jun 11 14:30:55.597: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 14:30:58.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6686" for this suite.
â€¢{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":137,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:30:58.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:31:15.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4786" for this suite.

â€¢ [SLOW TEST:16.220 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":138,"skipped":2639,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:31:15.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:31:15.131: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 11 14:31:20.146: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jun 11 14:31:20.170: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jun 11 14:31:20.209: INFO: observed ReplicaSet test-rs in namespace replicaset-4198 with ReadyReplicas 1, AvailableReplicas 1
Jun 11 14:31:20.224: INFO: observed ReplicaSet test-rs in namespace replicaset-4198 with ReadyReplicas 1, AvailableReplicas 1
Jun 11 14:31:20.251: INFO: observed ReplicaSet test-rs in namespace replicaset-4198 with ReadyReplicas 1, AvailableReplicas 1
Jun 11 14:31:20.280: INFO: observed ReplicaSet test-rs in namespace replicaset-4198 with ReadyReplicas 1, AvailableReplicas 1
Jun 11 14:31:21.433: INFO: observed ReplicaSet test-rs in namespace replicaset-4198 with ReadyReplicas 2, AvailableReplicas 2
Jun 11 14:31:21.867: INFO: observed Replicaset test-rs in namespace replicaset-4198 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 14:31:21.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4198" for this suite.

â€¢ [SLOW TEST:6.825 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":139,"skipped":2642,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:31:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 in namespace container-probe-820
Jun 11 14:31:23.967: INFO: Started pod liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 in namespace container-probe-820
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 14:31:23.973: INFO: Initial restart count of pod liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is 0
Jun 11 14:31:44.082: INFO: Restart count of pod container-probe-820/liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is now 1 (20.108220736s elapsed)
Jun 11 14:32:04.207: INFO: Restart count of pod container-probe-820/liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is now 2 (40.233758925s elapsed)
Jun 11 14:32:24.327: INFO: Restart count of pod container-probe-820/liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is now 3 (1m0.353651698s elapsed)
Jun 11 14:32:44.417: INFO: Restart count of pod container-probe-820/liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is now 4 (1m20.443646974s elapsed)
Jun 11 14:33:46.793: INFO: Restart count of pod container-probe-820/liveness-dd6043a3-18d4-4691-a786-1b81d450eaf3 is now 5 (2m22.819059721s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 14:33:46.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-820" for this suite.

â€¢ [SLOW TEST:144.949 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":140,"skipped":2659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:33:46.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jun 11 14:33:46.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-8102 cluster-info'
Jun 11 14:33:47.086: INFO: stderr: ""
Jun 11 14:33:47.086: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:33:47.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8102" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":141,"skipped":2701,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:33:47.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jun 11 14:33:47.154: INFO: namespace kubectl-1474
Jun 11 14:33:47.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1474 create -f -'
Jun 11 14:33:47.510: INFO: stderr: ""
Jun 11 14:33:47.510: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 11 14:33:48.517: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 14:33:48.517: INFO: Found 0 / 1
Jun 11 14:33:49.519: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 14:33:49.519: INFO: Found 1 / 1
Jun 11 14:33:49.519: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 11 14:33:49.525: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 14:33:49.525: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 14:33:49.525: INFO: wait on agnhost-primary startup in kubectl-1474 
Jun 11 14:33:49.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1474 logs agnhost-primary-bx2kp agnhost-primary'
Jun 11 14:33:49.631: INFO: stderr: ""
Jun 11 14:33:49.631: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 11 14:33:49.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1474 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 11 14:33:49.739: INFO: stderr: ""
Jun 11 14:33:49.739: INFO: stdout: "service/rm2 exposed\n"
Jun 11 14:33:49.754: INFO: Service rm2 in namespace kubectl-1474 found.
STEP: exposing service
Jun 11 14:33:51.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-1474 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 11 14:33:51.907: INFO: stderr: ""
Jun 11 14:33:51.907: INFO: stdout: "service/rm3 exposed\n"
Jun 11 14:33:51.917: INFO: Service rm3 in namespace kubectl-1474 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:33:53.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1474" for this suite.

â€¢ [SLOW TEST:6.847 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":142,"skipped":2714,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:33:53.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:33:54.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b" in namespace "projected-6190" to be "Succeeded or Failed"
Jun 11 14:33:54.058: INFO: Pod "downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.432989ms
Jun 11 14:33:56.067: INFO: Pod "downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02511898s
Jun 11 14:33:58.075: INFO: Pod "downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033067855s
STEP: Saw pod success
Jun 11 14:33:58.075: INFO: Pod "downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b" satisfied condition "Succeeded or Failed"
Jun 11 14:33:58.079: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b container client-container: <nil>
STEP: delete the pod
Jun 11 14:33:58.113: INFO: Waiting for pod downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b to disappear
Jun 11 14:33:58.127: INFO: Pod downwardapi-volume-7488d55d-80e8-48d8-bff8-3105dbc4da9b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:33:58.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6190" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:33:58.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:33:58.201: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 11 14:33:58.216: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 11 14:34:03.223: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 14:34:03.223: INFO: Creating deployment "test-rolling-update-deployment"
Jun 11 14:34:03.231: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 11 14:34:03.239: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 11 14:34:05.251: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 11 14:34:05.256: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:34:05.271: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3899  676c39dd-7835-418e-a8a0-c3cf240de80e 23411 1 2022-06-11 14:34:03 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-06-11 14:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002dfe5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-11 14:34:03 +0000 UTC,LastTransitionTime:2022-06-11 14:34:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-5579c56cf8" has successfully progressed.,LastUpdateTime:2022-06-11 14:34:04 +0000 UTC,LastTransitionTime:2022-06-11 14:34:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 14:34:05.280: INFO: New ReplicaSet "test-rolling-update-deployment-5579c56cf8" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-5579c56cf8  deployment-3899  889a41b8-7abc-4b91-89f9-29d71006f619 23401 1 2022-06-11 14:34:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 676c39dd-7835-418e-a8a0-c3cf240de80e 0xc002dff017 0xc002dff018}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"676c39dd-7835-418e-a8a0-c3cf240de80e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:34:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5579c56cf8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002dff0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:34:05.280: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 11 14:34:05.280: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3899  f43e567a-198a-4bf2-b174-a5a1f4b5ac1c 23410 2 2022-06-11 14:33:58 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 676c39dd-7835-418e-a8a0-c3cf240de80e 0xc002dfeed7 0xc002dfeed8}] []  [{e2e.test Update apps/v1 2022-06-11 14:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"676c39dd-7835-418e-a8a0-c3cf240de80e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:34:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002dfef98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:34:05.292: INFO: Pod "test-rolling-update-deployment-5579c56cf8-lzbch" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-5579c56cf8-lzbch test-rolling-update-deployment-5579c56cf8- deployment-3899  5ec4ff10-3917-4913-813e-52c75c8d04d6 23400 0 2022-06-11 14:34:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:5579c56cf8] map[cni.projectcalico.org/containerID:63b4d8a2c3d170c7e2314505ce7cfdfccaba8189bb74e773f97ca735739a1cd2 cni.projectcalico.org/podIP:192.168.203.17/32 cni.projectcalico.org/podIPs:192.168.203.17/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-5579c56cf8 889a41b8-7abc-4b91-89f9-29d71006f619 0xc006b30197 0xc006b30198}] []  [{Go-http-client Update v1 2022-06-11 14:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"889a41b8-7abc-4b91-89f9-29d71006f619\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:34:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk4j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk4j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:34:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:34:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:34:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:34:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.17,StartTime:2022-06-11 14:34:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:34:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8,ContainerID:containerd://093905260f9514d4ef022b7f8532b9d67f3215d04289a66528fcb05d7cb34217,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:34:05.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3899" for this suite.

â€¢ [SLOW TEST:7.158 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":144,"skipped":2769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:05.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jun 11 14:34:05.357: INFO: Creating e2e-svc-a-qn74m
Jun 11 14:34:05.390: INFO: Creating e2e-svc-b-z4bw6
Jun 11 14:34:05.427: INFO: Creating e2e-svc-c-kl6hr
STEP: deleting service collection
Jun 11 14:34:05.544: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:34:05.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5527" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
â€¢{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":145,"skipped":2807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:05.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:34:06.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:34:09.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:34:09.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3124-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:34:12.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6535" for this suite.
STEP: Destroying namespace "webhook-6535-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:7.038 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":146,"skipped":2956,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:12.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:34:12.694: INFO: Creating ReplicaSet my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603
Jun 11 14:34:12.782: INFO: Pod name my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603: Found 1 pods out of 1
Jun 11 14:34:12.782: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603" is running
Jun 11 14:34:14.813: INFO: Pod "my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603-lx9wt" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-11 14:34:12 +0000 UTC Reason: Message:}])
Jun 11 14:34:14.813: INFO: Trying to dial the pod
Jun 11 14:34:19.837: INFO: Controller my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603: Got expected result from replica 1 [my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603-lx9wt]: "my-hostname-basic-158a067d-9e57-404e-97ed-76edf77b7603-lx9wt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 14:34:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3273" for this suite.

â€¢ [SLOW TEST:7.241 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":147,"skipped":2956,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4954.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4954.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4954.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 14:34:21.975: INFO: DNS probes using dns-4954/dns-test-7fe87f3b-450a-495a-a4fa-2b921eddbfcf succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 14:34:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4954" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":148,"skipped":2967,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:22.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:34:28.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2302" for this suite.
STEP: Destroying namespace "nsdeletetest-3609" for this suite.
Jun 11 14:34:28.219: INFO: Namespace nsdeletetest-3609 was already deleted
STEP: Destroying namespace "nsdeletetest-3244" for this suite.

â€¢ [SLOW TEST:6.184 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":149,"skipped":2976,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:28.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9826
STEP: creating service affinity-clusterip in namespace services-9826
STEP: creating replication controller affinity-clusterip in namespace services-9826
I0611 14:34:28.306272      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9826, replica count: 3
I0611 14:34:31.357543      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 14:34:31.377: INFO: Creating new exec pod
Jun 11 14:34:34.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9826 exec execpod-affinitycbmgs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun 11 14:34:34.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 11 14:34:34.625: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:34:34.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9826 exec execpod-affinitycbmgs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.154.216 80'
Jun 11 14:34:34.885: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.154.216 80\nConnection to 10.107.154.216 80 port [tcp/http] succeeded!\n"
Jun 11 14:34:34.885: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:34:34.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9826 exec execpod-affinitycbmgs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.154.216:80/ ; done'
Jun 11 14:34:35.185: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.154.216:80/\n"
Jun 11 14:34:35.185: INFO: stdout: "\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm\naffinity-clusterip-sb5sm"
Jun 11 14:34:35.185: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.185: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.185: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Received response from host: affinity-clusterip-sb5sm
Jun 11 14:34:35.186: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9826, will wait for the garbage collector to delete the pods
Jun 11 14:34:35.311: INFO: Deleting ReplicationController affinity-clusterip took: 25.186ms
Jun 11 14:34:35.412: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.18662ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:34:37.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9826" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:9.429 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":150,"skipped":2985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:37.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:34:37.740: INFO: The status of Pod busybox-scheduling-e0895e23-0fe9-45b4-894f-c6dc13bb49aa is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:34:39.754: INFO: The status of Pod busybox-scheduling-e0895e23-0fe9-45b4-894f-c6dc13bb49aa is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jun 11 14:34:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4955" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":151,"skipped":3074,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 11 14:34:39.892: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 11 14:34:39.902: INFO: starting watch
STEP: patching
STEP: updating
Jun 11 14:34:39.936: INFO: waiting for watch events with expected annotations
Jun 11 14:34:39.937: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jun 11 14:34:40.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5753" for this suite.
â€¢{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":152,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:40.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:34:40.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2" in namespace "downward-api-7079" to be "Succeeded or Failed"
Jun 11 14:34:40.113: INFO: Pod "downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.6406ms
Jun 11 14:34:42.123: INFO: Pod "downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020096284s
Jun 11 14:34:44.137: INFO: Pod "downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03408884s
STEP: Saw pod success
Jun 11 14:34:44.137: INFO: Pod "downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2" satisfied condition "Succeeded or Failed"
Jun 11 14:34:44.142: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2 container client-container: <nil>
STEP: delete the pod
Jun 11 14:34:44.170: INFO: Waiting for pod downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2 to disappear
Jun 11 14:34:44.176: INFO: Pod downwardapi-volume-b0b143bf-71a7-4b0d-b3eb-1024541e6bb2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:34:44.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7079" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":3115,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 11 14:34:44.257: INFO: Waiting up to 5m0s for pod "pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3" in namespace "emptydir-6726" to be "Succeeded or Failed"
Jun 11 14:34:44.265: INFO: Pod "pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941359ms
Jun 11 14:34:46.276: INFO: Pod "pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019147565s
Jun 11 14:34:48.290: INFO: Pod "pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032526447s
STEP: Saw pod success
Jun 11 14:34:48.290: INFO: Pod "pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3" satisfied condition "Succeeded or Failed"
Jun 11 14:34:48.300: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3 container test-container: <nil>
STEP: delete the pod
Jun 11 14:34:48.336: INFO: Waiting for pod pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3 to disappear
Jun 11 14:34:48.344: INFO: Pod pod-0c4d228a-b253-46e0-be6a-55d3d19c1df3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:34:48.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6726" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":154,"skipped":3126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:48.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jun 11 14:34:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5654" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":155,"skipped":3176,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:34:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jun 11 14:39:48.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1057" for this suite.

â€¢ [SLOW TEST:300.090 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":156,"skipped":3178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:39:48.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jun 11 14:39:50.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-75" for this suite.
â€¢{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":3216,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:39:50.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:39:50.918: INFO: Creating pod...
Jun 11 14:39:52.941: INFO: Creating service...
Jun 11 14:39:52.966: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=DELETE
Jun 11 14:39:52.976: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 11 14:39:52.976: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=OPTIONS
Jun 11 14:39:52.988: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 11 14:39:52.989: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=PATCH
Jun 11 14:39:52.999: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 11 14:39:53.000: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=POST
Jun 11 14:39:53.011: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 11 14:39:53.011: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=PUT
Jun 11 14:39:53.018: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 11 14:39:53.018: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=DELETE
Jun 11 14:39:53.027: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 11 14:39:53.027: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun 11 14:39:53.036: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 11 14:39:53.038: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=PATCH
Jun 11 14:39:53.047: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 11 14:39:53.048: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=POST
Jun 11 14:39:53.056: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 11 14:39:53.056: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=PUT
Jun 11 14:39:53.065: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 11 14:39:53.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=GET
Jun 11 14:39:53.070: INFO: http.Client request:GET StatusCode:301
Jun 11 14:39:53.071: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=GET
Jun 11 14:39:53.092: INFO: http.Client request:GET StatusCode:301
Jun 11 14:39:53.092: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/pods/agnhost/proxy?method=HEAD
Jun 11 14:39:53.096: INFO: http.Client request:HEAD StatusCode:301
Jun 11 14:39:53.096: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-133/services/e2e-proxy-test-service/proxy?method=HEAD
Jun 11 14:39:53.103: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jun 11 14:39:53.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-133" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":158,"skipped":3230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:39:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-553ce372-f669-4934-8ffd-8b71de941510 in namespace container-probe-3754
Jun 11 14:39:55.205: INFO: Started pod busybox-553ce372-f669-4934-8ffd-8b71de941510 in namespace container-probe-3754
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 14:39:55.210: INFO: Initial restart count of pod busybox-553ce372-f669-4934-8ffd-8b71de941510 is 0
Jun 11 14:40:45.491: INFO: Restart count of pod container-probe-3754/busybox-553ce372-f669-4934-8ffd-8b71de941510 is now 1 (50.281197537s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 14:40:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3754" for this suite.

â€¢ [SLOW TEST:52.406 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":3259,"failed":0}
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:40:45.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 11 14:40:47.623: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7071 PodName:var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:40:47.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:40:47.624: INFO: ExecWithOptions: Clientset creation
Jun 11 14:40:47.625: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7071/pods/var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jun 11 14:40:47.720: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7071 PodName:var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:40:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:40:47.727: INFO: ExecWithOptions: Clientset creation
Jun 11 14:40:47.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7071/pods/var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jun 11 14:40:48.332: INFO: Successfully updated pod "var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 11 14:40:48.341: INFO: Deleting pod "var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09" in namespace "var-expansion-7071"
Jun 11 14:40:48.355: INFO: Wait up to 5m0s for pod "var-expansion-c3806df9-3bc0-44eb-8387-cf1514a03f09" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 14:41:22.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7071" for this suite.

â€¢ [SLOW TEST:36.846 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":160,"skipped":3259,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:41:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:41:35.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8729" for this suite.
STEP: Destroying namespace "nsdeletetest-8351" for this suite.
Jun 11 14:41:35.539: INFO: Namespace nsdeletetest-8351 was already deleted
STEP: Destroying namespace "nsdeletetest-5447" for this suite.

â€¢ [SLOW TEST:13.166 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":161,"skipped":3260,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:41:35.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-b9a2daff-12d7-4546-a0f2-7ab81a23c8ab
STEP: Creating secret with name s-test-opt-upd-4e4bd190-f985-4911-bb32-fe820cb7cbdd
STEP: Creating the pod
Jun 11 14:41:35.633: INFO: The status of Pod pod-secrets-218a717b-d87f-43c9-90cb-4867ce7681b8 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:41:37.640: INFO: The status of Pod pod-secrets-218a717b-d87f-43c9-90cb-4867ce7681b8 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-b9a2daff-12d7-4546-a0f2-7ab81a23c8ab
STEP: Updating secret s-test-opt-upd-4e4bd190-f985-4911-bb32-fe820cb7cbdd
STEP: Creating secret with name s-test-opt-create-bd74cfba-7580-4857-a3b3-8afe50709dd6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:41:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3533" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":162,"skipped":3280,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:41:39.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3010.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3010.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 14.152.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.152.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.152.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.152.14_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3010.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3010.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3010.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3010.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3010.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 14.152.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.152.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.152.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.152.14_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 14:41:49.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.891: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.897: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.903: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.931: INFO: Unable to read jessie_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.937: INFO: Unable to read jessie_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.942: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:49.966: INFO: Lookups using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b failed for: [wheezy_udp@dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_udp@dns-test-service.dns-3010.svc.cluster.local jessie_tcp@dns-test-service.dns-3010.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local]

Jun 11 14:41:54.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:54.979: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:54.984: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:54.990: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:55.022: INFO: Unable to read jessie_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:55.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:55.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:55.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:55.065: INFO: Lookups using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b failed for: [wheezy_udp@dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_udp@dns-test-service.dns-3010.svc.cluster.local jessie_tcp@dns-test-service.dns-3010.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local]

Jun 11 14:41:59.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:59.978: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:59.985: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:41:59.990: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:00.023: INFO: Unable to read jessie_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:00.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:00.034: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:00.038: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:00.059: INFO: Lookups using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b failed for: [wheezy_udp@dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_udp@dns-test-service.dns-3010.svc.cluster.local jessie_tcp@dns-test-service.dns-3010.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local]

Jun 11 14:42:04.973: INFO: Unable to read wheezy_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:04.979: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:04.984: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:04.989: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:05.016: INFO: Unable to read jessie_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:05.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:05.028: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:05.034: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:05.055: INFO: Lookups using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b failed for: [wheezy_udp@dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_udp@dns-test-service.dns-3010.svc.cluster.local jessie_tcp@dns-test-service.dns-3010.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local]

Jun 11 14:42:09.977: INFO: Unable to read wheezy_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:09.984: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:09.992: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.002: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.037: INFO: Unable to read jessie_udp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.048: INFO: Unable to read jessie_tcp@dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.056: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local from pod dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b: the server could not find the requested resource (get pods dns-test-716743c4-7167-431a-9c95-b1ccf173356b)
Jun 11 14:42:10.108: INFO: Lookups using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b failed for: [wheezy_udp@dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@dns-test-service.dns-3010.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_udp@dns-test-service.dns-3010.svc.cluster.local jessie_tcp@dns-test-service.dns-3010.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3010.svc.cluster.local]

Jun 11 14:42:15.064: INFO: DNS probes using dns-3010/dns-test-716743c4-7167-431a-9c95-b1ccf173356b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 14:42:15.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3010" for this suite.

â€¢ [SLOW TEST:35.445 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":163,"skipped":3280,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:15.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jun 11 14:42:15.301: INFO: The status of Pod labelsupdated327ba06-6cf2-44e6-ad31-2623b81246da is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:42:17.308: INFO: The status of Pod labelsupdated327ba06-6cf2-44e6-ad31-2623b81246da is Running (Ready = true)
Jun 11 14:42:17.848: INFO: Successfully updated pod "labelsupdated327ba06-6cf2-44e6-ad31-2623b81246da"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:42:21.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9136" for this suite.

â€¢ [SLOW TEST:6.687 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":164,"skipped":3281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jun 11 14:42:26.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7717" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":165,"skipped":3334,"failed":0}
SSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jun 11 14:42:26.121: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:42:28.134: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.41.158 on the node which pod1 resides and expect scheduled
Jun 11 14:42:28.152: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:42:30.162: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.41.158 but use UDP protocol on the node which pod2 resides
Jun 11 14:42:30.179: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:42:32.186: INFO: The status of Pod pod3 is Running (Ready = false)
Jun 11 14:42:34.187: INFO: The status of Pod pod3 is Running (Ready = true)
Jun 11 14:42:34.201: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:42:36.211: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jun 11 14:42:36.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.41.158 http://127.0.0.1:54323/hostname] Namespace:hostport-7917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:42:36.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:42:36.219: INFO: ExecWithOptions: Clientset creation
Jun 11 14:42:36.219: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7917/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.41.158+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.41.158, port: 54323
Jun 11 14:42:36.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.41.158:54323/hostname] Namespace:hostport-7917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:42:36.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:42:36.311: INFO: ExecWithOptions: Clientset creation
Jun 11 14:42:36.311: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7917/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.41.158%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.41.158, port: 54323 UDP
Jun 11 14:42:36.428: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.41.158 54323] Namespace:hostport-7917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:42:36.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:42:36.430: INFO: ExecWithOptions: Clientset creation
Jun 11 14:42:36.430: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7917/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.31.41.158+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jun 11 14:42:41.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7917" for this suite.

â€¢ [SLOW TEST:15.528 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":166,"skipped":3341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:41.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:42:41.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e" in namespace "downward-api-2413" to be "Succeeded or Failed"
Jun 11 14:42:41.690: INFO: Pod "downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814218ms
Jun 11 14:42:43.697: INFO: Pod "downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012765273s
Jun 11 14:42:45.706: INFO: Pod "downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021790499s
STEP: Saw pod success
Jun 11 14:42:45.706: INFO: Pod "downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e" satisfied condition "Succeeded or Failed"
Jun 11 14:42:45.713: INFO: Trying to get logs from node ip-172-31-27-214 pod downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e container client-container: <nil>
STEP: delete the pod
Jun 11 14:42:45.763: INFO: Waiting for pod downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e to disappear
Jun 11 14:42:45.769: INFO: Pod downwardapi-volume-9c87be51-2a42-4b6f-9695-ede6f489ca1e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:42:45.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2413" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":3382,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:45.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jun 11 14:42:45.852: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 11 14:42:50.862: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jun 11 14:42:50.867: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jun 11 14:42:50.880: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jun 11 14:42:50.883: INFO: Observed &ReplicaSet event: ADDED
Jun 11 14:42:50.883: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.883: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.883: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.884: INFO: Found replicaset test-rs in namespace replicaset-73 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 11 14:42:50.884: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jun 11 14:42:50.884: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 11 14:42:50.891: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jun 11 14:42:50.895: INFO: Observed &ReplicaSet event: ADDED
Jun 11 14:42:50.895: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.895: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.897: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.897: INFO: Observed replicaset test-rs in namespace replicaset-73 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 11 14:42:50.898: INFO: Observed &ReplicaSet event: MODIFIED
Jun 11 14:42:50.898: INFO: Found replicaset test-rs in namespace replicaset-73 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 11 14:42:50.898: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 14:42:50.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-73" for this suite.

â€¢ [SLOW TEST:5.127 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":168,"skipped":3392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:42:50.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jun 11 14:44:00.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2136" for this suite.

â€¢ [SLOW TEST:70.081 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":169,"skipped":3459,"failed":0}
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:00.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jun 11 14:44:03.082: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jun 11 14:44:05.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9165" for this suite.
â€¢{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":170,"skipped":3459,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:05.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 11 14:44:05.196: INFO: Waiting up to 5m0s for pod "pod-911b164f-cb37-4b45-be28-b6676f20ad27" in namespace "emptydir-9674" to be "Succeeded or Failed"
Jun 11 14:44:05.200: INFO: Pod "pod-911b164f-cb37-4b45-be28-b6676f20ad27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.132272ms
Jun 11 14:44:07.207: INFO: Pod "pod-911b164f-cb37-4b45-be28-b6676f20ad27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011423074s
Jun 11 14:44:09.220: INFO: Pod "pod-911b164f-cb37-4b45-be28-b6676f20ad27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024218263s
STEP: Saw pod success
Jun 11 14:44:09.220: INFO: Pod "pod-911b164f-cb37-4b45-be28-b6676f20ad27" satisfied condition "Succeeded or Failed"
Jun 11 14:44:09.226: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-911b164f-cb37-4b45-be28-b6676f20ad27 container test-container: <nil>
STEP: delete the pod
Jun 11 14:44:09.264: INFO: Waiting for pod pod-911b164f-cb37-4b45-be28-b6676f20ad27 to disappear
Jun 11 14:44:09.269: INFO: Pod pod-911b164f-cb37-4b45-be28-b6676f20ad27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:44:09.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9674" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":171,"skipped":3472,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:09.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:44:09.345: INFO: Creating deployment "webserver-deployment"
Jun 11 14:44:09.353: INFO: Waiting for observed generation 1
Jun 11 14:44:11.371: INFO: Waiting for all required pods to come up
Jun 11 14:44:11.381: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 11 14:44:13.407: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 11 14:44:13.416: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 11 14:44:13.438: INFO: Updating deployment webserver-deployment
Jun 11 14:44:13.438: INFO: Waiting for observed generation 2
Jun 11 14:44:15.456: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 11 14:44:15.463: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 11 14:44:15.468: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 11 14:44:15.486: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 11 14:44:15.486: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 11 14:44:15.491: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 11 14:44:15.504: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 11 14:44:15.505: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 11 14:44:15.519: INFO: Updating deployment webserver-deployment
Jun 11 14:44:15.519: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 11 14:44:15.533: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 11 14:44:15.542: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:44:17.565: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8725  132632dd-a1e1-4cb5-b683-5cbe77e1c875 26627 3 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b30b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-11 14:44:15 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-06-11 14:44:15 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 11 14:44:17.571: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-8725  c74032c5-abd0-4c05-810e-e74d2057368e 26623 3 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 132632dd-a1e1-4cb5-b683-5cbe77e1c875 0xc004869257 0xc004869258}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"132632dd-a1e1-4cb5-b683-5cbe77e1c875\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048692f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:44:17.571: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 11 14:44:17.571: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-8725  13d3ba17-3f10-412d-ae84-83e28145afea 26608 3 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 132632dd-a1e1-4cb5-b683-5cbe77e1c875 0xc004869147 0xc004869148}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"132632dd-a1e1-4cb5-b683-5cbe77e1c875\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048691f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:44:17.585: INFO: Pod "webserver-deployment-55df494869-6ddxd" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6ddxd webserver-deployment-55df494869- deployment-8725  ebed66cb-3682-4e33-95b0-c4743a2db152 26396 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:3fd5ee965d9827305a6bd113853c937167b98d5a761387865340085b871d9330 cni.projectcalico.org/podIP:192.168.149.104/32 cni.projectcalico.org/podIPs:192.168.149.104/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc0048697a7 0xc0048697a8}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnqht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnqht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:192.168.149.104,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1848c0118f8600f03a049c1e68c3fea05eacce1f4fc788b1b43c4e03e312b144,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.585: INFO: Pod "webserver-deployment-55df494869-6p9sv" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6p9sv webserver-deployment-55df494869- deployment-8725  a0efadc9-5778-47a0-b061-6192062cc870 26687 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc0048699b7 0xc0048699b8}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bk4vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bk4vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:,StartTime:2022-06-11 14:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.585: INFO: Pod "webserver-deployment-55df494869-6tx75" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6tx75 webserver-deployment-55df494869- deployment-8725  b8a4409e-8a3f-4614-ba11-f7797567548b 26703 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e6105e00578d53c13d60242b07a31611436c765aa9a07ddf5d642172998a619f cni.projectcalico.org/podIP:192.168.149.93/32 cni.projectcalico.org/podIPs:192.168.149.93/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc004869b87 0xc004869b88}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87b2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87b2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.586: INFO: Pod "webserver-deployment-55df494869-75x5t" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-75x5t webserver-deployment-55df494869- deployment-8725  0cbf010c-fede-4fb9-9d19-ee33d9c8b852 26690 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:0b4353169b11d4945f171d66efdc5543a2e0f2ea3c06f1e8953e3eec2835464e cni.projectcalico.org/podIP:192.168.149.98/32 cni.projectcalico.org/podIPs:192.168.149.98/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc004869d10 0xc004869d11}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7925,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.586: INFO: Pod "webserver-deployment-55df494869-79q9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-79q9p webserver-deployment-55df494869- deployment-8725  d7ce6370-ea5d-475e-95b0-014d028530cb 26593 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc004869eb0 0xc004869eb1}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bc98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bc98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.587: INFO: Pod "webserver-deployment-55df494869-7j47s" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7j47s webserver-deployment-55df494869- deployment-8725  a4dde055-3e5d-4371-b132-d50d524b23bf 26651 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:89ac1f6ad4aae530fbbbc2f98576a9d519d91904ddb77ff7e8c2a10729a1d5bc cni.projectcalico.org/podIP:192.168.203.25/32 cni.projectcalico.org/podIPs:192.168.203.25/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293c110 0xc00293c111}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8gj9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8gj9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.587: INFO: Pod "webserver-deployment-55df494869-7xrzx" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7xrzx webserver-deployment-55df494869- deployment-8725  e9a79069-9d99-4d26-9b1c-590111578f29 26668 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:659365a705b6d7303b998f6b38f1522e75def328e9673be50e7fb3cb3674da42 cni.projectcalico.org/podIP:192.168.149.94/32 cni.projectcalico.org/podIPs:192.168.149.94/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293c937 0xc00293c938}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4zg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4zg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.588: INFO: Pod "webserver-deployment-55df494869-c7rvh" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-c7rvh webserver-deployment-55df494869- deployment-8725  57b4e935-8c41-43de-8128-12ba04696b56 26388 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:412034f875a16d5fcdd0f105063282589430918a8b5056ab7da2b410ea345cad cni.projectcalico.org/podIP:192.168.203.10/32 cni.projectcalico.org/podIPs:192.168.203.10/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293cac0 0xc00293cac1}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cc8n4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cc8n4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.10,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fa3a9dbd911b25f6341425b99e389779521a466db7cb6056d2b409abf76f1919,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.588: INFO: Pod "webserver-deployment-55df494869-cmfgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-cmfgb webserver-deployment-55df494869- deployment-8725  6fa745c5-d90c-430e-b8e7-56f1f5d0af6d 26592 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293ccf7 0xc00293ccf8}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pmkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pmkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.588: INFO: Pod "webserver-deployment-55df494869-hmfgl" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-hmfgl webserver-deployment-55df494869- deployment-8725  6cf7fa9a-4e24-480e-a4a6-94021d063390 26671 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f7c3ba033357999409b3cdd2b6398f0127313ac12ff3321c82c6505a0659fcf4 cni.projectcalico.org/podIP:192.168.203.23/32 cni.projectcalico.org/podIPs:192.168.203.23/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293ce70 0xc00293ce71}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49r5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49r5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.589: INFO: Pod "webserver-deployment-55df494869-k2g85" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-k2g85 webserver-deployment-55df494869- deployment-8725  2a4815b1-737e-444b-bab8-7c829857b666 26436 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:7ee27c210dbc5f98573cb7c85b8ba99d2d8c12983b5af8c05e31d92780bb8273 cni.projectcalico.org/podIP:192.168.203.40/32 cni.projectcalico.org/podIPs:192.168.203.40/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d010 0xc00293d011}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wv2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wv2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.40,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b00f48f083920d24505c8f2b01a411b4c0d7da1507824a703e7dc960b24c7457,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.589: INFO: Pod "webserver-deployment-55df494869-kbxrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kbxrp webserver-deployment-55df494869- deployment-8725  0d631925-bffa-446d-bb7c-6332f4e2355e 26682 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f6b9d59ea6f001e997354dca5b575cb281526b32ecf1a0e9a51176b2932b1015 cni.projectcalico.org/podIP:192.168.149.92/32 cni.projectcalico.org/podIPs:192.168.149.92/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d227 0xc00293d228}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvl64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvl64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.589: INFO: Pod "webserver-deployment-55df494869-nrsf9" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-nrsf9 webserver-deployment-55df494869- deployment-8725  25f24cb3-a0e0-42a6-87f4-80a52f785634 26402 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:bce314e04f746fef3d237e98fbefeae3a186cb3566195c15a4396a80ab76be98 cni.projectcalico.org/podIP:192.168.149.78/32 cni.projectcalico.org/podIPs:192.168.149.78/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d3b0 0xc00293d3b1}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk26b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk26b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:192.168.149.78,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://28be080e4db51ed0f22be8aee6377f8c28d6e80816424288dde951c11e3bf1d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.590: INFO: Pod "webserver-deployment-55df494869-pvchh" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-pvchh webserver-deployment-55df494869- deployment-8725  53cde9a1-56fe-4f85-ab10-c843133606eb 26595 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d5c7 0xc00293d5c8}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhs26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhs26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.590: INFO: Pod "webserver-deployment-55df494869-snwqs" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-snwqs webserver-deployment-55df494869- deployment-8725  c3497f95-f1db-4a47-90cc-996a36ab46bc 26683 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:b43ebb53a26e9d13c00098e7ca5aaab36994dc23d023ebee94459f2864fcd15f cni.projectcalico.org/podIP:192.168.203.48/32 cni.projectcalico.org/podIPs:192.168.203.48/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d730 0xc00293d731}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rggw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rggw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.590: INFO: Pod "webserver-deployment-55df494869-srp57" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-srp57 webserver-deployment-55df494869- deployment-8725  58bc18fa-f8ef-46c0-80e0-0bfc7fb88e63 26410 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:428f58393dd5a7e6520b9226f301e9e52cf08c83903aeb3bcaa464438fc83929 cni.projectcalico.org/podIP:192.168.203.42/32 cni.projectcalico.org/podIPs:192.168.203.42/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293d927 0xc00293d928}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f926p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f926p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.42,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8fcd5fe75749b4219bbb96eace00ed69887ff51c82cc9f2995892ffe0081cf77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.591: INFO: Pod "webserver-deployment-55df494869-tbmdx" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-tbmdx webserver-deployment-55df494869- deployment-8725  e610b1b0-af8e-4e84-8769-ff776356b2bd 26370 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:424930822901236483e5aae756b5f8ea9bba4d45cecd8b6c1ba295f9dd099317 cni.projectcalico.org/podIP:192.168.203.21/32 cni.projectcalico.org/podIPs:192.168.203.21/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293db37 0xc00293db38}] []  [{Go-http-client Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5s24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5s24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.21,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1b75fcb683d086bf42922f1c1819fcfe194991ab2a2d74a27c24627deab6e88c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.591: INFO: Pod "webserver-deployment-55df494869-v6cc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-v6cc4 webserver-deployment-55df494869- deployment-8725  bdcc8812-e8c9-454a-ba89-106d4ee47ce5 26596 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293dd47 0xc00293dd48}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wkn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wkn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.591: INFO: Pod "webserver-deployment-55df494869-vsqx6" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vsqx6 webserver-deployment-55df494869- deployment-8725  a1ac8ae8-9a5c-43fd-8404-5d5e2a769576 26432 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e33e7a991afd9c1b58a4ab6ef3fe021db60454c2a50c91c24c877d76401e0654 cni.projectcalico.org/podIP:192.168.149.70/32 cni.projectcalico.org/podIPs:192.168.149.70/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc00293dec0 0xc00293dec1}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kt6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kt6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:192.168.149.70,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f2667916bdaaacdd089f366823877684fbb7d965dffac91b73f1bf27a108182,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.149.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.595: INFO: Pod "webserver-deployment-55df494869-wsq9p" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-wsq9p webserver-deployment-55df494869- deployment-8725  4b76fcc1-e32d-4ef9-b7b6-955a3dfae122 26423 0 2022-06-11 14:44:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ca3491b5483a7d3a0394fee46f288cb963bcfc0d3372ae3ddd2e1d0e87cc7e6c cni.projectcalico.org/podIP:192.168.203.33/32 cni.projectcalico.org/podIPs:192.168.203.33/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 13d3ba17-3f10-412d-ae84-83e28145afea 0xc0043880e7 0xc0043880e8}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13d3ba17-3f10-412d-ae84-83e28145afea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5b6ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5b6ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.33,StartTime:2022-06-11 14:44:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ef2ff1aebc94c00bdb2a65536609f810c8351a57ed8be82e7e6e65ae042e1d4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.596: INFO: Pod "webserver-deployment-57ccb67bb8-4skrm" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4skrm webserver-deployment-57ccb67bb8- deployment-8725  83abfe48-67d9-4e8c-a986-f5ada8809bde 26603 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388307 0xc004388308}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vn6sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn6sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.596: INFO: Pod "webserver-deployment-57ccb67bb8-7l8bw" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-7l8bw webserver-deployment-57ccb67bb8- deployment-8725  9dcde556-f640-4f85-9ce6-9a7d3243a7b2 26656 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f6a2429f842bcaa71cda49fa9311270697b236af595afc3592cb8dcbd5a7b96d cni.projectcalico.org/podIP:192.168.203.41/32 cni.projectcalico.org/podIPs:192.168.203.41/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388560 0xc004388561}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwvpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwvpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.596: INFO: Pod "webserver-deployment-57ccb67bb8-8n2mr" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-8n2mr webserver-deployment-57ccb67bb8- deployment-8725  e898cc1c-815d-47f0-87c8-39dce2cb38e7 26614 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388787 0xc004388788}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6kzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6kzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.597: INFO: Pod "webserver-deployment-57ccb67bb8-9fvf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-9fvf2 webserver-deployment-57ccb67bb8- deployment-8725  e8b0544d-4246-4b92-a1f0-c01fb8ef7a62 26696 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:d2d83bb537e93edd7b4c1d9e5081fe0af797a70102511a8e240c5d3f1b168c22 cni.projectcalico.org/podIP:192.168.203.47/32 cni.projectcalico.org/podIPs:192.168.203.47/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388900 0xc004388901}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btqsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btqsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.597: INFO: Pod "webserver-deployment-57ccb67bb8-9vpbf" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-9vpbf webserver-deployment-57ccb67bb8- deployment-8725  7445c763-e6bc-430f-8d07-7d03cb8bd4fd 26497 0 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:40faa438b7e5bc6ce63ccd6f08f3662c39f4465999c85ffaf9090641bb548e09 cni.projectcalico.org/podIP:192.168.203.27/32 cni.projectcalico.org/podIPs:192.168.203.27/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388b17 0xc004388b18}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-06-11 14:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scvqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scvqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.597: INFO: Pod "webserver-deployment-57ccb67bb8-bfqpv" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-bfqpv webserver-deployment-57ccb67bb8- deployment-8725  6665af1e-0a31-47ec-b611-866fd542a51b 26526 0 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:f037345dc61dd943c7885019dd5c9d991b57a895daeb3c6cca22c7a7b65f2df3 cni.projectcalico.org/podIP:192.168.149.90/32 cni.projectcalico.org/podIPs:192.168.149.90/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388d27 0xc004388d28}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg9r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg9r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:,StartTime:2022-06-11 14:44:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.598: INFO: Pod "webserver-deployment-57ccb67bb8-bgj8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-bgj8q webserver-deployment-57ccb67bb8- deployment-8725  42958e2b-c96f-4a45-af51-126ee9d91437 26534 0 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:dea8166ad814ffe517b34ad305aa00874dbdaa76dccfa9301d20bcb51b8920b3 cni.projectcalico.org/podIP:192.168.149.91/32 cni.projectcalico.org/podIPs:192.168.149.91/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004388f37 0xc004388f38}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjqq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjqq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:,StartTime:2022-06-11 14:44:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.598: INFO: Pod "webserver-deployment-57ccb67bb8-f4m6r" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-f4m6r webserver-deployment-57ccb67bb8- deployment-8725  5aff6e5b-1c9c-4eca-8721-9482a26c2981 26528 0 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7a5d1e8a720173ef6ae43db025c758b8d01fdb2eb2f3d0eef071d85daaf20e10 cni.projectcalico.org/podIP:192.168.149.110/32 cni.projectcalico.org/podIPs:192.168.149.110/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004389147 0xc004389148}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 14:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67nw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67nw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.214,PodIP:,StartTime:2022-06-11 14:44:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.599: INFO: Pod "webserver-deployment-57ccb67bb8-js4g2" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-js4g2 webserver-deployment-57ccb67bb8- deployment-8725  e276dfdf-13fb-40cf-8381-6d741909c60f 26601 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004389357 0xc004389358}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qh24w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qh24w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.599: INFO: Pod "webserver-deployment-57ccb67bb8-rccvc" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-rccvc webserver-deployment-57ccb67bb8- deployment-8725  10cdd0fa-23e6-48bd-bb59-8d35044390b1 26584 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc0043894d0 0xc0043894d1}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74qq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74qq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.599: INFO: Pod "webserver-deployment-57ccb67bb8-sk9hn" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-sk9hn webserver-deployment-57ccb67bb8- deployment-8725  0c41cee8-6446-4446-8220-eaa20557cdf8 26605 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004389920 0xc004389921}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plw7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plw7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.599: INFO: Pod "webserver-deployment-57ccb67bb8-wggbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wggbp webserver-deployment-57ccb67bb8- deployment-8725  54e7038d-5e51-4fa4-b633-50f12a05af6b 26604 0 2022-06-11 14:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004389a90 0xc004389a91}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fng8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fng8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:44:17.600: INFO: Pod "webserver-deployment-57ccb67bb8-zv4dv" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-zv4dv webserver-deployment-57ccb67bb8- deployment-8725  c3c3ccf6-068d-4d78-8526-0bbab55227dc 26503 0 2022-06-11 14:44:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7aa22d3eff867b0f3db1c5c94504a339ce94ec39f80c98fec1c754d2f2c7a75a cni.projectcalico.org/podIP:192.168.203.63/32 cni.projectcalico.org/podIPs:192.168.203.63/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 c74032c5-abd0-4c05-810e-e74d2057368e 0xc004389c20 0xc004389c21}] []  [{kube-controller-manager Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c74032c5-abd0-4c05-810e-e74d2057368e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:44:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-06-11 14:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brd98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brd98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:44:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 14:44:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:44:17.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8725" for this suite.

â€¢ [SLOW TEST:8.329 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":172,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:44:17.725: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b2cd2f62-7ed3-4282-8eab-d5773631dc4e", Controller:(*bool)(0xc00298d116), BlockOwnerDeletion:(*bool)(0xc00298d117)}}
Jun 11 14:44:17.736: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a7b32feb-f9eb-4e41-80b7-195fb2c24e14", Controller:(*bool)(0xc00298d356), BlockOwnerDeletion:(*bool)(0xc00298d357)}}
Jun 11 14:44:17.748: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9834ce0a-9635-44bc-b7b6-ff9f1fb21f9c", Controller:(*bool)(0xc0046f37e6), BlockOwnerDeletion:(*bool)(0xc0046f37e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 14:44:22.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9724" for this suite.

â€¢ [SLOW TEST:5.167 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":173,"skipped":3506,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:22.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 11 14:44:22.860: INFO: Waiting up to 5m0s for pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7" in namespace "emptydir-1616" to be "Succeeded or Failed"
Jun 11 14:44:22.873: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.420068ms
Jun 11 14:44:24.879: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018337526s
Jun 11 14:44:26.885: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024781111s
Jun 11 14:44:28.893: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032316451s
Jun 11 14:44:30.902: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041459667s
STEP: Saw pod success
Jun 11 14:44:30.902: INFO: Pod "pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7" satisfied condition "Succeeded or Failed"
Jun 11 14:44:30.936: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7 container test-container: <nil>
STEP: delete the pod
Jun 11 14:44:30.969: INFO: Waiting for pod pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7 to disappear
Jun 11 14:44:30.983: INFO: Pod pod-a25aab49-94f8-43e7-bc4f-59421b25b9d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:44:30.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1616" for this suite.

â€¢ [SLOW TEST:8.264 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":174,"skipped":3525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jun 11 14:44:31.113: INFO: Waiting up to 5m0s for pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522" in namespace "containers-6938" to be "Succeeded or Failed"
Jun 11 14:44:31.119: INFO: Pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522": Phase="Pending", Reason="", readiness=false. Elapsed: 5.90843ms
Jun 11 14:44:33.126: INFO: Pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012936263s
Jun 11 14:44:35.136: INFO: Pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02263564s
Jun 11 14:44:37.143: INFO: Pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029284866s
STEP: Saw pod success
Jun 11 14:44:37.143: INFO: Pod "client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522" satisfied condition "Succeeded or Failed"
Jun 11 14:44:37.148: INFO: Trying to get logs from node ip-172-31-41-158 pod client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:44:37.170: INFO: Waiting for pod client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522 to disappear
Jun 11 14:44:37.174: INFO: Pod client-containers-d11d7e8d-3f5d-41f4-8e9a-8b80cba00522 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jun 11 14:44:37.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6938" for this suite.

â€¢ [SLOW TEST:6.124 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":175,"skipped":3560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jun 11 14:44:37.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-9598 api-versions'
Jun 11 14:44:37.333: INFO: stderr: ""
Jun 11 14:44:37.333: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperator.tigera.io/v1\npolicy/v1\npolicy/v1beta1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:44:37.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9598" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":176,"skipped":3591,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jun 11 14:44:37.401: INFO: The status of Pod annotationupdated5907f58-d9c5-41e4-af45-2a315d87a3a1 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:44:39.407: INFO: The status of Pod annotationupdated5907f58-d9c5-41e4-af45-2a315d87a3a1 is Running (Ready = true)
Jun 11 14:44:39.940: INFO: Successfully updated pod "annotationupdated5907f58-d9c5-41e4-af45-2a315d87a3a1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:44:41.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8701" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":177,"skipped":3612,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-3097
STEP: creating replication controller nodeport-test in namespace services-3097
I0611 14:44:42.083183      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3097, replica count: 2
I0611 14:44:45.134157      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 14:44:45.134: INFO: Creating new exec pod
Jun 11 14:44:48.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 11 14:44:48.349: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 11 14:44:48.349: INFO: stdout: "nodeport-test-6d2w4"
Jun 11 14:44:48.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.43.80 80'
Jun 11 14:44:48.516: INFO: stderr: "+ + nc -v -t -wecho 2 hostName 10.102.43.80\n 80\nConnection to 10.102.43.80 80 port [tcp/http] succeeded!\n"
Jun 11 14:44:48.516: INFO: stdout: ""
Jun 11 14:44:49.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.43.80 80'
Jun 11 14:44:49.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.43.80 80\nConnection to 10.102.43.80 80 port [tcp/http] succeeded!\n"
Jun 11 14:44:49.695: INFO: stdout: "nodeport-test-xg4ts"
Jun 11 14:44:49.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 32547'
Jun 11 14:44:49.859: INFO: stderr: "+ nc -v -t -w 2 172.31.27.214 32547\n+ echo hostName\nConnection to 172.31.27.214 32547 port [tcp/*] succeeded!\n"
Jun 11 14:44:49.859: INFO: stdout: ""
Jun 11 14:44:50.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 32547'
Jun 11 14:44:51.078: INFO: stderr: "+ nc -v -t -w 2 172.31.27.214 32547\n+ echo hostName\nConnection to 172.31.27.214 32547 port [tcp/*] succeeded!\n"
Jun 11 14:44:51.078: INFO: stdout: ""
Jun 11 14:44:51.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 32547'
Jun 11 14:44:52.027: INFO: stderr: "+ nc -v -t -w 2 172.31.27.214 32547\n+ echo hostName\nConnection to 172.31.27.214 32547 port [tcp/*] succeeded!\n"
Jun 11 14:44:52.027: INFO: stdout: "nodeport-test-xg4ts"
Jun 11 14:44:52.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-3097 exec execpodbsdwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 32547'
Jun 11 14:44:52.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.41.158 32547\nConnection to 172.31.41.158 32547 port [tcp/*] succeeded!\n"
Jun 11 14:44:52.188: INFO: stdout: "nodeport-test-6d2w4"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:44:52.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3097" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:10.222 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":178,"skipped":3613,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:52.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 11 14:44:52.260: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 11 14:44:57.268: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jun 11 14:44:58.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7255" for this suite.

â€¢ [SLOW TEST:6.116 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":179,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:44:58.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:44:58.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0" in namespace "downward-api-4059" to be "Succeeded or Failed"
Jun 11 14:44:58.382: INFO: Pod "downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419767ms
Jun 11 14:45:00.388: INFO: Pod "downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010914802s
Jun 11 14:45:02.398: INFO: Pod "downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020472181s
STEP: Saw pod success
Jun 11 14:45:02.398: INFO: Pod "downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0" satisfied condition "Succeeded or Failed"
Jun 11 14:45:02.403: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0 container client-container: <nil>
STEP: delete the pod
Jun 11 14:45:02.433: INFO: Waiting for pod downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0 to disappear
Jun 11 14:45:02.438: INFO: Pod downwardapi-volume-4d868ff5-d39b-499d-960e-33ed8b9e42e0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:45:02.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4059" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":180,"skipped":3663,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-7107
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 14:45:02.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 11 14:45:02.533: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:45:04.549: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:06.544: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:08.542: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:10.551: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:12.549: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:14.540: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:16.573: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:18.551: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:20.545: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:22.544: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 14:45:24.550: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 11 14:45:24.570: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jun 11 14:45:26.636: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jun 11 14:45:26.636: INFO: Going to poll 192.168.149.115 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jun 11 14:45:26.639: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.149.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7107 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:45:26.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:45:26.640: INFO: ExecWithOptions: Clientset creation
Jun 11 14:45:26.640: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7107/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.149.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 11 14:45:27.731: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 11 14:45:27.731: INFO: Going to poll 192.168.203.26 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jun 11 14:45:27.738: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.203.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7107 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 14:45:27.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 14:45:27.739: INFO: ExecWithOptions: Clientset creation
Jun 11 14:45:27.739: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7107/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.203.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 11 14:45:28.825: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jun 11 14:45:28.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7107" for this suite.

â€¢ [SLOW TEST:26.401 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":181,"skipped":3664,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:45:29.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:45:32.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:45:32.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8110" for this suite.
STEP: Destroying namespace "webhook-8110-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":182,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:32.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:45:32.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04" in namespace "projected-13" to be "Succeeded or Failed"
Jun 11 14:45:32.431: INFO: Pod "downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851409ms
Jun 11 14:45:34.437: INFO: Pod "downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009763056s
Jun 11 14:45:36.445: INFO: Pod "downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017574461s
STEP: Saw pod success
Jun 11 14:45:36.445: INFO: Pod "downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04" satisfied condition "Succeeded or Failed"
Jun 11 14:45:36.449: INFO: Trying to get logs from node ip-172-31-27-214 pod downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04 container client-container: <nil>
STEP: delete the pod
Jun 11 14:45:36.492: INFO: Waiting for pod downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04 to disappear
Jun 11 14:45:36.497: INFO: Pod downwardapi-volume-1a9c04e9-2090-448d-a2a2-4336b6381e04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 14:45:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-13" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":183,"skipped":3716,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:36.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jun 11 14:45:36.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jun 11 14:45:37.158: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 11 14:45:39.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:45:41.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:45:43.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:45:45.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:45:47.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 14, 45, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 11 14:45:50.013: INFO: Waited 739.352953ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jun 11 14:45:50.129: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jun 11 14:45:50.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7718" for this suite.

â€¢ [SLOW TEST:14.180 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":184,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:50.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-77df8436-c76b-4cbb-87c1-e7fed66e6f68
STEP: Creating a pod to test consume configMaps
Jun 11 14:45:50.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba" in namespace "projected-7872" to be "Succeeded or Failed"
Jun 11 14:45:50.750: INFO: Pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859824ms
Jun 11 14:45:52.760: INFO: Pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014755293s
Jun 11 14:45:54.768: INFO: Pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.023066848s
Jun 11 14:45:56.773: INFO: Pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028027695s
STEP: Saw pod success
Jun 11 14:45:56.773: INFO: Pod "pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba" satisfied condition "Succeeded or Failed"
Jun 11 14:45:56.778: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:45:56.806: INFO: Waiting for pod pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba to disappear
Jun 11 14:45:56.812: INFO: Pod pod-projected-configmaps-d530575d-0fb4-4c4d-a0ae-b28f7b1423ba no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 14:45:56.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7872" for this suite.

â€¢ [SLOW TEST:6.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":185,"skipped":3764,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:45:56.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 11 14:47:57.415: INFO: Successfully updated pod "var-expansion-8387b121-7e7b-458d-9ce7-127e807631bb"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 11 14:47:59.431: INFO: Deleting pod "var-expansion-8387b121-7e7b-458d-9ce7-127e807631bb" in namespace "var-expansion-6362"
Jun 11 14:47:59.444: INFO: Wait up to 5m0s for pod "var-expansion-8387b121-7e7b-458d-9ce7-127e807631bb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 14:48:31.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6362" for this suite.

â€¢ [SLOW TEST:154.651 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":186,"skipped":3768,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:31.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:48:31.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1" in namespace "downward-api-3352" to be "Succeeded or Failed"
Jun 11 14:48:31.631: INFO: Pod "downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.817676ms
Jun 11 14:48:33.646: INFO: Pod "downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049361904s
Jun 11 14:48:35.653: INFO: Pod "downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05697263s
STEP: Saw pod success
Jun 11 14:48:35.654: INFO: Pod "downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1" satisfied condition "Succeeded or Failed"
Jun 11 14:48:35.659: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1 container client-container: <nil>
STEP: delete the pod
Jun 11 14:48:35.694: INFO: Waiting for pod downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1 to disappear
Jun 11 14:48:35.698: INFO: Pod downwardapi-volume-f9a5ecbf-5b5a-4a5d-a3a8-7c8af024f9b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:48:35.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3352" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:35.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:48:35.758: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2c11d306-c5f1-48b3-a2ff-85d7a01942d1" in namespace "security-context-test-4657" to be "Succeeded or Failed"
Jun 11 14:48:35.769: INFO: Pod "busybox-user-65534-2c11d306-c5f1-48b3-a2ff-85d7a01942d1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.97972ms
Jun 11 14:48:37.776: INFO: Pod "busybox-user-65534-2c11d306-c5f1-48b3-a2ff-85d7a01942d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017107696s
Jun 11 14:48:39.785: INFO: Pod "busybox-user-65534-2c11d306-c5f1-48b3-a2ff-85d7a01942d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026022881s
Jun 11 14:48:39.785: INFO: Pod "busybox-user-65534-2c11d306-c5f1-48b3-a2ff-85d7a01942d1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 14:48:39.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4657" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3803,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:39.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 14:48:39.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97" in namespace "downward-api-8274" to be "Succeeded or Failed"
Jun 11 14:48:39.881: INFO: Pod "downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029301ms
Jun 11 14:48:41.889: INFO: Pod "downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97": Phase="Running", Reason="", readiness=false. Elapsed: 2.015180847s
Jun 11 14:48:43.897: INFO: Pod "downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023488767s
STEP: Saw pod success
Jun 11 14:48:43.897: INFO: Pod "downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97" satisfied condition "Succeeded or Failed"
Jun 11 14:48:43.901: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97 container client-container: <nil>
STEP: delete the pod
Jun 11 14:48:43.927: INFO: Waiting for pod downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97 to disappear
Jun 11 14:48:43.931: INFO: Pod downwardapi-volume-95f40063-d12b-49bf-839e-b3759b8c5f97 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:48:43.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8274" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":189,"skipped":3824,"failed":0}

------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:43.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:48:43.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3694" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":190,"skipped":3824,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:44.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:48:44.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3289
I0611 14:48:44.065926      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3289, replica count: 1
I0611 14:48:45.117293      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0611 14:48:46.118324      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 14:48:46.238: INFO: Created: latency-svc-vnb47
Jun 11 14:48:46.263: INFO: Got endpoints: latency-svc-vnb47 [45.182093ms]
Jun 11 14:48:46.311: INFO: Created: latency-svc-gwwfm
Jun 11 14:48:46.325: INFO: Got endpoints: latency-svc-gwwfm [60.120109ms]
Jun 11 14:48:46.338: INFO: Created: latency-svc-wq4pj
Jun 11 14:48:46.359: INFO: Got endpoints: latency-svc-wq4pj [94.890213ms]
Jun 11 14:48:46.369: INFO: Created: latency-svc-fsjzz
Jun 11 14:48:46.386: INFO: Got endpoints: latency-svc-fsjzz [119.688849ms]
Jun 11 14:48:46.395: INFO: Created: latency-svc-4jm6m
Jun 11 14:48:46.411: INFO: Got endpoints: latency-svc-4jm6m [146.286751ms]
Jun 11 14:48:46.422: INFO: Created: latency-svc-q4khm
Jun 11 14:48:46.426: INFO: Got endpoints: latency-svc-q4khm [160.427834ms]
Jun 11 14:48:46.442: INFO: Created: latency-svc-9ml8n
Jun 11 14:48:46.478: INFO: Got endpoints: latency-svc-9ml8n [212.631364ms]
Jun 11 14:48:46.486: INFO: Created: latency-svc-bphnk
Jun 11 14:48:46.499: INFO: Got endpoints: latency-svc-bphnk [233.508389ms]
Jun 11 14:48:46.506: INFO: Created: latency-svc-dr9ds
Jun 11 14:48:46.519: INFO: Got endpoints: latency-svc-dr9ds [253.647593ms]
Jun 11 14:48:46.531: INFO: Created: latency-svc-pksm2
Jun 11 14:48:46.541: INFO: Got endpoints: latency-svc-pksm2 [275.616856ms]
Jun 11 14:48:46.548: INFO: Created: latency-svc-5t9rl
Jun 11 14:48:46.560: INFO: Got endpoints: latency-svc-5t9rl [294.752435ms]
Jun 11 14:48:46.569: INFO: Created: latency-svc-dptz8
Jun 11 14:48:46.585: INFO: Got endpoints: latency-svc-dptz8 [319.718003ms]
Jun 11 14:48:46.598: INFO: Created: latency-svc-q84wm
Jun 11 14:48:46.614: INFO: Got endpoints: latency-svc-q84wm [347.934958ms]
Jun 11 14:48:46.626: INFO: Created: latency-svc-ps5fr
Jun 11 14:48:46.652: INFO: Got endpoints: latency-svc-ps5fr [385.984924ms]
Jun 11 14:48:46.678: INFO: Created: latency-svc-x9lhv
Jun 11 14:48:46.693: INFO: Got endpoints: latency-svc-x9lhv [426.553067ms]
Jun 11 14:48:46.702: INFO: Created: latency-svc-bz5kd
Jun 11 14:48:46.719: INFO: Created: latency-svc-w7v42
Jun 11 14:48:46.720: INFO: Got endpoints: latency-svc-bz5kd [453.624787ms]
Jun 11 14:48:46.735: INFO: Got endpoints: latency-svc-w7v42 [409.823734ms]
Jun 11 14:48:46.752: INFO: Created: latency-svc-56m29
Jun 11 14:48:46.759: INFO: Got endpoints: latency-svc-56m29 [399.652851ms]
Jun 11 14:48:46.771: INFO: Created: latency-svc-4mbwp
Jun 11 14:48:46.781: INFO: Got endpoints: latency-svc-4mbwp [394.988771ms]
Jun 11 14:48:46.801: INFO: Created: latency-svc-5ft8k
Jun 11 14:48:46.810: INFO: Created: latency-svc-8mq9k
Jun 11 14:48:46.821: INFO: Got endpoints: latency-svc-5ft8k [409.199586ms]
Jun 11 14:48:46.823: INFO: Got endpoints: latency-svc-8mq9k [396.544154ms]
Jun 11 14:48:46.843: INFO: Created: latency-svc-k8bj5
Jun 11 14:48:46.848: INFO: Got endpoints: latency-svc-k8bj5 [370.453149ms]
Jun 11 14:48:46.857: INFO: Created: latency-svc-hghxb
Jun 11 14:48:46.874: INFO: Got endpoints: latency-svc-hghxb [375.535856ms]
Jun 11 14:48:46.889: INFO: Created: latency-svc-sg8gh
Jun 11 14:48:46.904: INFO: Got endpoints: latency-svc-sg8gh [384.618674ms]
Jun 11 14:48:46.906: INFO: Created: latency-svc-bbc5n
Jun 11 14:48:46.925: INFO: Got endpoints: latency-svc-bbc5n [383.234761ms]
Jun 11 14:48:46.925: INFO: Created: latency-svc-k9dxd
Jun 11 14:48:46.934: INFO: Got endpoints: latency-svc-k9dxd [373.401713ms]
Jun 11 14:48:46.941: INFO: Created: latency-svc-ss6kc
Jun 11 14:48:46.959: INFO: Created: latency-svc-xzzmq
Jun 11 14:48:46.967: INFO: Got endpoints: latency-svc-ss6kc [381.012128ms]
Jun 11 14:48:46.969: INFO: Got endpoints: latency-svc-xzzmq [354.750751ms]
Jun 11 14:48:46.977: INFO: Created: latency-svc-dz2vm
Jun 11 14:48:47.017: INFO: Got endpoints: latency-svc-dz2vm [364.731422ms]
Jun 11 14:48:47.026: INFO: Created: latency-svc-pz28p
Jun 11 14:48:47.039: INFO: Got endpoints: latency-svc-pz28p [346.509676ms]
Jun 11 14:48:47.045: INFO: Created: latency-svc-fd58t
Jun 11 14:48:47.061: INFO: Got endpoints: latency-svc-fd58t [340.857841ms]
Jun 11 14:48:47.066: INFO: Created: latency-svc-795nc
Jun 11 14:48:47.081: INFO: Got endpoints: latency-svc-795nc [346.0432ms]
Jun 11 14:48:47.093: INFO: Created: latency-svc-4vgg7
Jun 11 14:48:47.103: INFO: Got endpoints: latency-svc-4vgg7 [344.204232ms]
Jun 11 14:48:47.110: INFO: Created: latency-svc-vqdqf
Jun 11 14:48:47.128: INFO: Got endpoints: latency-svc-vqdqf [346.827392ms]
Jun 11 14:48:47.139: INFO: Created: latency-svc-ft78x
Jun 11 14:48:47.150: INFO: Created: latency-svc-lwz5f
Jun 11 14:48:47.160: INFO: Got endpoints: latency-svc-lwz5f [336.583247ms]
Jun 11 14:48:47.160: INFO: Got endpoints: latency-svc-ft78x [339.273018ms]
Jun 11 14:48:47.170: INFO: Created: latency-svc-xflgh
Jun 11 14:48:47.214: INFO: Got endpoints: latency-svc-xflgh [365.440993ms]
Jun 11 14:48:47.219: INFO: Created: latency-svc-cfxdk
Jun 11 14:48:47.243: INFO: Got endpoints: latency-svc-cfxdk [368.084834ms]
Jun 11 14:48:47.253: INFO: Created: latency-svc-ngm2g
Jun 11 14:48:47.272: INFO: Got endpoints: latency-svc-ngm2g [367.985312ms]
Jun 11 14:48:47.283: INFO: Created: latency-svc-vpdsx
Jun 11 14:48:47.299: INFO: Got endpoints: latency-svc-vpdsx [374.179358ms]
Jun 11 14:48:47.315: INFO: Created: latency-svc-k6ckr
Jun 11 14:48:47.337: INFO: Got endpoints: latency-svc-k6ckr [403.204476ms]
Jun 11 14:48:47.355: INFO: Created: latency-svc-8ndgv
Jun 11 14:48:47.369: INFO: Got endpoints: latency-svc-8ndgv [402.445425ms]
Jun 11 14:48:47.392: INFO: Created: latency-svc-7rs5j
Jun 11 14:48:47.400: INFO: Got endpoints: latency-svc-7rs5j [431.404753ms]
Jun 11 14:48:47.415: INFO: Created: latency-svc-kbjlj
Jun 11 14:48:47.428: INFO: Got endpoints: latency-svc-kbjlj [411.284529ms]
Jun 11 14:48:47.437: INFO: Created: latency-svc-qm5gk
Jun 11 14:48:47.461: INFO: Got endpoints: latency-svc-qm5gk [421.234328ms]
Jun 11 14:48:47.468: INFO: Created: latency-svc-nqhrw
Jun 11 14:48:47.475: INFO: Got endpoints: latency-svc-nqhrw [413.973598ms]
Jun 11 14:48:47.488: INFO: Created: latency-svc-zznqf
Jun 11 14:48:47.503: INFO: Created: latency-svc-sspfn
Jun 11 14:48:47.506: INFO: Got endpoints: latency-svc-zznqf [424.72692ms]
Jun 11 14:48:47.517: INFO: Got endpoints: latency-svc-sspfn [413.40693ms]
Jun 11 14:48:47.531: INFO: Created: latency-svc-42hrf
Jun 11 14:48:47.547: INFO: Got endpoints: latency-svc-42hrf [418.287574ms]
Jun 11 14:48:47.553: INFO: Created: latency-svc-8wql9
Jun 11 14:48:47.568: INFO: Got endpoints: latency-svc-8wql9 [407.749255ms]
Jun 11 14:48:47.574: INFO: Created: latency-svc-k9s56
Jun 11 14:48:47.587: INFO: Got endpoints: latency-svc-k9s56 [426.421187ms]
Jun 11 14:48:47.593: INFO: Created: latency-svc-ts6g4
Jun 11 14:48:47.611: INFO: Got endpoints: latency-svc-ts6g4 [396.423382ms]
Jun 11 14:48:47.617: INFO: Created: latency-svc-shc89
Jun 11 14:48:47.627: INFO: Got endpoints: latency-svc-shc89 [384.41253ms]
Jun 11 14:48:47.637: INFO: Created: latency-svc-gdvwm
Jun 11 14:48:47.652: INFO: Got endpoints: latency-svc-gdvwm [379.675969ms]
Jun 11 14:48:47.661: INFO: Created: latency-svc-flb56
Jun 11 14:48:47.673: INFO: Got endpoints: latency-svc-flb56 [373.613107ms]
Jun 11 14:48:47.681: INFO: Created: latency-svc-nprns
Jun 11 14:48:47.691: INFO: Got endpoints: latency-svc-nprns [353.662896ms]
Jun 11 14:48:47.698: INFO: Created: latency-svc-2wjrc
Jun 11 14:48:47.708: INFO: Got endpoints: latency-svc-2wjrc [339.030744ms]
Jun 11 14:48:47.716: INFO: Created: latency-svc-qdftq
Jun 11 14:48:47.730: INFO: Got endpoints: latency-svc-qdftq [329.025333ms]
Jun 11 14:48:47.736: INFO: Created: latency-svc-cpk96
Jun 11 14:48:47.744: INFO: Got endpoints: latency-svc-cpk96 [315.61266ms]
Jun 11 14:48:47.750: INFO: Created: latency-svc-4hcrs
Jun 11 14:48:47.763: INFO: Created: latency-svc-ng6rs
Jun 11 14:48:47.784: INFO: Got endpoints: latency-svc-4hcrs [323.282837ms]
Jun 11 14:48:47.785: INFO: Created: latency-svc-dzkjt
Jun 11 14:48:47.799: INFO: Created: latency-svc-rwz7v
Jun 11 14:48:47.812: INFO: Created: latency-svc-s2qj7
Jun 11 14:48:47.822: INFO: Got endpoints: latency-svc-ng6rs [347.013046ms]
Jun 11 14:48:47.829: INFO: Created: latency-svc-svw42
Jun 11 14:48:47.852: INFO: Created: latency-svc-n6xcr
Jun 11 14:48:47.864: INFO: Got endpoints: latency-svc-dzkjt [358.046443ms]
Jun 11 14:48:47.868: INFO: Created: latency-svc-9wwlw
Jun 11 14:48:47.892: INFO: Created: latency-svc-gp6nh
Jun 11 14:48:47.898: INFO: Created: latency-svc-52tw4
Jun 11 14:48:47.913: INFO: Created: latency-svc-zbrms
Jun 11 14:48:47.923: INFO: Got endpoints: latency-svc-rwz7v [405.767603ms]
Jun 11 14:48:47.929: INFO: Created: latency-svc-v78h4
Jun 11 14:48:47.945: INFO: Created: latency-svc-8vvlt
Jun 11 14:48:47.958: INFO: Created: latency-svc-rxr2f
Jun 11 14:48:47.970: INFO: Got endpoints: latency-svc-s2qj7 [422.67944ms]
Jun 11 14:48:47.973: INFO: Created: latency-svc-c74vr
Jun 11 14:48:47.988: INFO: Created: latency-svc-9dlbq
Jun 11 14:48:48.002: INFO: Created: latency-svc-ljgp2
Jun 11 14:48:48.029: INFO: Got endpoints: latency-svc-svw42 [461.151121ms]
Jun 11 14:48:48.036: INFO: Created: latency-svc-g59lg
Jun 11 14:48:48.050: INFO: Created: latency-svc-vjq62
Jun 11 14:48:48.066: INFO: Created: latency-svc-zl27k
Jun 11 14:48:48.068: INFO: Got endpoints: latency-svc-n6xcr [481.072362ms]
Jun 11 14:48:48.082: INFO: Created: latency-svc-lxzwz
Jun 11 14:48:48.113: INFO: Created: latency-svc-j7s6v
Jun 11 14:48:48.122: INFO: Got endpoints: latency-svc-9wwlw [510.936683ms]
Jun 11 14:48:48.145: INFO: Created: latency-svc-nj9xj
Jun 11 14:48:48.165: INFO: Got endpoints: latency-svc-gp6nh [537.436323ms]
Jun 11 14:48:48.183: INFO: Created: latency-svc-rq2np
Jun 11 14:48:48.217: INFO: Got endpoints: latency-svc-52tw4 [564.892198ms]
Jun 11 14:48:48.235: INFO: Created: latency-svc-t7dw4
Jun 11 14:48:48.264: INFO: Got endpoints: latency-svc-zbrms [590.197621ms]
Jun 11 14:48:48.287: INFO: Created: latency-svc-bktqb
Jun 11 14:48:48.327: INFO: Got endpoints: latency-svc-v78h4 [635.921203ms]
Jun 11 14:48:48.349: INFO: Created: latency-svc-8kcqp
Jun 11 14:48:48.367: INFO: Got endpoints: latency-svc-8vvlt [658.219689ms]
Jun 11 14:48:48.387: INFO: Created: latency-svc-xnsdk
Jun 11 14:48:48.416: INFO: Got endpoints: latency-svc-rxr2f [686.302733ms]
Jun 11 14:48:48.437: INFO: Created: latency-svc-q67n4
Jun 11 14:48:48.467: INFO: Got endpoints: latency-svc-c74vr [723.270951ms]
Jun 11 14:48:48.487: INFO: Created: latency-svc-m9rd7
Jun 11 14:48:48.517: INFO: Got endpoints: latency-svc-9dlbq [732.091285ms]
Jun 11 14:48:48.542: INFO: Created: latency-svc-9rzp5
Jun 11 14:48:48.567: INFO: Got endpoints: latency-svc-ljgp2 [744.158336ms]
Jun 11 14:48:48.586: INFO: Created: latency-svc-c5gm9
Jun 11 14:48:48.614: INFO: Got endpoints: latency-svc-g59lg [749.643878ms]
Jun 11 14:48:48.633: INFO: Created: latency-svc-jwsbt
Jun 11 14:48:48.665: INFO: Got endpoints: latency-svc-vjq62 [742.002723ms]
Jun 11 14:48:48.686: INFO: Created: latency-svc-2hr44
Jun 11 14:48:48.714: INFO: Got endpoints: latency-svc-zl27k [743.981213ms]
Jun 11 14:48:48.732: INFO: Created: latency-svc-5d77k
Jun 11 14:48:48.766: INFO: Got endpoints: latency-svc-lxzwz [736.603652ms]
Jun 11 14:48:48.785: INFO: Created: latency-svc-f2lv7
Jun 11 14:48:48.816: INFO: Got endpoints: latency-svc-j7s6v [747.244581ms]
Jun 11 14:48:48.832: INFO: Created: latency-svc-j7twm
Jun 11 14:48:48.866: INFO: Got endpoints: latency-svc-nj9xj [744.037924ms]
Jun 11 14:48:48.886: INFO: Created: latency-svc-rnqds
Jun 11 14:48:48.914: INFO: Got endpoints: latency-svc-rq2np [749.261203ms]
Jun 11 14:48:48.930: INFO: Created: latency-svc-s9ppd
Jun 11 14:48:48.969: INFO: Got endpoints: latency-svc-t7dw4 [751.182412ms]
Jun 11 14:48:48.993: INFO: Created: latency-svc-4frvb
Jun 11 14:48:49.015: INFO: Got endpoints: latency-svc-bktqb [751.72973ms]
Jun 11 14:48:49.041: INFO: Created: latency-svc-wjqcq
Jun 11 14:48:49.065: INFO: Got endpoints: latency-svc-8kcqp [737.20509ms]
Jun 11 14:48:49.086: INFO: Created: latency-svc-fwbch
Jun 11 14:48:49.126: INFO: Got endpoints: latency-svc-xnsdk [758.795145ms]
Jun 11 14:48:49.153: INFO: Created: latency-svc-ghqqj
Jun 11 14:48:49.168: INFO: Got endpoints: latency-svc-q67n4 [752.386848ms]
Jun 11 14:48:49.196: INFO: Created: latency-svc-bqg8v
Jun 11 14:48:49.220: INFO: Got endpoints: latency-svc-m9rd7 [752.183197ms]
Jun 11 14:48:49.246: INFO: Created: latency-svc-llhvj
Jun 11 14:48:49.268: INFO: Got endpoints: latency-svc-9rzp5 [751.451356ms]
Jun 11 14:48:49.308: INFO: Created: latency-svc-fqbsx
Jun 11 14:48:49.326: INFO: Got endpoints: latency-svc-c5gm9 [759.532009ms]
Jun 11 14:48:49.374: INFO: Got endpoints: latency-svc-jwsbt [759.753323ms]
Jun 11 14:48:49.389: INFO: Created: latency-svc-q9pqt
Jun 11 14:48:49.408: INFO: Created: latency-svc-7m9b2
Jun 11 14:48:49.421: INFO: Got endpoints: latency-svc-2hr44 [755.763183ms]
Jun 11 14:48:49.457: INFO: Created: latency-svc-knbqg
Jun 11 14:48:49.468: INFO: Got endpoints: latency-svc-5d77k [754.32568ms]
Jun 11 14:48:49.501: INFO: Created: latency-svc-gr8p8
Jun 11 14:48:49.515: INFO: Got endpoints: latency-svc-f2lv7 [749.04712ms]
Jun 11 14:48:49.548: INFO: Created: latency-svc-97t72
Jun 11 14:48:49.574: INFO: Got endpoints: latency-svc-j7twm [757.951135ms]
Jun 11 14:48:49.620: INFO: Created: latency-svc-bxq9s
Jun 11 14:48:49.625: INFO: Got endpoints: latency-svc-rnqds [758.733157ms]
Jun 11 14:48:49.667: INFO: Got endpoints: latency-svc-s9ppd [752.261478ms]
Jun 11 14:48:49.670: INFO: Created: latency-svc-5x56s
Jun 11 14:48:49.710: INFO: Created: latency-svc-rcnsp
Jun 11 14:48:49.725: INFO: Got endpoints: latency-svc-4frvb [756.35814ms]
Jun 11 14:48:49.750: INFO: Created: latency-svc-6vrwv
Jun 11 14:48:49.766: INFO: Got endpoints: latency-svc-wjqcq [750.231607ms]
Jun 11 14:48:49.815: INFO: Created: latency-svc-plpp6
Jun 11 14:48:49.827: INFO: Got endpoints: latency-svc-fwbch [762.450222ms]
Jun 11 14:48:49.849: INFO: Created: latency-svc-fdmd6
Jun 11 14:48:49.866: INFO: Got endpoints: latency-svc-ghqqj [740.037683ms]
Jun 11 14:48:49.892: INFO: Created: latency-svc-gh2mm
Jun 11 14:48:49.924: INFO: Got endpoints: latency-svc-bqg8v [755.320624ms]
Jun 11 14:48:49.953: INFO: Created: latency-svc-d8zb7
Jun 11 14:48:49.969: INFO: Got endpoints: latency-svc-llhvj [748.279726ms]
Jun 11 14:48:49.986: INFO: Created: latency-svc-kh546
Jun 11 14:48:50.017: INFO: Got endpoints: latency-svc-fqbsx [748.700744ms]
Jun 11 14:48:50.050: INFO: Created: latency-svc-rflp7
Jun 11 14:48:50.072: INFO: Got endpoints: latency-svc-q9pqt [745.78918ms]
Jun 11 14:48:50.098: INFO: Created: latency-svc-m7pmf
Jun 11 14:48:50.113: INFO: Got endpoints: latency-svc-7m9b2 [739.109839ms]
Jun 11 14:48:50.153: INFO: Created: latency-svc-6rjs2
Jun 11 14:48:50.166: INFO: Got endpoints: latency-svc-knbqg [744.689234ms]
Jun 11 14:48:50.192: INFO: Created: latency-svc-stcfs
Jun 11 14:48:50.215: INFO: Got endpoints: latency-svc-gr8p8 [746.423161ms]
Jun 11 14:48:50.233: INFO: Created: latency-svc-ls2vt
Jun 11 14:48:50.266: INFO: Got endpoints: latency-svc-97t72 [751.166763ms]
Jun 11 14:48:50.291: INFO: Created: latency-svc-ghcpt
Jun 11 14:48:50.325: INFO: Got endpoints: latency-svc-bxq9s [750.793548ms]
Jun 11 14:48:50.378: INFO: Created: latency-svc-hgcj4
Jun 11 14:48:50.379: INFO: Got endpoints: latency-svc-5x56s [752.810729ms]
Jun 11 14:48:50.420: INFO: Got endpoints: latency-svc-rcnsp [752.594925ms]
Jun 11 14:48:50.427: INFO: Created: latency-svc-j67lt
Jun 11 14:48:50.495: INFO: Got endpoints: latency-svc-6vrwv [769.752724ms]
Jun 11 14:48:50.496: INFO: Created: latency-svc-tmgph
Jun 11 14:48:50.544: INFO: Got endpoints: latency-svc-plpp6 [778.191862ms]
Jun 11 14:48:50.550: INFO: Created: latency-svc-m2wfv
Jun 11 14:48:50.570: INFO: Got endpoints: latency-svc-fdmd6 [742.103706ms]
Jun 11 14:48:50.571: INFO: Created: latency-svc-2w7dd
Jun 11 14:48:50.594: INFO: Created: latency-svc-xdqnq
Jun 11 14:48:50.625: INFO: Got endpoints: latency-svc-gh2mm [758.347482ms]
Jun 11 14:48:50.655: INFO: Created: latency-svc-k5j9k
Jun 11 14:48:50.669: INFO: Got endpoints: latency-svc-d8zb7 [743.57938ms]
Jun 11 14:48:50.707: INFO: Created: latency-svc-wvfrx
Jun 11 14:48:50.716: INFO: Got endpoints: latency-svc-kh546 [747.338667ms]
Jun 11 14:48:50.740: INFO: Created: latency-svc-btz56
Jun 11 14:48:50.769: INFO: Got endpoints: latency-svc-rflp7 [751.180734ms]
Jun 11 14:48:50.791: INFO: Created: latency-svc-jhlbv
Jun 11 14:48:50.822: INFO: Got endpoints: latency-svc-m7pmf [749.528349ms]
Jun 11 14:48:50.852: INFO: Created: latency-svc-x78dv
Jun 11 14:48:50.866: INFO: Got endpoints: latency-svc-6rjs2 [752.346991ms]
Jun 11 14:48:50.890: INFO: Created: latency-svc-z9tln
Jun 11 14:48:50.914: INFO: Got endpoints: latency-svc-stcfs [747.884104ms]
Jun 11 14:48:50.932: INFO: Created: latency-svc-64t2n
Jun 11 14:48:50.964: INFO: Got endpoints: latency-svc-ls2vt [749.195953ms]
Jun 11 14:48:50.987: INFO: Created: latency-svc-8948g
Jun 11 14:48:51.019: INFO: Got endpoints: latency-svc-ghcpt [751.964523ms]
Jun 11 14:48:51.040: INFO: Created: latency-svc-9z4dg
Jun 11 14:48:51.066: INFO: Got endpoints: latency-svc-hgcj4 [740.817204ms]
Jun 11 14:48:51.086: INFO: Created: latency-svc-prd5r
Jun 11 14:48:51.114: INFO: Got endpoints: latency-svc-j67lt [735.471714ms]
Jun 11 14:48:51.138: INFO: Created: latency-svc-vt64r
Jun 11 14:48:51.168: INFO: Got endpoints: latency-svc-tmgph [747.784981ms]
Jun 11 14:48:51.187: INFO: Created: latency-svc-8s5xw
Jun 11 14:48:51.214: INFO: Got endpoints: latency-svc-m2wfv [719.039436ms]
Jun 11 14:48:51.234: INFO: Created: latency-svc-n6pcb
Jun 11 14:48:51.266: INFO: Got endpoints: latency-svc-2w7dd [721.216159ms]
Jun 11 14:48:51.292: INFO: Created: latency-svc-8sm2n
Jun 11 14:48:51.321: INFO: Got endpoints: latency-svc-xdqnq [750.548982ms]
Jun 11 14:48:51.345: INFO: Created: latency-svc-fd2kq
Jun 11 14:48:51.367: INFO: Got endpoints: latency-svc-k5j9k [741.616768ms]
Jun 11 14:48:51.396: INFO: Created: latency-svc-mnswc
Jun 11 14:48:51.418: INFO: Got endpoints: latency-svc-wvfrx [749.319894ms]
Jun 11 14:48:51.453: INFO: Created: latency-svc-vntxs
Jun 11 14:48:51.466: INFO: Got endpoints: latency-svc-btz56 [749.621508ms]
Jun 11 14:48:51.493: INFO: Created: latency-svc-df9lb
Jun 11 14:48:51.518: INFO: Got endpoints: latency-svc-jhlbv [748.818067ms]
Jun 11 14:48:51.551: INFO: Created: latency-svc-wz95k
Jun 11 14:48:51.565: INFO: Got endpoints: latency-svc-x78dv [742.483071ms]
Jun 11 14:48:51.588: INFO: Created: latency-svc-8q5dd
Jun 11 14:48:51.616: INFO: Got endpoints: latency-svc-z9tln [750.239158ms]
Jun 11 14:48:51.636: INFO: Created: latency-svc-hdfb9
Jun 11 14:48:51.664: INFO: Got endpoints: latency-svc-64t2n [749.652179ms]
Jun 11 14:48:51.682: INFO: Created: latency-svc-bjfg7
Jun 11 14:48:51.716: INFO: Got endpoints: latency-svc-8948g [751.692351ms]
Jun 11 14:48:51.734: INFO: Created: latency-svc-f94p6
Jun 11 14:48:51.765: INFO: Got endpoints: latency-svc-9z4dg [746.587753ms]
Jun 11 14:48:51.783: INFO: Created: latency-svc-c22b9
Jun 11 14:48:51.814: INFO: Got endpoints: latency-svc-prd5r [747.491537ms]
Jun 11 14:48:51.833: INFO: Created: latency-svc-psght
Jun 11 14:48:51.868: INFO: Got endpoints: latency-svc-vt64r [754.176728ms]
Jun 11 14:48:51.887: INFO: Created: latency-svc-w5jsv
Jun 11 14:48:51.914: INFO: Got endpoints: latency-svc-8s5xw [746.528773ms]
Jun 11 14:48:51.931: INFO: Created: latency-svc-2f7jg
Jun 11 14:48:51.965: INFO: Got endpoints: latency-svc-n6pcb [750.660274ms]
Jun 11 14:48:51.984: INFO: Created: latency-svc-qlqrt
Jun 11 14:48:52.016: INFO: Got endpoints: latency-svc-8sm2n [750.31415ms]
Jun 11 14:48:52.035: INFO: Created: latency-svc-jxj8g
Jun 11 14:48:52.065: INFO: Got endpoints: latency-svc-fd2kq [743.311115ms]
Jun 11 14:48:52.089: INFO: Created: latency-svc-7lg6w
Jun 11 14:48:52.117: INFO: Got endpoints: latency-svc-mnswc [750.340921ms]
Jun 11 14:48:52.136: INFO: Created: latency-svc-rmc8j
Jun 11 14:48:52.163: INFO: Got endpoints: latency-svc-vntxs [744.850908ms]
Jun 11 14:48:52.198: INFO: Created: latency-svc-j25wt
Jun 11 14:48:52.216: INFO: Got endpoints: latency-svc-df9lb [750.454053ms]
Jun 11 14:48:52.233: INFO: Created: latency-svc-wt4wh
Jun 11 14:48:52.263: INFO: Got endpoints: latency-svc-wz95k [745.693752ms]
Jun 11 14:48:52.290: INFO: Created: latency-svc-ftg77
Jun 11 14:48:52.325: INFO: Got endpoints: latency-svc-8q5dd [760.302942ms]
Jun 11 14:48:52.361: INFO: Created: latency-svc-v746j
Jun 11 14:48:52.372: INFO: Got endpoints: latency-svc-hdfb9 [756.003127ms]
Jun 11 14:48:52.398: INFO: Created: latency-svc-wn5s4
Jun 11 14:48:52.420: INFO: Got endpoints: latency-svc-bjfg7 [755.652792ms]
Jun 11 14:48:52.442: INFO: Created: latency-svc-v7hq5
Jun 11 14:48:52.465: INFO: Got endpoints: latency-svc-f94p6 [749.367617ms]
Jun 11 14:48:52.491: INFO: Created: latency-svc-p9tf8
Jun 11 14:48:52.520: INFO: Got endpoints: latency-svc-c22b9 [754.123828ms]
Jun 11 14:48:52.537: INFO: Created: latency-svc-q45pk
Jun 11 14:48:52.564: INFO: Got endpoints: latency-svc-psght [750.654377ms]
Jun 11 14:48:52.589: INFO: Created: latency-svc-674b5
Jun 11 14:48:52.615: INFO: Got endpoints: latency-svc-w5jsv [746.612816ms]
Jun 11 14:48:52.635: INFO: Created: latency-svc-vhjz8
Jun 11 14:48:52.665: INFO: Got endpoints: latency-svc-2f7jg [750.248961ms]
Jun 11 14:48:52.684: INFO: Created: latency-svc-rsvzp
Jun 11 14:48:52.717: INFO: Got endpoints: latency-svc-qlqrt [751.767802ms]
Jun 11 14:48:52.742: INFO: Created: latency-svc-swckp
Jun 11 14:48:52.766: INFO: Got endpoints: latency-svc-jxj8g [749.812472ms]
Jun 11 14:48:52.787: INFO: Created: latency-svc-hbfwm
Jun 11 14:48:52.814: INFO: Got endpoints: latency-svc-7lg6w [748.786746ms]
Jun 11 14:48:52.837: INFO: Created: latency-svc-lsdqm
Jun 11 14:48:52.867: INFO: Got endpoints: latency-svc-rmc8j [749.434296ms]
Jun 11 14:48:52.886: INFO: Created: latency-svc-4knpr
Jun 11 14:48:52.915: INFO: Got endpoints: latency-svc-j25wt [752.053025ms]
Jun 11 14:48:52.934: INFO: Created: latency-svc-jbxk8
Jun 11 14:48:52.966: INFO: Got endpoints: latency-svc-wt4wh [749.864881ms]
Jun 11 14:48:52.984: INFO: Created: latency-svc-xptkp
Jun 11 14:48:53.015: INFO: Got endpoints: latency-svc-ftg77 [751.417334ms]
Jun 11 14:48:53.035: INFO: Created: latency-svc-zpgqm
Jun 11 14:48:53.066: INFO: Got endpoints: latency-svc-v746j [740.755612ms]
Jun 11 14:48:53.094: INFO: Created: latency-svc-fznn7
Jun 11 14:48:53.115: INFO: Got endpoints: latency-svc-wn5s4 [742.133595ms]
Jun 11 14:48:53.139: INFO: Created: latency-svc-pxvkh
Jun 11 14:48:53.166: INFO: Got endpoints: latency-svc-v7hq5 [745.340502ms]
Jun 11 14:48:53.184: INFO: Created: latency-svc-64jww
Jun 11 14:48:53.214: INFO: Got endpoints: latency-svc-p9tf8 [748.887287ms]
Jun 11 14:48:53.231: INFO: Created: latency-svc-tkl6m
Jun 11 14:48:53.265: INFO: Got endpoints: latency-svc-q45pk [745.181671ms]
Jun 11 14:48:53.290: INFO: Created: latency-svc-9krng
Jun 11 14:48:53.320: INFO: Got endpoints: latency-svc-674b5 [755.048149ms]
Jun 11 14:48:53.341: INFO: Created: latency-svc-zv5wr
Jun 11 14:48:53.369: INFO: Got endpoints: latency-svc-vhjz8 [753.990943ms]
Jun 11 14:48:53.394: INFO: Created: latency-svc-tn8hv
Jun 11 14:48:53.418: INFO: Got endpoints: latency-svc-rsvzp [753.15098ms]
Jun 11 14:48:53.450: INFO: Created: latency-svc-fhgw4
Jun 11 14:48:53.475: INFO: Got endpoints: latency-svc-swckp [757.532096ms]
Jun 11 14:48:53.498: INFO: Created: latency-svc-xvd7d
Jun 11 14:48:53.516: INFO: Got endpoints: latency-svc-hbfwm [749.88836ms]
Jun 11 14:48:53.537: INFO: Created: latency-svc-dp5jw
Jun 11 14:48:53.566: INFO: Got endpoints: latency-svc-lsdqm [751.775009ms]
Jun 11 14:48:53.590: INFO: Created: latency-svc-29gh6
Jun 11 14:48:53.614: INFO: Got endpoints: latency-svc-4knpr [747.14751ms]
Jun 11 14:48:53.643: INFO: Created: latency-svc-p7cn5
Jun 11 14:48:53.667: INFO: Got endpoints: latency-svc-jbxk8 [750.377868ms]
Jun 11 14:48:53.694: INFO: Created: latency-svc-wlthm
Jun 11 14:48:53.714: INFO: Got endpoints: latency-svc-xptkp [747.437414ms]
Jun 11 14:48:53.744: INFO: Created: latency-svc-bj5wj
Jun 11 14:48:53.769: INFO: Got endpoints: latency-svc-zpgqm [752.731365ms]
Jun 11 14:48:53.811: INFO: Created: latency-svc-hdvgx
Jun 11 14:48:53.817: INFO: Got endpoints: latency-svc-fznn7 [750.927837ms]
Jun 11 14:48:53.836: INFO: Created: latency-svc-pbx5x
Jun 11 14:48:53.865: INFO: Got endpoints: latency-svc-pxvkh [750.168895ms]
Jun 11 14:48:53.893: INFO: Created: latency-svc-7h888
Jun 11 14:48:53.916: INFO: Got endpoints: latency-svc-64jww [749.942711ms]
Jun 11 14:48:53.937: INFO: Created: latency-svc-5zh79
Jun 11 14:48:53.967: INFO: Got endpoints: latency-svc-tkl6m [752.639083ms]
Jun 11 14:48:53.987: INFO: Created: latency-svc-zkpjm
Jun 11 14:48:54.026: INFO: Got endpoints: latency-svc-9krng [760.839906ms]
Jun 11 14:48:54.051: INFO: Created: latency-svc-bnjwc
Jun 11 14:48:54.067: INFO: Got endpoints: latency-svc-zv5wr [746.818034ms]
Jun 11 14:48:54.093: INFO: Created: latency-svc-xt5ml
Jun 11 14:48:54.115: INFO: Got endpoints: latency-svc-tn8hv [745.086339ms]
Jun 11 14:48:54.166: INFO: Got endpoints: latency-svc-fhgw4 [747.699229ms]
Jun 11 14:48:54.213: INFO: Got endpoints: latency-svc-xvd7d [738.013052ms]
Jun 11 14:48:54.264: INFO: Got endpoints: latency-svc-dp5jw [748.104076ms]
Jun 11 14:48:54.324: INFO: Got endpoints: latency-svc-29gh6 [757.63734ms]
Jun 11 14:48:54.368: INFO: Got endpoints: latency-svc-p7cn5 [753.807322ms]
Jun 11 14:48:54.417: INFO: Got endpoints: latency-svc-wlthm [750.389491ms]
Jun 11 14:48:54.467: INFO: Got endpoints: latency-svc-bj5wj [753.055791ms]
Jun 11 14:48:54.515: INFO: Got endpoints: latency-svc-hdvgx [746.385032ms]
Jun 11 14:48:54.566: INFO: Got endpoints: latency-svc-pbx5x [748.3825ms]
Jun 11 14:48:54.613: INFO: Got endpoints: latency-svc-7h888 [747.846392ms]
Jun 11 14:48:54.669: INFO: Got endpoints: latency-svc-5zh79 [752.556633ms]
Jun 11 14:48:54.715: INFO: Got endpoints: latency-svc-zkpjm [747.80863ms]
Jun 11 14:48:54.765: INFO: Got endpoints: latency-svc-bnjwc [738.402118ms]
Jun 11 14:48:54.816: INFO: Got endpoints: latency-svc-xt5ml [748.768425ms]
Jun 11 14:48:54.817: INFO: Latencies: [60.120109ms 94.890213ms 119.688849ms 146.286751ms 160.427834ms 212.631364ms 233.508389ms 253.647593ms 275.616856ms 294.752435ms 315.61266ms 319.718003ms 323.282837ms 329.025333ms 336.583247ms 339.030744ms 339.273018ms 340.857841ms 344.204232ms 346.0432ms 346.509676ms 346.827392ms 347.013046ms 347.934958ms 353.662896ms 354.750751ms 358.046443ms 364.731422ms 365.440993ms 367.985312ms 368.084834ms 370.453149ms 373.401713ms 373.613107ms 374.179358ms 375.535856ms 379.675969ms 381.012128ms 383.234761ms 384.41253ms 384.618674ms 385.984924ms 394.988771ms 396.423382ms 396.544154ms 399.652851ms 402.445425ms 403.204476ms 405.767603ms 407.749255ms 409.199586ms 409.823734ms 411.284529ms 413.40693ms 413.973598ms 418.287574ms 421.234328ms 422.67944ms 424.72692ms 426.421187ms 426.553067ms 431.404753ms 453.624787ms 461.151121ms 481.072362ms 510.936683ms 537.436323ms 564.892198ms 590.197621ms 635.921203ms 658.219689ms 686.302733ms 719.039436ms 721.216159ms 723.270951ms 732.091285ms 735.471714ms 736.603652ms 737.20509ms 738.013052ms 738.402118ms 739.109839ms 740.037683ms 740.755612ms 740.817204ms 741.616768ms 742.002723ms 742.103706ms 742.133595ms 742.483071ms 743.311115ms 743.57938ms 743.981213ms 744.037924ms 744.158336ms 744.689234ms 744.850908ms 745.086339ms 745.181671ms 745.340502ms 745.693752ms 745.78918ms 746.385032ms 746.423161ms 746.528773ms 746.587753ms 746.612816ms 746.818034ms 747.14751ms 747.244581ms 747.338667ms 747.437414ms 747.491537ms 747.699229ms 747.784981ms 747.80863ms 747.846392ms 747.884104ms 748.104076ms 748.279726ms 748.3825ms 748.700744ms 748.768425ms 748.786746ms 748.818067ms 748.887287ms 749.04712ms 749.195953ms 749.261203ms 749.319894ms 749.367617ms 749.434296ms 749.528349ms 749.621508ms 749.643878ms 749.652179ms 749.812472ms 749.864881ms 749.88836ms 749.942711ms 750.168895ms 750.231607ms 750.239158ms 750.248961ms 750.31415ms 750.340921ms 750.377868ms 750.389491ms 750.454053ms 750.548982ms 750.654377ms 750.660274ms 750.793548ms 750.927837ms 751.166763ms 751.180734ms 751.182412ms 751.417334ms 751.451356ms 751.692351ms 751.72973ms 751.767802ms 751.775009ms 751.964523ms 752.053025ms 752.183197ms 752.261478ms 752.346991ms 752.386848ms 752.556633ms 752.594925ms 752.639083ms 752.731365ms 752.810729ms 753.055791ms 753.15098ms 753.807322ms 753.990943ms 754.123828ms 754.176728ms 754.32568ms 755.048149ms 755.320624ms 755.652792ms 755.763183ms 756.003127ms 756.35814ms 757.532096ms 757.63734ms 757.951135ms 758.347482ms 758.733157ms 758.795145ms 759.532009ms 759.753323ms 760.302942ms 760.839906ms 762.450222ms 769.752724ms 778.191862ms]
Jun 11 14:48:54.817: INFO: 50 %ile: 745.693752ms
Jun 11 14:48:54.817: INFO: 90 %ile: 754.32568ms
Jun 11 14:48:54.818: INFO: 99 %ile: 769.752724ms
Jun 11 14:48:54.818: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jun 11 14:48:54.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3289" for this suite.

â€¢ [SLOW TEST:10.836 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":191,"skipped":3825,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:48:54.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9837
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jun 11 14:48:54.922: INFO: Found 0 stateful pods, waiting for 3
Jun 11 14:49:04.939: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:49:04.939: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:49:04.940: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jun 11 14:49:04.992: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 11 14:49:15.050: INFO: Updating stateful set ss2
Jun 11 14:49:15.064: INFO: Waiting for Pod statefulset-9837/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jun 11 14:49:25.115: INFO: Found 1 stateful pods, waiting for 3
Jun 11 14:49:35.126: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:49:35.126: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 14:49:35.126: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 11 14:49:35.164: INFO: Updating stateful set ss2
Jun 11 14:49:35.174: INFO: Waiting for Pod statefulset-9837/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jun 11 14:49:45.218: INFO: Updating stateful set ss2
Jun 11 14:49:45.233: INFO: Waiting for StatefulSet statefulset-9837/ss2 to complete update
Jun 11 14:49:45.233: INFO: Waiting for Pod statefulset-9837/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 14:49:55.247: INFO: Deleting all statefulset in ns statefulset-9837
Jun 11 14:49:55.253: INFO: Scaling statefulset ss2 to 0
Jun 11 14:50:05.294: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 14:50:05.301: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 14:50:05.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9837" for this suite.

â€¢ [SLOW TEST:70.489 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":192,"skipped":3826,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:50:05.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-2498-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jun 11 14:50:05.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2498" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":193,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:50:05.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 14:50:07.543: INFO: DNS probes using dns-3589/dns-test-09347c77-e9dd-422c-b1bc-b60ad7df2b9f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 14:50:07.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3589" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":194,"skipped":3859,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:50:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-b4e35aae-3ab3-44f4-854e-6bfec75d4a58
STEP: Creating a pod to test consume secrets
Jun 11 14:50:07.643: INFO: Waiting up to 5m0s for pod "pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b" in namespace "secrets-6863" to be "Succeeded or Failed"
Jun 11 14:50:07.650: INFO: Pod "pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.331795ms
Jun 11 14:50:09.655: INFO: Pod "pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012003475s
Jun 11 14:50:11.664: INFO: Pod "pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021072923s
STEP: Saw pod success
Jun 11 14:50:11.665: INFO: Pod "pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b" satisfied condition "Succeeded or Failed"
Jun 11 14:50:11.670: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 14:50:11.695: INFO: Waiting for pod pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b to disappear
Jun 11 14:50:11.699: INFO: Pod pod-secrets-4c955f22-e5d0-407c-9ee1-bf8d29d6bc9b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 14:50:11.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6863" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":195,"skipped":3860,"failed":0}
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:50:11.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jun 11 14:50:11.748: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 14:51:11.797: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:51:11.804: INFO: Starting informer...
STEP: Starting pod...
Jun 11 14:51:12.031: INFO: Pod is running on ip-172-31-41-158. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 11 14:51:12.064: INFO: Pod wasn't evicted. Proceeding
Jun 11 14:51:12.064: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 11 14:52:27.098: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:52:27.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3250" for this suite.

â€¢ [SLOW TEST:135.412 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":196,"skipped":3864,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:52:27.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:52:27.205: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 14:52:27.223: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:27.223: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:27.224: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:27.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:52:27.229: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 14:52:28.239: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:28.239: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:28.239: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:28.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:52:28.245: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 14:52:29.238: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:29.238: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:29.238: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:29.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 14:52:29.244: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 11 14:52:29.302: INFO: Wrong image for pod: daemon-set-9m7hl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun 11 14:52:29.302: INFO: Wrong image for pod: daemon-set-xbgnw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun 11 14:52:29.310: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:29.310: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:29.310: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:30.322: INFO: Wrong image for pod: daemon-set-xbgnw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun 11 14:52:30.329: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:30.329: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:30.329: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:31.326: INFO: Pod daemon-set-pbxn6 is not available
Jun 11 14:52:31.326: INFO: Wrong image for pod: daemon-set-xbgnw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.36, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun 11 14:52:31.337: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:31.338: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:31.338: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:32.330: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:32.330: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:32.330: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:33.341: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:33.341: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:33.341: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:34.328: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:34.328: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:34.328: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:35.327: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:35.327: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:35.327: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.323: INFO: Pod daemon-set-bws84 is not available
Jun 11 14:52:36.329: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.329: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.329: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 11 14:52:36.336: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.337: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.337: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:36.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 14:52:36.344: INFO: Node ip-172-31-41-158 is running 0 daemon pod, expected 1
Jun 11 14:52:37.357: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:37.357: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:37.357: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 14:52:37.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 14:52:37.363: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6950, will wait for the garbage collector to delete the pods
Jun 11 14:52:37.457: INFO: Deleting DaemonSet.extensions daemon-set took: 8.899445ms
Jun 11 14:52:37.558: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.180889ms
Jun 11 14:52:39.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 14:52:39.468: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 14:52:39.473: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31765"},"items":null}

Jun 11 14:52:39.477: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31765"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:52:39.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6950" for this suite.

â€¢ [SLOW TEST:12.385 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":197,"skipped":3878,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:52:39.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 14:52:40.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 14:52:43.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 11 14:52:43.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 14:52:43.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4050" for this suite.
STEP: Destroying namespace "webhook-4050-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":198,"skipped":3878,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:52:43.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jun 11 14:58:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2430" for this suite.

â€¢ [SLOW TEST:318.145 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":199,"skipped":3891,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 11 14:58:01.644: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 11 14:58:01.662: INFO: Waiting for terminating namespaces to be deleted...
Jun 11 14:58:01.668: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-27-214 before test
Jun 11 14:58:01.681: INFO: calico-kube-controllers-68884f975d-h49sk from calico-system started at 2022-06-11 13:45:50 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 11 14:58:01.681: INFO: calico-node-6kzfb from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:58:01.681: INFO: calico-typha-7bb48cc99f-25j2w from calico-system started at 2022-06-11 13:45:39 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:58:01.681: INFO: kube-proxy-9l4fz from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:58:01.681: INFO: nginx-proxy-ip-172-31-27-214 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:58:01.681: INFO: sonobuoy-e2e-job-f40c26145eda47fd from sonobuoy started at 2022-06-11 13:53:19 +0000 UTC (2 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container e2e ready: true, restart count 0
Jun 11 14:58:01.681: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:58:01.681: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-pjdgw from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:58:01.681: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:58:01.682: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 11 14:58:01.682: INFO: tigera-operator-5fb55776df-zr59s from tigera-operator started at 2022-06-11 13:45:20 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.682: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 11 14:58:01.682: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-41-158 before test
Jun 11 14:58:01.696: INFO: calico-apiserver-697d674bb5-vdt6n from calico-apiserver started at 2022-06-11 14:51:12 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container calico-apiserver ready: true, restart count 0
Jun 11 14:58:01.696: INFO: calico-node-7hz6q from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container calico-node ready: true, restart count 0
Jun 11 14:58:01.696: INFO: calico-typha-7bb48cc99f-nt5gz from calico-system started at 2022-06-11 13:45:31 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container calico-typha ready: true, restart count 0
Jun 11 14:58:01.696: INFO: forbid-27582653-j22zm from cronjob-2430 started at 2022-06-11 14:53:00 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container c ready: false, restart count 0
Jun 11 14:58:01.696: INFO: kube-proxy-ljlz6 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 11 14:58:01.696: INFO: nginx-proxy-ip-172-31-41-158 from kube-system started at 2022-06-11 13:32:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 11 14:58:01.696: INFO: sonobuoy from sonobuoy started at 2022-06-11 13:53:15 +0000 UTC (1 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 11 14:58:01.696: INFO: sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-gzshn from sonobuoy started at 2022-06-11 13:53:20 +0000 UTC (2 container statuses recorded)
Jun 11 14:58:01.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 11 14:58:01.696: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node ip-172-31-27-214
STEP: verifying the node has the label node ip-172-31-41-158
Jun 11 14:58:01.802: INFO: Pod calico-apiserver-697d674bb5-vdt6n requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.802: INFO: Pod calico-kube-controllers-68884f975d-h49sk requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.802: INFO: Pod calico-node-6kzfb requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.802: INFO: Pod calico-node-7hz6q requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.803: INFO: Pod calico-typha-7bb48cc99f-25j2w requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.803: INFO: Pod calico-typha-7bb48cc99f-nt5gz requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.803: INFO: Pod forbid-27582653-j22zm requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.803: INFO: Pod kube-proxy-9l4fz requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.804: INFO: Pod kube-proxy-ljlz6 requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.804: INFO: Pod nginx-proxy-ip-172-31-27-214 requesting resource cpu=25m on Node ip-172-31-27-214
Jun 11 14:58:01.804: INFO: Pod nginx-proxy-ip-172-31-41-158 requesting resource cpu=25m on Node ip-172-31-41-158
Jun 11 14:58:01.804: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.806: INFO: Pod sonobuoy-e2e-job-f40c26145eda47fd requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.806: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-gzshn requesting resource cpu=0m on Node ip-172-31-41-158
Jun 11 14:58:01.806: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e98335962f3403f-pjdgw requesting resource cpu=0m on Node ip-172-31-27-214
Jun 11 14:58:01.806: INFO: Pod tigera-operator-5fb55776df-zr59s requesting resource cpu=0m on Node ip-172-31-27-214
STEP: Starting Pods to consume most of the cluster CPU.
Jun 11 14:58:01.807: INFO: Creating a pod which consumes cpu=1382m on Node ip-172-31-27-214
Jun 11 14:58:01.824: INFO: Creating a pod which consumes cpu=1382m on Node ip-172-31-41-158
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f.16f798e1604cd135], Reason = [Scheduled], Message = [Successfully assigned sched-pred-10/filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f to ip-172-31-27-214]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f.16f798e18975dedb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f.16f798e18af4efa8], Reason = [Created], Message = [Created container filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f.16f798e18f75964a], Reason = [Started], Message = [Started container filler-pod-ce7d9cb3-db79-4a46-a7e6-2b2928d1a08f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa9d8233-96ce-428c-8712-208d876ef610.16f798e1614c2748], Reason = [Scheduled], Message = [Successfully assigned sched-pred-10/filler-pod-fa9d8233-96ce-428c-8712-208d876ef610 to ip-172-31-41-158]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa9d8233-96ce-428c-8712-208d876ef610.16f798e18ed41b21], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa9d8233-96ce-428c-8712-208d876ef610.16f798e1908fc077], Reason = [Created], Message = [Created container filler-pod-fa9d8233-96ce-428c-8712-208d876ef610]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa9d8233-96ce-428c-8712-208d876ef610.16f798e1949bd43b], Reason = [Started], Message = [Started container filler-pod-fa9d8233-96ce-428c-8712-208d876ef610]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f798e1d9cf5ff7], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.]
STEP: removing the label node off the node ip-172-31-27-214
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-41-158
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:58:04.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-10" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":200,"skipped":3891,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:04.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jun 11 14:58:05.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7969" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":201,"skipped":3908,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:05.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:58:05.080: INFO: Creating simple deployment test-new-deployment
Jun 11 14:58:05.099: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 14:58:07.154: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8688  35bee91b-2017-4d8c-997f-201b7c1f9618 32896 3 2022-06-11 14:58:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-06-11 14:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046ecd48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-11 14:58:06 +0000 UTC,LastTransitionTime:2022-06-11 14:58:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-06-11 14:58:06 +0000 UTC,LastTransitionTime:2022-06-11 14:58:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 14:58:07.161: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-8688  aff2af8c-20e7-4fb8-8a98-ac73a3be7f18 32895 2 2022-06-11 14:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 35bee91b-2017-4d8c-997f-201b7c1f9618 0xc0046ed2a7 0xc0046ed2a8}] []  [{kube-controller-manager Update apps/v1 2022-06-11 14:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35bee91b-2017-4d8c-997f-201b7c1f9618\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 14:58:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046ed338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 14:58:07.169: INFO: Pod "test-new-deployment-55df494869-6qc7w" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-6qc7w test-new-deployment-55df494869- deployment-8688  24b08fe8-e2cf-4895-a05f-29d6c49ac394 32900 0 2022-06-11 14:58:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 aff2af8c-20e7-4fb8-8a98-ac73a3be7f18 0xc0046ed737 0xc0046ed738}] []  [{kube-controller-manager Update v1 2022-06-11 14:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aff2af8c-20e7-4fb8-8a98-ac73a3be7f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkpzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkpzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:58:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 14:58:07.170: INFO: Pod "test-new-deployment-55df494869-dc5lc" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-dc5lc test-new-deployment-55df494869- deployment-8688  278cc3b0-6a5d-4f77-a326-8de21a5998da 32876 0 2022-06-11 14:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:b274e99838853171327394d1fbf440263a871a9bcb859b558f54a2a70baccc63 cni.projectcalico.org/podIP:192.168.203.24/32 cni.projectcalico.org/podIPs:192.168.203.24/32] [{apps/v1 ReplicaSet test-new-deployment-55df494869 aff2af8c-20e7-4fb8-8a98-ac73a3be7f18 0xc0046ed8a0 0xc0046ed8a1}] []  [{Go-http-client Update v1 2022-06-11 14:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-11 14:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aff2af8c-20e7-4fb8-8a98-ac73a3be7f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 14:58:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfz7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfz7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:58:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:58:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 14:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.24,StartTime:2022-06-11 14:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 14:58:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4d71c37fd1f53a7dcd6d26dd326c3fc8d7440bbc7ae218da63bb6b21631f820d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 14:58:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8688" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":202,"skipped":3912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:07.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-9469
Jun 11 14:58:07.259: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:58:09.268: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun 11 14:58:09.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 11 14:58:09.473: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 11 14:58:09.473: INFO: stdout: "iptables"
Jun 11 14:58:09.473: INFO: proxyMode: iptables
Jun 11 14:58:09.490: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 11 14:58:09.496: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9469
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9469
I0611 14:58:09.530887      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9469, replica count: 3
I0611 14:58:12.582027      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 14:58:12.605: INFO: Creating new exec pod
Jun 11 14:58:15.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec execpod-affinity8zkl6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jun 11 14:58:15.842: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 11 14:58:15.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:58:15.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec execpod-affinity8zkl6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.131.207 80'
Jun 11 14:58:16.022: INFO: stderr: "+ nc -v -t -w 2 10.96.131.207 80\nConnection to 10.96.131.207 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jun 11 14:58:16.022: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 14:58:16.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec execpod-affinity8zkl6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.131.207:80/ ; done'
Jun 11 14:58:16.281: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n"
Jun 11 14:58:16.281: INFO: stdout: "\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q\naffinity-clusterip-timeout-fwn6q"
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.281: INFO: Received response from host: affinity-clusterip-timeout-fwn6q
Jun 11 14:58:16.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec execpod-affinity8zkl6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.131.207:80/'
Jun 11 14:58:16.493: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n"
Jun 11 14:58:16.493: INFO: stdout: "affinity-clusterip-timeout-fwn6q"
Jun 11 14:58:36.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-9469 exec execpod-affinity8zkl6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.131.207:80/'
Jun 11 14:58:36.746: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.131.207:80/\n"
Jun 11 14:58:36.746: INFO: stdout: "affinity-clusterip-timeout-5bqj6"
Jun 11 14:58:36.746: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9469, will wait for the garbage collector to delete the pods
Jun 11 14:58:36.837: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.522029ms
Jun 11 14:58:36.938: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.011445ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 14:58:39.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9469" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:32.293 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":203,"skipped":3950,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:39.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 14:58:39.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-499" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":204,"skipped":3951,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:39.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jun 11 14:58:39.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5839" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":205,"skipped":3961,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:39.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-99b66d13-65ec-497c-9493-8c33b0c6f3e4
STEP: Creating a pod to test consume configMaps
Jun 11 14:58:39.691: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782" in namespace "projected-1061" to be "Succeeded or Failed"
Jun 11 14:58:39.695: INFO: Pod "pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620175ms
Jun 11 14:58:41.704: INFO: Pod "pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013354497s
Jun 11 14:58:43.721: INFO: Pod "pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029937895s
STEP: Saw pod success
Jun 11 14:58:43.722: INFO: Pod "pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782" satisfied condition "Succeeded or Failed"
Jun 11 14:58:43.727: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 14:58:43.765: INFO: Waiting for pod pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782 to disappear
Jun 11 14:58:43.769: INFO: Pod pod-projected-configmaps-a1c074eb-5f39-4b8b-98bc-52028dc27782 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 14:58:43.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1061" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3965,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 11 14:58:43.832: INFO: Waiting up to 5m0s for pod "pod-7effd831-abd6-4fc6-856e-8512383e1e3e" in namespace "emptydir-8652" to be "Succeeded or Failed"
Jun 11 14:58:43.840: INFO: Pod "pod-7effd831-abd6-4fc6-856e-8512383e1e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.576185ms
Jun 11 14:58:45.846: INFO: Pod "pod-7effd831-abd6-4fc6-856e-8512383e1e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013671179s
Jun 11 14:58:47.872: INFO: Pod "pod-7effd831-abd6-4fc6-856e-8512383e1e3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039596549s
STEP: Saw pod success
Jun 11 14:58:47.872: INFO: Pod "pod-7effd831-abd6-4fc6-856e-8512383e1e3e" satisfied condition "Succeeded or Failed"
Jun 11 14:58:47.876: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-7effd831-abd6-4fc6-856e-8512383e1e3e container test-container: <nil>
STEP: delete the pod
Jun 11 14:58:47.910: INFO: Waiting for pod pod-7effd831-abd6-4fc6-856e-8512383e1e3e to disappear
Jun 11 14:58:47.916: INFO: Pod pod-7effd831-abd6-4fc6-856e-8512383e1e3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 14:58:47.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8652" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3965,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:47.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jun 11 14:58:47.989: INFO: The status of Pod labelsupdate5475d642-13d5-4ff9-a7f0-1205fe701852 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:58:49.996: INFO: The status of Pod labelsupdate5475d642-13d5-4ff9-a7f0-1205fe701852 is Running (Ready = true)
Jun 11 14:58:50.528: INFO: Successfully updated pod "labelsupdate5475d642-13d5-4ff9-a7f0-1205fe701852"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 14:58:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8652" for this suite.

â€¢ [SLOW TEST:6.664 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":208,"skipped":3966,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:58:54.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 11 14:58:54.698: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 14:59:54.769: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:59:54.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:59:54.836: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun 11 14:59:54.841: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jun 11 14:59:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4184" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jun 11 14:59:54.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2433" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

â€¢ [SLOW TEST:60.377 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":209,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:59:54.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6ab929f2-6591-4126-8089-36d578542ff1
STEP: Creating the pod
Jun 11 14:59:55.038: INFO: The status of Pod pod-projected-configmaps-6cc605bc-5314-4a0c-af04-c7d43193e14e is Pending, waiting for it to be Running (with Ready = true)
Jun 11 14:59:57.045: INFO: The status of Pod pod-projected-configmaps-6cc605bc-5314-4a0c-af04-c7d43193e14e is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-6ab929f2-6591-4126-8089-36d578542ff1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 14:59:59.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4038" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:59:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 14:59:59.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-688 version'
Jun 11 14:59:59.249: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun 11 14:59:59.249: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.1\", GitCommit:\"3ddd0f45aa91e2f30c70734b175631bec5b5825a\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:26:19Z\", GoVersion:\"go1.18.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.1\", GitCommit:\"3ddd0f45aa91e2f30c70734b175631bec5b5825a\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:18:48Z\", GoVersion:\"go1.18.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 14:59:59.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-688" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":211,"skipped":4101,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 14:59:59.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 15:00:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7799" for this suite.

â€¢ [SLOW TEST:60.088 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":4111,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:00:59.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jun 11 15:00:59.395: INFO: Major version: 1
STEP: Confirm minor version
Jun 11 15:00:59.396: INFO: cleanMinorVersion: 24
Jun 11 15:00:59.396: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jun 11 15:00:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1705" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":213,"skipped":4113,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:00:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 11 15:00:59.484: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 15:01:59.539: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jun 11 15:01:59.576: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 11 15:01:59.584: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 11 15:01:59.618: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 11 15:01:59.628: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jun 11 15:02:13.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4851" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

â€¢ [SLOW TEST:74.374 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":214,"skipped":4120,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:02:13.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 11 15:02:13.842: INFO: Waiting up to 5m0s for pod "pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011" in namespace "emptydir-2007" to be "Succeeded or Failed"
Jun 11 15:02:13.851: INFO: Pod "pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011": Phase="Pending", Reason="", readiness=false. Elapsed: 8.240684ms
Jun 11 15:02:15.858: INFO: Pod "pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01571089s
Jun 11 15:02:17.867: INFO: Pod "pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024843335s
STEP: Saw pod success
Jun 11 15:02:17.867: INFO: Pod "pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011" satisfied condition "Succeeded or Failed"
Jun 11 15:02:17.873: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011 container test-container: <nil>
STEP: delete the pod
Jun 11 15:02:17.913: INFO: Waiting for pod pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011 to disappear
Jun 11 15:02:17.919: INFO: Pod pod-6b6d1d1e-ea95-4406-8758-6d6b8cf61011 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:02:17.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2007" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":215,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:02:17.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun 11 15:02:18.028: INFO: observed Pod pod-test in namespace pods-6279 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 11 15:02:18.034: INFO: observed Pod pod-test in namespace pods-6279 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  }]
Jun 11 15:02:18.056: INFO: observed Pod pod-test in namespace pods-6279 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  }]
Jun 11 15:02:18.613: INFO: observed Pod pod-test in namespace pods-6279 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  }]
Jun 11 15:02:19.880: INFO: Found Pod pod-test in namespace pods-6279 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:02:18 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun 11 15:02:19.937: INFO: observed event type MODIFIED
Jun 11 15:02:21.855: INFO: observed event type MODIFIED
Jun 11 15:02:22.077: INFO: observed event type MODIFIED
Jun 11 15:02:22.860: INFO: observed event type MODIFIED
Jun 11 15:02:22.869: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 15:02:22.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6279" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":216,"skipped":4157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:02:22.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6757
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-6757
Jun 11 15:02:22.972: INFO: Found 0 stateful pods, waiting for 1
Jun 11 15:02:32.981: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jun 11 15:02:33.012: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jun 11 15:02:33.026: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jun 11 15:02:33.029: INFO: Observed &StatefulSet event: ADDED
Jun 11 15:02:33.029: INFO: Found Statefulset ss in namespace statefulset-6757 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 11 15:02:33.029: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jun 11 15:02:33.029: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 11 15:02:33.043: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jun 11 15:02:33.046: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 15:02:33.047: INFO: Deleting all statefulset in ns statefulset-6757
Jun 11 15:02:33.052: INFO: Scaling statefulset ss to 0
Jun 11 15:02:43.077: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:02:43.081: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 15:02:43.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6757" for this suite.

â€¢ [SLOW TEST:20.204 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":217,"skipped":4304,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:02:43.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 11 15:02:43.181: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34417 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:02:43.182: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34418 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:02:43.182: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34420 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 11 15:02:53.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34464 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:02:53.229: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34466 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:02:53.230: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5969  6dadccb9-6c6e-4343-84ac-5c58ac2b700b 34468 0 2022-06-11 15:02:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-11 15:02:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jun 11 15:02:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5969" for this suite.

â€¢ [SLOW TEST:10.130 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":218,"skipped":4315,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:02:53.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5213
Jun 11 15:02:53.315: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:02:55.322: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun 11 15:02:55.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 11 15:02:55.541: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 11 15:02:55.541: INFO: stdout: "iptables"
Jun 11 15:02:55.541: INFO: proxyMode: iptables
Jun 11 15:02:55.556: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 11 15:02:55.560: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5213
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5213
I0611 15:02:55.588117      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5213, replica count: 3
I0611 15:02:58.640355      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:02:58.667: INFO: Creating new exec pod
Jun 11 15:03:01.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jun 11 15:03:01.893: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 11 15:03:01.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:03:01.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.245.28 80'
Jun 11 15:03:02.086: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.245.28 80\nConnection to 10.104.245.28 80 port [tcp/http] succeeded!\n"
Jun 11 15:03:02.086: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:03:02.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 31814'
Jun 11 15:03:02.291: INFO: stderr: "+ nc -v -t -w 2 172.31.27.214 31814\n+ echo hostName\nConnection to 172.31.27.214 31814 port [tcp/*] succeeded!\n"
Jun 11 15:03:02.291: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:03:02.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 31814'
Jun 11 15:03:02.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.41.158 31814\nConnection to 172.31.41.158 31814 port [tcp/*] succeeded!\n"
Jun 11 15:03:02.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:03:02.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.214:31814/ ; done'
Jun 11 15:03:02.763: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n"
Jun 11 15:03:02.763: INFO: stdout: "\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4\naffinity-nodeport-timeout-2hgf4"
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.763: INFO: Received response from host: affinity-nodeport-timeout-2hgf4
Jun 11 15:03:02.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.27.214:31814/'
Jun 11 15:03:02.942: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n"
Jun 11 15:03:02.942: INFO: stdout: "affinity-nodeport-timeout-2hgf4"
Jun 11 15:03:22.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5213 exec execpod-affinityfsmzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.27.214:31814/'
Jun 11 15:03:23.155: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.27.214:31814/\n"
Jun 11 15:03:23.155: INFO: stdout: "affinity-nodeport-timeout-dq9tg"
Jun 11 15:03:23.155: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5213, will wait for the garbage collector to delete the pods
Jun 11 15:03:23.238: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.176208ms
Jun 11 15:03:23.339: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.169924ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:03:25.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5213" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:32.259 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":219,"skipped":4318,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:03:25.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7395
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7395
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7395
Jun 11 15:03:25.566: INFO: Found 0 stateful pods, waiting for 1
Jun 11 15:03:35.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 11 15:03:35.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 15:03:35.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 15:03:35.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 15:03:35.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 15:03:35.781: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 11 15:03:45.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 15:03:45.790: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:03:45.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999954s
Jun 11 15:03:46.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993379316s
Jun 11 15:03:47.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988315656s
Jun 11 15:03:48.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983490196s
Jun 11 15:03:49.858: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974443552s
Jun 11 15:03:50.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.949883514s
Jun 11 15:03:51.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.941353887s
Jun 11 15:03:52.891: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.930677777s
Jun 11 15:03:53.902: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.917663843s
Jun 11 15:03:54.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 907.291618ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7395
Jun 11 15:03:55.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 15:03:56.120: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 15:03:56.120: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 15:03:56.120: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 15:03:56.125: INFO: Found 1 stateful pods, waiting for 3
Jun 11 15:04:06.138: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 15:04:06.138: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 15:04:06.138: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 11 15:04:06.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 15:04:06.331: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 15:04:06.331: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 15:04:06.331: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 15:04:06.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 15:04:06.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 15:04:06.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 15:04:06.544: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 15:04:06.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 11 15:04:06.750: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 11 15:04:06.750: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 11 15:04:06.750: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 11 15:04:06.750: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:04:06.756: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 11 15:04:16.772: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 15:04:16.772: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 15:04:16.773: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 11 15:04:16.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999959s
Jun 11 15:04:17.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994336586s
Jun 11 15:04:18.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984110876s
Jun 11 15:04:19.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975017712s
Jun 11 15:04:20.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962662211s
Jun 11 15:04:21.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956072154s
Jun 11 15:04:22.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948634651s
Jun 11 15:04:23.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941248141s
Jun 11 15:04:24.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930573542s
Jun 11 15:04:25.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 923.747949ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7395
Jun 11 15:04:26.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 15:04:27.081: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 15:04:27.081: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 15:04:27.081: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 15:04:27.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 15:04:27.271: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 15:04:27.271: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 15:04:27.271: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 15:04:27.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=statefulset-7395 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 11 15:04:27.479: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 11 15:04:27.479: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 11 15:04:27.479: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 11 15:04:27.479: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 15:04:37.505: INFO: Deleting all statefulset in ns statefulset-7395
Jun 11 15:04:37.510: INFO: Scaling statefulset ss to 0
Jun 11 15:04:37.524: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:04:37.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 15:04:37.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7395" for this suite.

â€¢ [SLOW TEST:72.052 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":220,"skipped":4335,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:04:37.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jun 11 15:04:37.608: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 11 15:04:42.620: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 15:04:42.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6432" for this suite.

â€¢ [SLOW TEST:5.124 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":221,"skipped":4339,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:04:42.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jun 11 15:04:42.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 create -f -'
Jun 11 15:04:44.301: INFO: stderr: ""
Jun 11 15:04:44.301: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 15:04:44.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 15:04:44.506: INFO: stderr: ""
Jun 11 15:04:44.506: INFO: stdout: "update-demo-nautilus-4j5bf update-demo-nautilus-jkrgr "
Jun 11 15:04:44.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:44.672: INFO: stderr: ""
Jun 11 15:04:44.672: INFO: stdout: ""
Jun 11 15:04:44.672: INFO: update-demo-nautilus-4j5bf is created but not running
Jun 11 15:04:49.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 15:04:49.794: INFO: stderr: ""
Jun 11 15:04:49.794: INFO: stdout: "update-demo-nautilus-4j5bf update-demo-nautilus-jkrgr "
Jun 11 15:04:49.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:49.895: INFO: stderr: ""
Jun 11 15:04:49.896: INFO: stdout: "true"
Jun 11 15:04:49.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:49.987: INFO: stderr: ""
Jun 11 15:04:49.987: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:49.987: INFO: validating pod update-demo-nautilus-4j5bf
Jun 11 15:04:49.995: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:49.995: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:49.995: INFO: update-demo-nautilus-4j5bf is verified up and running
Jun 11 15:04:49.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-jkrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:50.069: INFO: stderr: ""
Jun 11 15:04:50.069: INFO: stdout: "true"
Jun 11 15:04:50.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-jkrgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:50.149: INFO: stderr: ""
Jun 11 15:04:50.149: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:50.149: INFO: validating pod update-demo-nautilus-jkrgr
Jun 11 15:04:50.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:50.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:50.156: INFO: update-demo-nautilus-jkrgr is verified up and running
STEP: scaling down the replication controller
Jun 11 15:04:50.158: INFO: scanned /root for discovery docs: <nil>
Jun 11 15:04:50.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 11 15:04:51.305: INFO: stderr: ""
Jun 11 15:04:51.305: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 15:04:51.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 15:04:51.385: INFO: stderr: ""
Jun 11 15:04:51.385: INFO: stdout: "update-demo-nautilus-4j5bf "
Jun 11 15:04:51.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:51.473: INFO: stderr: ""
Jun 11 15:04:51.473: INFO: stdout: "true"
Jun 11 15:04:51.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:51.570: INFO: stderr: ""
Jun 11 15:04:51.570: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:51.570: INFO: validating pod update-demo-nautilus-4j5bf
Jun 11 15:04:51.575: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:51.575: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:51.575: INFO: update-demo-nautilus-4j5bf is verified up and running
STEP: scaling up the replication controller
Jun 11 15:04:51.577: INFO: scanned /root for discovery docs: <nil>
Jun 11 15:04:51.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 11 15:04:52.739: INFO: stderr: ""
Jun 11 15:04:52.740: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 11 15:04:52.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 15:04:52.820: INFO: stderr: ""
Jun 11 15:04:52.820: INFO: stdout: "update-demo-nautilus-4j5bf update-demo-nautilus-s5jl9 "
Jun 11 15:04:52.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:52.899: INFO: stderr: ""
Jun 11 15:04:52.899: INFO: stdout: "true"
Jun 11 15:04:52.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:52.982: INFO: stderr: ""
Jun 11 15:04:52.982: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:52.982: INFO: validating pod update-demo-nautilus-4j5bf
Jun 11 15:04:52.988: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:52.988: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:52.988: INFO: update-demo-nautilus-4j5bf is verified up and running
Jun 11 15:04:52.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-s5jl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:53.062: INFO: stderr: ""
Jun 11 15:04:53.062: INFO: stdout: ""
Jun 11 15:04:53.062: INFO: update-demo-nautilus-s5jl9 is created but not running
Jun 11 15:04:58.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 11 15:04:58.157: INFO: stderr: ""
Jun 11 15:04:58.157: INFO: stdout: "update-demo-nautilus-4j5bf update-demo-nautilus-s5jl9 "
Jun 11 15:04:58.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:58.259: INFO: stderr: ""
Jun 11 15:04:58.259: INFO: stdout: "true"
Jun 11 15:04:58.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-4j5bf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:58.345: INFO: stderr: ""
Jun 11 15:04:58.345: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:58.345: INFO: validating pod update-demo-nautilus-4j5bf
Jun 11 15:04:58.351: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:58.351: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:58.351: INFO: update-demo-nautilus-4j5bf is verified up and running
Jun 11 15:04:58.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-s5jl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 11 15:04:58.445: INFO: stderr: ""
Jun 11 15:04:58.445: INFO: stdout: "true"
Jun 11 15:04:58.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods update-demo-nautilus-s5jl9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 11 15:04:58.552: INFO: stderr: ""
Jun 11 15:04:58.552: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun 11 15:04:58.552: INFO: validating pod update-demo-nautilus-s5jl9
Jun 11 15:04:58.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 11 15:04:58.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 11 15:04:58.559: INFO: update-demo-nautilus-s5jl9 is verified up and running
STEP: using delete to clean up resources
Jun 11 15:04:58.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 delete --grace-period=0 --force -f -'
Jun 11 15:04:58.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 15:04:58.648: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 11 15:04:58.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get rc,svc -l name=update-demo --no-headers'
Jun 11 15:04:58.778: INFO: stderr: "No resources found in kubectl-5321 namespace.\n"
Jun 11 15:04:58.778: INFO: stdout: ""
Jun 11 15:04:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-5321 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 15:04:58.916: INFO: stderr: ""
Jun 11 15:04:58.916: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:04:58.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5321" for this suite.

â€¢ [SLOW TEST:16.251 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":222,"skipped":4341,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:04:58.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 15:05:03.033: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jun 11 15:05:03.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3652" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":223,"skipped":4349,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:03.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 15:05:05.208: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-094db8ae-3684-4586-bdfc-a66dd7a71f7a contains '' instead of 'foo.example.com.'
Jun 11 15:05:05.208: INFO: Lookups using dns-3419/dns-test-094db8ae-3684-4586-bdfc-a66dd7a71f7a failed for: [jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:10.220: INFO: DNS probes using dns-test-094db8ae-3684-4586-bdfc-a66dd7a71f7a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 15:05:14.287: INFO: File wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:14.295: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:14.295: INFO: Lookups using dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 failed for: [wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:19.305: INFO: File wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:19.310: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:19.310: INFO: Lookups using dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 failed for: [wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:24.305: INFO: File wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:24.313: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:24.313: INFO: Lookups using dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 failed for: [wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:29.303: INFO: File wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:29.309: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:29.309: INFO: Lookups using dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 failed for: [wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:34.301: INFO: File wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:34.307: INFO: File jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local from pod  dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 11 15:05:34.307: INFO: Lookups using dns-3419/dns-test-9936e45f-466a-483c-9af9-78464e39d556 failed for: [wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local]

Jun 11 15:05:39.306: INFO: DNS probes using dns-test-9936e45f-466a-483c-9af9-78464e39d556 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3419.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3419.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 15:05:41.496: INFO: DNS probes using dns-test-9c29a918-77b3-4309-baf5-88f26c3fb790 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 15:05:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3419" for this suite.

â€¢ [SLOW TEST:38.534 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":224,"skipped":4420,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:05:42.096: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 11 15:05:42.099: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 11 15:05:42.099: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 11 15:05:42.099: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 11 15:05:42.099: INFO: Checking APIGroup: apps
Jun 11 15:05:42.101: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 11 15:05:42.101: INFO: Versions found [{apps/v1 v1}]
Jun 11 15:05:42.101: INFO: apps/v1 matches apps/v1
Jun 11 15:05:42.101: INFO: Checking APIGroup: events.k8s.io
Jun 11 15:05:42.104: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 11 15:05:42.104: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 11 15:05:42.104: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 11 15:05:42.104: INFO: Checking APIGroup: authentication.k8s.io
Jun 11 15:05:42.106: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 11 15:05:42.106: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 11 15:05:42.106: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 11 15:05:42.106: INFO: Checking APIGroup: authorization.k8s.io
Jun 11 15:05:42.108: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 11 15:05:42.108: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 11 15:05:42.108: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 11 15:05:42.108: INFO: Checking APIGroup: autoscaling
Jun 11 15:05:42.111: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun 11 15:05:42.111: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 11 15:05:42.111: INFO: autoscaling/v2 matches autoscaling/v2
Jun 11 15:05:42.111: INFO: Checking APIGroup: batch
Jun 11 15:05:42.113: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 11 15:05:42.113: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 11 15:05:42.113: INFO: batch/v1 matches batch/v1
Jun 11 15:05:42.113: INFO: Checking APIGroup: certificates.k8s.io
Jun 11 15:05:42.114: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 11 15:05:42.114: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 11 15:05:42.114: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 11 15:05:42.114: INFO: Checking APIGroup: networking.k8s.io
Jun 11 15:05:42.117: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 11 15:05:42.117: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 11 15:05:42.117: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 11 15:05:42.117: INFO: Checking APIGroup: policy
Jun 11 15:05:42.119: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 11 15:05:42.119: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jun 11 15:05:42.119: INFO: policy/v1 matches policy/v1
Jun 11 15:05:42.119: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 11 15:05:42.121: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 11 15:05:42.121: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 11 15:05:42.121: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 11 15:05:42.121: INFO: Checking APIGroup: storage.k8s.io
Jun 11 15:05:42.124: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 11 15:05:42.124: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 11 15:05:42.124: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 11 15:05:42.124: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 11 15:05:42.126: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 11 15:05:42.126: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 11 15:05:42.126: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 11 15:05:42.126: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 11 15:05:42.132: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 11 15:05:42.132: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 11 15:05:42.132: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 11 15:05:42.132: INFO: Checking APIGroup: scheduling.k8s.io
Jun 11 15:05:42.133: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 11 15:05:42.133: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 11 15:05:42.134: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 11 15:05:42.134: INFO: Checking APIGroup: coordination.k8s.io
Jun 11 15:05:42.135: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 11 15:05:42.135: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 11 15:05:42.135: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 11 15:05:42.135: INFO: Checking APIGroup: node.k8s.io
Jun 11 15:05:42.137: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 11 15:05:42.137: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun 11 15:05:42.137: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 11 15:05:42.137: INFO: Checking APIGroup: discovery.k8s.io
Jun 11 15:05:42.139: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 11 15:05:42.139: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jun 11 15:05:42.139: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 11 15:05:42.139: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 11 15:05:42.140: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jun 11 15:05:42.140: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 11 15:05:42.140: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jun 11 15:05:42.140: INFO: Checking APIGroup: projectcalico.org
Jun 11 15:05:42.142: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
Jun 11 15:05:42.142: INFO: Versions found [{projectcalico.org/v3 v3}]
Jun 11 15:05:42.142: INFO: projectcalico.org/v3 matches projectcalico.org/v3
Jun 11 15:05:42.142: INFO: Checking APIGroup: crd.projectcalico.org
Jun 11 15:05:42.144: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 11 15:05:42.144: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 11 15:05:42.144: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 11 15:05:42.144: INFO: Checking APIGroup: operator.tigera.io
Jun 11 15:05:42.145: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 11 15:05:42.145: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 11 15:05:42.145: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jun 11 15:05:42.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6909" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":225,"skipped":4439,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:42.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7713
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7713
STEP: creating replication controller externalsvc in namespace services-7713
I0611 15:05:42.238563      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7713, replica count: 2
I0611 15:05:45.289884      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 11 15:05:45.327: INFO: Creating new exec pod
Jun 11 15:05:47.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-7713 exec execpodkznwh -- /bin/sh -x -c nslookup clusterip-service.services-7713.svc.cluster.local'
Jun 11 15:05:47.574: INFO: stderr: "+ nslookup clusterip-service.services-7713.svc.cluster.local\n"
Jun 11 15:05:47.574: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7713.svc.cluster.local\tcanonical name = externalsvc.services-7713.svc.cluster.local.\nName:\texternalsvc.services-7713.svc.cluster.local\nAddress: 10.108.71.184\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7713, will wait for the garbage collector to delete the pods
Jun 11 15:05:47.641: INFO: Deleting ReplicationController externalsvc took: 10.833624ms
Jun 11 15:05:47.742: INFO: Terminating ReplicationController externalsvc pods took: 101.025896ms
Jun 11 15:05:49.571: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:05:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7713" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:7.445 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":226,"skipped":4440,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:49.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:05:49.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3603" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":227,"skipped":4448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:49.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jun 11 15:05:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2198" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":228,"skipped":4471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:05:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jun 11 15:05:52.683: INFO: PodSpec: initContainers in spec.initContainers
Jun 11 15:06:35.586: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f191b0e0-c4f7-4813-b998-b167e685585b", GenerateName:"", Namespace:"init-container-5447", SelfLink:"", UID:"f0622ff0-5a59-4b84-840b-2bab0f39402b", ResourceVersion:"36098", Generation:0, CreationTimestamp:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"683457951"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"07ec5a73c22038797f38a639741095df588c21a793f50d206d20b88d1873c19b", "cni.projectcalico.org/podIP":"192.168.203.62/32", "cni.projectcalico.org/podIPs":"192.168.203.62/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030abbf0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 11, 15, 5, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030abc20), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 11, 15, 5, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0030abc50), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-2v7cf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00298eac0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2v7cf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2v7cf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2v7cf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028ac248), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-41-158", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00362c700), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028ac2d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028ac2f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0028ac2f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0028ac2fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002037a30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.41.158", PodIP:"192.168.203.62", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.203.62"}}, StartTime:time.Date(2022, time.June, 11, 15, 5, 52, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00362c7e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00362c850)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e202b586da6235b384ac4da41d7c24f778c18377ad6e2214471fe89d0bd63bc2", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00298eb40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00298eb20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc0028ac374)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 15:06:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5447" for this suite.

â€¢ [SLOW TEST:42.992 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":229,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:06:35.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 15:06:48.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4006" for this suite.

â€¢ [SLOW TEST:13.199 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":230,"skipped":4530,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:06:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9447
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:06:48.914: INFO: Found 0 stateful pods, waiting for 1
Jun 11 15:06:58.925: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jun 11 15:06:58.953: INFO: Found 1 stateful pods, waiting for 2
Jun 11 15:07:08.967: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 11 15:07:08.967: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 15:07:09.001: INFO: Deleting all statefulset in ns statefulset-9447
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 15:07:09.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9447" for this suite.

â€¢ [SLOW TEST:20.200 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":231,"skipped":4530,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:09.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun 11 15:07:11.130: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-968 PodName:pod-sharedvolume-7ee089b9-c3b9-401a-b564-85b95850cc23 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:07:11.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:07:11.131: INFO: ExecWithOptions: Clientset creation
Jun 11 15:07:11.131: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-968/pods/pod-sharedvolume-7ee089b9-c3b9-401a-b564-85b95850cc23/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun 11 15:07:11.213: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:07:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-968" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":232,"skipped":4540,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:11.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jun 11 15:07:11.280: INFO: The status of Pod annotationupdate50b708a4-cea5-4d20-bcbe-273f76a2702c is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:07:13.287: INFO: The status of Pod annotationupdate50b708a4-cea5-4d20-bcbe-273f76a2702c is Running (Ready = true)
Jun 11 15:07:13.849: INFO: Successfully updated pod "annotationupdate50b708a4-cea5-4d20-bcbe-273f76a2702c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 15:07:17.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8536" for this suite.

â€¢ [SLOW TEST:6.661 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":233,"skipped":4541,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:17.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-fc6a3051-b849-4621-8a8a-c14e4c2aabbe
STEP: Creating a pod to test consume secrets
Jun 11 15:07:17.952: INFO: Waiting up to 5m0s for pod "pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a" in namespace "secrets-8780" to be "Succeeded or Failed"
Jun 11 15:07:17.960: INFO: Pod "pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.196319ms
Jun 11 15:07:19.965: INFO: Pod "pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012227227s
Jun 11 15:07:21.969: INFO: Pod "pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016805189s
STEP: Saw pod success
Jun 11 15:07:21.970: INFO: Pod "pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a" satisfied condition "Succeeded or Failed"
Jun 11 15:07:21.974: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 15:07:22.010: INFO: Waiting for pod pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a to disappear
Jun 11 15:07:22.014: INFO: Pod pod-secrets-d070d5b5-efef-4010-b810-c60dd928822a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 15:07:22.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8780" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":234,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:22.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:07:23.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:07:26.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:07:36.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1184" for this suite.
STEP: Destroying namespace "webhook-1184-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:14.375 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":235,"skipped":4565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:36.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:07:36.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6151" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":236,"skipped":4600,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:36.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:07:37.159: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:07:40.207: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:07:40.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:07:43.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-619" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

â€¢ [SLOW TEST:7.181 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":237,"skipped":4602,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 11 15:07:43.890: INFO: Waiting up to 5m0s for pod "pod-8066515d-0772-42f5-a270-1dc46d116e4c" in namespace "emptydir-2599" to be "Succeeded or Failed"
Jun 11 15:07:43.941: INFO: Pod "pod-8066515d-0772-42f5-a270-1dc46d116e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 51.03644ms
Jun 11 15:07:45.949: INFO: Pod "pod-8066515d-0772-42f5-a270-1dc46d116e4c": Phase="Running", Reason="", readiness=false. Elapsed: 2.058863538s
Jun 11 15:07:47.958: INFO: Pod "pod-8066515d-0772-42f5-a270-1dc46d116e4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067689506s
STEP: Saw pod success
Jun 11 15:07:47.958: INFO: Pod "pod-8066515d-0772-42f5-a270-1dc46d116e4c" satisfied condition "Succeeded or Failed"
Jun 11 15:07:47.963: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-8066515d-0772-42f5-a270-1dc46d116e4c container test-container: <nil>
STEP: delete the pod
Jun 11 15:07:47.991: INFO: Waiting for pod pod-8066515d-0772-42f5-a270-1dc46d116e4c to disappear
Jun 11 15:07:47.996: INFO: Pod pod-8066515d-0772-42f5-a270-1dc46d116e4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:07:47.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2599" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":238,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:48.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jun 11 15:07:48.094: INFO: Waiting up to 5m0s for pod "downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6" in namespace "downward-api-701" to be "Succeeded or Failed"
Jun 11 15:07:48.102: INFO: Pod "downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.456793ms
Jun 11 15:07:50.113: INFO: Pod "downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018505111s
Jun 11 15:07:52.120: INFO: Pod "downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02580745s
STEP: Saw pod success
Jun 11 15:07:52.121: INFO: Pod "downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6" satisfied condition "Succeeded or Failed"
Jun 11 15:07:52.125: INFO: Trying to get logs from node ip-172-31-41-158 pod downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6 container dapi-container: <nil>
STEP: delete the pod
Jun 11 15:07:52.150: INFO: Waiting for pod downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6 to disappear
Jun 11 15:07:52.161: INFO: Pod downward-api-634bf250-9c03-4a62-85a5-2e4dce4d57c6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jun 11 15:07:52.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-701" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":239,"skipped":4640,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:52.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 11 15:07:52.220: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9843  ceb739a1-2925-4460-b922-3944015b78bd 36791 0 2022-06-11 15:07:52 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-06-11 15:07:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcqdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcqdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 11 15:07:52.229: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:07:54.242: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 11 15:07:54.242: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9843 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:07:54.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:07:54.243: INFO: ExecWithOptions: Clientset creation
Jun 11 15:07:54.243: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9843/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jun 11 15:07:54.358: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9843 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:07:54.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:07:54.359: INFO: ExecWithOptions: Clientset creation
Jun 11 15:07:54.359: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9843/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 11 15:07:54.441: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 15:07:54.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9843" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":240,"skipped":4655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 11 15:07:54.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8110  6c8b2e1a-cb8b-4813-86d4-ac1a888f2286 36825 0 2022-06-11 15:07:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-11 15:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:07:54.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8110  6c8b2e1a-cb8b-4813-86d4-ac1a888f2286 36826 0 2022-06-11 15:07:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-11 15:07:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 11 15:07:54.567: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8110  6c8b2e1a-cb8b-4813-86d4-ac1a888f2286 36827 0 2022-06-11 15:07:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-11 15:07:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:07:54.567: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8110  6c8b2e1a-cb8b-4813-86d4-ac1a888f2286 36828 0 2022-06-11 15:07:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-11 15:07:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jun 11 15:07:54.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8110" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":241,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:07:54.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jun 11 15:08:18.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1683" for this suite.

â€¢ [SLOW TEST:24.408 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4717,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:18.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 15:08:19.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5896" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":243,"skipped":4721,"failed":0}
SS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:19.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 11 15:08:19.201: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 11 15:08:19.210: INFO: starting watch
STEP: patching
STEP: updating
Jun 11 15:08:19.235: INFO: waiting for watch events with expected annotations
Jun 11 15:08:19.236: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jun 11 15:08:19.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4300" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":244,"skipped":4723,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:19.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-rgcl
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 15:08:19.404: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rgcl" in namespace "subpath-2342" to be "Succeeded or Failed"
Jun 11 15:08:19.412: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.567374ms
Jun 11 15:08:21.425: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 2.02072694s
Jun 11 15:08:23.433: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.02831979s
Jun 11 15:08:25.440: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 6.03570522s
Jun 11 15:08:27.446: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 8.041532866s
Jun 11 15:08:29.452: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 10.047151618s
Jun 11 15:08:31.460: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 12.055893598s
Jun 11 15:08:33.465: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 14.060333631s
Jun 11 15:08:35.474: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 16.069362446s
Jun 11 15:08:37.480: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 18.075363704s
Jun 11 15:08:39.485: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=true. Elapsed: 20.080501071s
Jun 11 15:08:41.490: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Running", Reason="", readiness=false. Elapsed: 22.085441028s
Jun 11 15:08:43.495: INFO: Pod "pod-subpath-test-configmap-rgcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.090752986s
STEP: Saw pod success
Jun 11 15:08:43.495: INFO: Pod "pod-subpath-test-configmap-rgcl" satisfied condition "Succeeded or Failed"
Jun 11 15:08:43.499: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-subpath-test-configmap-rgcl container test-container-subpath-configmap-rgcl: <nil>
STEP: delete the pod
Jun 11 15:08:43.519: INFO: Waiting for pod pod-subpath-test-configmap-rgcl to disappear
Jun 11 15:08:43.523: INFO: Pod pod-subpath-test-configmap-rgcl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rgcl
Jun 11 15:08:43.523: INFO: Deleting pod "pod-subpath-test-configmap-rgcl" in namespace "subpath-2342"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jun 11 15:08:43.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2342" for this suite.

â€¢ [SLOW TEST:24.203 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":245,"skipped":4735,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:43.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-fd927957-2400-41fa-85e7-7d3605276d8b
STEP: Creating a pod to test consume configMaps
Jun 11 15:08:43.610: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648" in namespace "projected-9259" to be "Succeeded or Failed"
Jun 11 15:08:43.614: INFO: Pod "pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648": Phase="Pending", Reason="", readiness=false. Elapsed: 3.727847ms
Jun 11 15:08:45.620: INFO: Pod "pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009692003s
Jun 11 15:08:47.625: INFO: Pod "pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014763407s
STEP: Saw pod success
Jun 11 15:08:47.625: INFO: Pod "pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648" satisfied condition "Succeeded or Failed"
Jun 11 15:08:47.632: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:08:47.654: INFO: Waiting for pod pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648 to disappear
Jun 11 15:08:47.658: INFO: Pod pod-projected-configmaps-2bb57dc9-3236-4625-adf1-64b0836d0648 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 15:08:47.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9259" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":246,"skipped":4742,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:47.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 11 15:08:52.248: INFO: Successfully updated pod "adopt-release-mwj6n"
STEP: Checking that the Job readopts the Pod
Jun 11 15:08:52.248: INFO: Waiting up to 15m0s for pod "adopt-release-mwj6n" in namespace "job-779" to be "adopted"
Jun 11 15:08:52.254: INFO: Pod "adopt-release-mwj6n": Phase="Running", Reason="", readiness=true. Elapsed: 6.557239ms
Jun 11 15:08:54.262: INFO: Pod "adopt-release-mwj6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.013780531s
Jun 11 15:08:54.262: INFO: Pod "adopt-release-mwj6n" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 11 15:08:54.789: INFO: Successfully updated pod "adopt-release-mwj6n"
STEP: Checking that the Job releases the Pod
Jun 11 15:08:54.789: INFO: Waiting up to 15m0s for pod "adopt-release-mwj6n" in namespace "job-779" to be "released"
Jun 11 15:08:54.794: INFO: Pod "adopt-release-mwj6n": Phase="Running", Reason="", readiness=true. Elapsed: 4.806692ms
Jun 11 15:08:56.800: INFO: Pod "adopt-release-mwj6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.010914499s
Jun 11 15:08:56.800: INFO: Pod "adopt-release-mwj6n" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jun 11 15:08:56.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-779" for this suite.

â€¢ [SLOW TEST:9.148 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":247,"skipped":4745,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:56.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jun 11 15:08:56.892: INFO: The status of Pod pod-hostip-ba91bef3-daa7-4bf1-9ee8-8b70a932b905 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:08:58.906: INFO: The status of Pod pod-hostip-ba91bef3-daa7-4bf1-9ee8-8b70a932b905 is Running (Ready = true)
Jun 11 15:08:58.915: INFO: Pod pod-hostip-ba91bef3-daa7-4bf1-9ee8-8b70a932b905 has hostIP: 172.31.27.214
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 15:08:58.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8791" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":248,"skipped":4761,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:08:58.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 11 15:08:58.975: INFO: Waiting up to 5m0s for pod "pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961" in namespace "emptydir-717" to be "Succeeded or Failed"
Jun 11 15:08:58.980: INFO: Pod "pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810923ms
Jun 11 15:09:00.990: INFO: Pod "pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014865688s
Jun 11 15:09:03.002: INFO: Pod "pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026529308s
STEP: Saw pod success
Jun 11 15:09:03.002: INFO: Pod "pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961" satisfied condition "Succeeded or Failed"
Jun 11 15:09:03.006: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961 container test-container: <nil>
STEP: delete the pod
Jun 11 15:09:03.029: INFO: Waiting for pod pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961 to disappear
Jun 11 15:09:03.034: INFO: Pod pod-2e5cde0a-e5c0-47b6-a6d9-cbcec73b2961 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:09:03.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-717" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":249,"skipped":4767,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:03.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-00a3b0ae-eea5-40ce-b03c-05dc890c0c4c
STEP: Creating a pod to test consume configMaps
Jun 11 15:09:03.116: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf" in namespace "projected-5124" to be "Succeeded or Failed"
Jun 11 15:09:03.121: INFO: Pod "pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689421ms
Jun 11 15:09:05.130: INFO: Pod "pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014072898s
Jun 11 15:09:07.137: INFO: Pod "pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021033319s
STEP: Saw pod success
Jun 11 15:09:07.138: INFO: Pod "pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf" satisfied condition "Succeeded or Failed"
Jun 11 15:09:07.142: INFO: Trying to get logs from node ip-172-31-27-214 pod pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:09:07.176: INFO: Waiting for pod pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf to disappear
Jun 11 15:09:07.180: INFO: Pod pod-projected-configmaps-6da1aec6-14fe-4a7d-9209-36909d0683bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 15:09:07.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5124" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4782,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:07.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:09:07.270: INFO: Create a RollingUpdate DaemonSet
Jun 11 15:09:07.276: INFO: Check that daemon pods launch on every node of the cluster
Jun 11 15:09:07.282: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:07.282: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:07.282: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:07.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:09:07.289: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:09:08.299: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:08.299: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:08.300: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:08.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:09:08.305: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:09:09.299: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:09.299: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:09.299: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:09.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 15:09:09.304: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jun 11 15:09:09.304: INFO: Update the DaemonSet to trigger a rollout
Jun 11 15:09:09.320: INFO: Updating DaemonSet daemon-set
Jun 11 15:09:12.341: INFO: Roll back the DaemonSet before rollout is complete
Jun 11 15:09:12.355: INFO: Updating DaemonSet daemon-set
Jun 11 15:09:12.355: INFO: Make sure DaemonSet rollback is complete
Jun 11 15:09:12.360: INFO: Wrong image for pod: daemon-set-dpxdj. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jun 11 15:09:12.360: INFO: Pod daemon-set-dpxdj is not available
Jun 11 15:09:12.366: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:12.367: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:12.367: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:13.378: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:13.378: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:13.378: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:14.383: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:14.383: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:14.383: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:15.376: INFO: Pod daemon-set-k9djg is not available
Jun 11 15:09:15.386: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:15.386: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:09:15.387: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2599, will wait for the garbage collector to delete the pods
Jun 11 15:09:15.475: INFO: Deleting DaemonSet.extensions daemon-set took: 13.690166ms
Jun 11 15:09:15.576: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.530268ms
Jun 11 15:09:16.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:09:16.984: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 15:09:16.988: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37548"},"items":null}

Jun 11 15:09:16.991: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37548"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 15:09:17.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2599" for this suite.

â€¢ [SLOW TEST:9.823 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":251,"skipped":4787,"failed":0}
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jun 11 15:09:17.085: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:09:19.095: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jun 11 15:09:19.113: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:09:21.122: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 11 15:09:21.126: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.127: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.127: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 11 15:09:21.257: INFO: Exec stderr: ""
Jun 11 15:09:21.257: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.259: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.259: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 11 15:09:21.360: INFO: Exec stderr: ""
Jun 11 15:09:21.360: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.361: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 11 15:09:21.475: INFO: Exec stderr: ""
Jun 11 15:09:21.476: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.478: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.479: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 11 15:09:21.621: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 11 15:09:21.622: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.623: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.623: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 11 15:09:21.719: INFO: Exec stderr: ""
Jun 11 15:09:21.719: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.720: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.720: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 11 15:09:21.797: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 11 15:09:21.797: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.798: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.798: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 11 15:09:21.894: INFO: Exec stderr: ""
Jun 11 15:09:21.895: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.895: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 11 15:09:21.989: INFO: Exec stderr: ""
Jun 11 15:09:21.989: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:21.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:21.990: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:21.990: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 11 15:09:22.071: INFO: Exec stderr: ""
Jun 11 15:09:22.071: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8786 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:09:22.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:09:22.072: INFO: ExecWithOptions: Clientset creation
Jun 11 15:09:22.072: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8786/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 11 15:09:22.150: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jun 11 15:09:22.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8786" for this suite.

â€¢ [SLOW TEST:5.146 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":252,"skipped":4791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:22.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jun 11 15:09:22.211: INFO: Waiting up to 5m0s for pod "var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57" in namespace "var-expansion-1720" to be "Succeeded or Failed"
Jun 11 15:09:22.219: INFO: Pod "var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543784ms
Jun 11 15:09:24.224: INFO: Pod "var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013158459s
Jun 11 15:09:26.232: INFO: Pod "var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020354162s
STEP: Saw pod success
Jun 11 15:09:26.232: INFO: Pod "var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57" satisfied condition "Succeeded or Failed"
Jun 11 15:09:26.236: INFO: Trying to get logs from node ip-172-31-27-214 pod var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57 container dapi-container: <nil>
STEP: delete the pod
Jun 11 15:09:26.271: INFO: Waiting for pod var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57 to disappear
Jun 11 15:09:26.278: INFO: Pod var-expansion-b8fc0e5f-1f77-4b6b-a5da-5d3d26479b57 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 15:09:26.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1720" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":253,"skipped":4823,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:26.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-b0298415-6e23-473d-b242-c12fbdc07c68 in namespace container-probe-903
Jun 11 15:09:28.385: INFO: Started pod liveness-b0298415-6e23-473d-b242-c12fbdc07c68 in namespace container-probe-903
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 15:09:28.389: INFO: Initial restart count of pod liveness-b0298415-6e23-473d-b242-c12fbdc07c68 is 0
Jun 11 15:09:48.460: INFO: Restart count of pod container-probe-903/liveness-b0298415-6e23-473d-b242-c12fbdc07c68 is now 1 (20.070528371s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 15:09:48.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-903" for this suite.

â€¢ [SLOW TEST:22.186 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":254,"skipped":4833,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:48.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-333.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-333.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-333.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-333.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 15:09:50.597: INFO: DNS probes using dns-333/dns-test-0c2fd83d-7d97-4af5-b079-06a56d244a73 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 15:09:50.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-333" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":255,"skipped":4841,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:50.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:09:50.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:09:51.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1315" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":256,"skipped":4841,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:51.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-8ce3c251-933a-4ee8-98c3-a6cdb2d98435
STEP: Creating a pod to test consume secrets
Jun 11 15:09:51.348: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab" in namespace "projected-485" to be "Succeeded or Failed"
Jun 11 15:09:51.354: INFO: Pod "pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190164ms
Jun 11 15:09:53.362: INFO: Pod "pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab": Phase="Running", Reason="", readiness=false. Elapsed: 2.014191653s
Jun 11 15:09:55.371: INFO: Pod "pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022865151s
STEP: Saw pod success
Jun 11 15:09:55.372: INFO: Pod "pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab" satisfied condition "Succeeded or Failed"
Jun 11 15:09:55.375: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 15:09:55.397: INFO: Waiting for pod pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab to disappear
Jun 11 15:09:55.401: INFO: Pod pod-projected-secrets-643266b4-1374-43d3-8049-eb90dbafadab no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 15:09:55.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-485" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4841,"failed":0}
SSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:55.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jun 11 15:09:57.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3827" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":258,"skipped":4847,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:09:57.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5191
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5191
I0611 15:09:57.613378      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5191, replica count: 2
I0611 15:10:00.665574      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:10:00.665: INFO: Creating new exec pod
Jun 11 15:10:03.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 11 15:10:03.883: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 11 15:10:03.883: INFO: stdout: ""
Jun 11 15:10:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 11 15:10:05.049: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 11 15:10:05.049: INFO: stdout: "externalname-service-d4lfh"
Jun 11 15:10:05.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.221.246 80'
Jun 11 15:10:05.242: INFO: stderr: "+ nc -v -t -w 2 10.97.221.246 80\n+ echo hostName\nConnection to 10.97.221.246 80 port [tcp/http] succeeded!\n"
Jun 11 15:10:05.242: INFO: stdout: ""
Jun 11 15:10:06.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.221.246 80'
Jun 11 15:10:06.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.221.246 80\nConnection to 10.97.221.246 80 port [tcp/http] succeeded!\n"
Jun 11 15:10:06.423: INFO: stdout: "externalname-service-d4lfh"
Jun 11 15:10:06.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 31910'
Jun 11 15:10:06.594: INFO: stderr: "+ nc -v -t -w 2 172.31.27.214 31910\n+ echo hostName\nConnection to 172.31.27.214 31910 port [tcp/*] succeeded!\n"
Jun 11 15:10:06.594: INFO: stdout: ""
Jun 11 15:10:07.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 31910'
Jun 11 15:10:07.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.214 31910\nConnection to 172.31.27.214 31910 port [tcp/*] succeeded!\n"
Jun 11 15:10:07.791: INFO: stdout: "externalname-service-nsjml"
Jun 11 15:10:07.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 31910'
Jun 11 15:10:07.979: INFO: stderr: "+ nc -v -t -w 2 172.31.41.158 31910\n+ echo hostName\nConnection to 172.31.41.158 31910 port [tcp/*] succeeded!\n"
Jun 11 15:10:07.979: INFO: stdout: ""
Jun 11 15:10:08.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 31910'
Jun 11 15:10:09.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.41.158 31910\nConnection to 172.31.41.158 31910 port [tcp/*] succeeded!\n"
Jun 11 15:10:09.246: INFO: stdout: ""
Jun 11 15:10:09.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-5191 exec execpodt6vf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 31910'
Jun 11 15:10:10.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.41.158 31910\nConnection to 172.31.41.158 31910 port [tcp/*] succeeded!\n"
Jun 11 15:10:10.160: INFO: stdout: "externalname-service-d4lfh"
Jun 11 15:10:10.160: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:10:10.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5191" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:12.724 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":259,"skipped":4859,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:10.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun 11 15:10:10.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-6982 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jun 11 15:10:10.388: INFO: stderr: ""
Jun 11 15:10:10.388: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jun 11 15:10:10.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-6982 delete pods e2e-test-httpd-pod'
Jun 11 15:10:12.349: INFO: stderr: ""
Jun 11 15:10:12.349: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:10:12.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6982" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":260,"skipped":4859,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:12.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4289
STEP: creating service affinity-nodeport in namespace services-4289
STEP: creating replication controller affinity-nodeport in namespace services-4289
I0611 15:10:12.428909      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4289, replica count: 3
I0611 15:10:15.480604      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:10:15.501: INFO: Creating new exec pod
Jun 11 15:10:18.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4289 exec execpod-affinityg6zkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun 11 15:10:18.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 11 15:10:18.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:10:18.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4289 exec execpod-affinityg6zkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.156.93 80'
Jun 11 15:10:18.900: INFO: stderr: "+ nc -v -t -w 2 10.105.156.93 80\nConnection to 10.105.156.93 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jun 11 15:10:18.900: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:10:18.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4289 exec execpod-affinityg6zkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.214 31090'
Jun 11 15:10:19.088: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.214 31090\nConnection to 172.31.27.214 31090 port [tcp/*] succeeded!\n"
Jun 11 15:10:19.088: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:10:19.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4289 exec execpod-affinityg6zkr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.41.158 31090'
Jun 11 15:10:19.344: INFO: stderr: "+ nc -v -t -w 2 172.31.41.158 31090\n+ echo hostName\nConnection to 172.31.41.158 31090 port [tcp/*] succeeded!\n"
Jun 11 15:10:19.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:10:19.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-4289 exec execpod-affinityg6zkr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.214:31090/ ; done'
Jun 11 15:10:19.694: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.214:31090/\n"
Jun 11 15:10:19.694: INFO: stdout: "\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl\naffinity-nodeport-2xlwl"
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Received response from host: affinity-nodeport-2xlwl
Jun 11 15:10:19.694: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4289, will wait for the garbage collector to delete the pods
Jun 11 15:10:19.774: INFO: Deleting ReplicationController affinity-nodeport took: 7.585584ms
Jun 11 15:10:19.877: INFO: Terminating ReplicationController affinity-nodeport pods took: 103.267581ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:10:22.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4289" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:10.163 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":261,"skipped":4878,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun 11 15:10:22.575: INFO: Waiting up to 5m0s for pod "security-context-af86ec25-d242-4585-a914-787756040e7a" in namespace "security-context-7573" to be "Succeeded or Failed"
Jun 11 15:10:22.580: INFO: Pod "security-context-af86ec25-d242-4585-a914-787756040e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090037ms
Jun 11 15:10:24.601: INFO: Pod "security-context-af86ec25-d242-4585-a914-787756040e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026182615s
Jun 11 15:10:26.616: INFO: Pod "security-context-af86ec25-d242-4585-a914-787756040e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041639981s
STEP: Saw pod success
Jun 11 15:10:26.617: INFO: Pod "security-context-af86ec25-d242-4585-a914-787756040e7a" satisfied condition "Succeeded or Failed"
Jun 11 15:10:26.626: INFO: Trying to get logs from node ip-172-31-41-158 pod security-context-af86ec25-d242-4585-a914-787756040e7a container test-container: <nil>
STEP: delete the pod
Jun 11 15:10:26.653: INFO: Waiting for pod security-context-af86ec25-d242-4585-a914-787756040e7a to disappear
Jun 11 15:10:26.661: INFO: Pod security-context-af86ec25-d242-4585-a914-787756040e7a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 15:10:26.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7573" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":262,"skipped":4899,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:26.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:10:27.383: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:10:30.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:10:30.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7998" for this suite.
STEP: Destroying namespace "webhook-7998-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":263,"skipped":4913,"failed":0}
SSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:30.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jun 11 15:10:30.652: INFO: Waiting up to 5m0s for pod "client-containers-df926738-507b-46d9-8651-8b9a0a17cb60" in namespace "containers-8898" to be "Succeeded or Failed"
Jun 11 15:10:30.662: INFO: Pod "client-containers-df926738-507b-46d9-8651-8b9a0a17cb60": Phase="Pending", Reason="", readiness=false. Elapsed: 9.589975ms
Jun 11 15:10:32.670: INFO: Pod "client-containers-df926738-507b-46d9-8651-8b9a0a17cb60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017731937s
Jun 11 15:10:34.680: INFO: Pod "client-containers-df926738-507b-46d9-8651-8b9a0a17cb60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027590967s
STEP: Saw pod success
Jun 11 15:10:34.680: INFO: Pod "client-containers-df926738-507b-46d9-8651-8b9a0a17cb60" satisfied condition "Succeeded or Failed"
Jun 11 15:10:34.684: INFO: Trying to get logs from node ip-172-31-41-158 pod client-containers-df926738-507b-46d9-8651-8b9a0a17cb60 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:10:34.707: INFO: Waiting for pod client-containers-df926738-507b-46d9-8651-8b9a0a17cb60 to disappear
Jun 11 15:10:34.711: INFO: Pod client-containers-df926738-507b-46d9-8651-8b9a0a17cb60 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jun 11 15:10:34.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8898" for this suite.
â€¢{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":4919,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:10:34.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-5be3b972-6b98-44db-a0f1-6a5e37ab4633 in namespace container-probe-1790
Jun 11 15:10:36.792: INFO: Started pod busybox-5be3b972-6b98-44db-a0f1-6a5e37ab4633 in namespace container-probe-1790
STEP: checking the pod's current state and verifying that restartCount is present
Jun 11 15:10:36.796: INFO: Initial restart count of pod busybox-5be3b972-6b98-44db-a0f1-6a5e37ab4633 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 15:14:37.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1790" for this suite.

â€¢ [SLOW TEST:243.139 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":265,"skipped":4926,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:14:37.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jun 11 15:14:37.916: INFO: Waiting up to 5m0s for pod "downward-api-da286044-8b3c-4560-b8d8-8913173fb31c" in namespace "downward-api-9670" to be "Succeeded or Failed"
Jun 11 15:14:37.922: INFO: Pod "downward-api-da286044-8b3c-4560-b8d8-8913173fb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075232ms
Jun 11 15:14:39.929: INFO: Pod "downward-api-da286044-8b3c-4560-b8d8-8913173fb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013322574s
Jun 11 15:14:41.935: INFO: Pod "downward-api-da286044-8b3c-4560-b8d8-8913173fb31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018749666s
STEP: Saw pod success
Jun 11 15:14:41.935: INFO: Pod "downward-api-da286044-8b3c-4560-b8d8-8913173fb31c" satisfied condition "Succeeded or Failed"
Jun 11 15:14:41.941: INFO: Trying to get logs from node ip-172-31-41-158 pod downward-api-da286044-8b3c-4560-b8d8-8913173fb31c container dapi-container: <nil>
STEP: delete the pod
Jun 11 15:14:41.977: INFO: Waiting for pod downward-api-da286044-8b3c-4560-b8d8-8913173fb31c to disappear
Jun 11 15:14:41.981: INFO: Pod downward-api-da286044-8b3c-4560-b8d8-8913173fb31c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jun 11 15:14:41.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9670" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":266,"skipped":4937,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:14:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64
Jun 11 15:14:42.069: INFO: Pod name my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64: Found 0 pods out of 1
Jun 11 15:14:47.077: INFO: Pod name my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64: Found 1 pods out of 1
Jun 11 15:14:47.077: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64" are running
Jun 11 15:14:47.080: INFO: Pod "my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64-xzf2c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-11 15:14:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-11 15:14:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-11 15:14:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-11 15:14:42 +0000 UTC Reason: Message:}])
Jun 11 15:14:47.081: INFO: Trying to dial the pod
Jun 11 15:14:52.105: INFO: Controller my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64: Got expected result from replica 1 [my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64-xzf2c]: "my-hostname-basic-1944b78a-29cb-4c42-8ac9-c87a0eb53f64-xzf2c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jun 11 15:14:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4448" for this suite.

â€¢ [SLOW TEST:10.127 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":267,"skipped":4948,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:14:52.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jun 11 15:14:52.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6384" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":268,"skipped":4969,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:14:52.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jun 11 15:14:52.299: INFO: created test-pod-1
Jun 11 15:14:52.308: INFO: created test-pod-2
Jun 11 15:14:52.317: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jun 11 15:14:52.317: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4447' to be running and ready
Jun 11 15:14:52.334: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 11 15:14:52.334: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 11 15:14:52.334: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 11 15:14:52.334: INFO: 0 / 3 pods in namespace 'pods-4447' are running and ready (0 seconds elapsed)
Jun 11 15:14:52.334: INFO: expected 0 pod replicas in namespace 'pods-4447', 0 are Running and Ready.
Jun 11 15:14:52.334: INFO: POD         NODE              PHASE    GRACE  CONDITIONS
Jun 11 15:14:52.334: INFO: test-pod-1  ip-172-31-41-158  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:14:52 +0000 UTC  }]
Jun 11 15:14:52.334: INFO: test-pod-2  ip-172-31-27-214  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-11 15:14:52 +0000 UTC  }]
Jun 11 15:14:52.334: INFO: test-pod-3                    Pending         []
Jun 11 15:14:52.334: INFO: 
Jun 11 15:14:54.350: INFO: 3 / 3 pods in namespace 'pods-4447' are running and ready (2 seconds elapsed)
Jun 11 15:14:54.350: INFO: expected 0 pod replicas in namespace 'pods-4447', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jun 11 15:14:54.381: INFO: Pod quantity 3 is different from expected quantity 0
Jun 11 15:14:55.389: INFO: Pod quantity 3 is different from expected quantity 0
Jun 11 15:14:56.387: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 15:14:57.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4447" for this suite.

â€¢ [SLOW TEST:5.149 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":269,"skipped":4977,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:14:57.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-tr5x
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 15:14:57.467: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tr5x" in namespace "subpath-1973" to be "Succeeded or Failed"
Jun 11 15:14:57.473: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658135ms
Jun 11 15:14:59.481: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 2.013482814s
Jun 11 15:15:01.504: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 4.036729606s
Jun 11 15:15:03.514: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 6.046750965s
Jun 11 15:15:05.520: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 8.052331139s
Jun 11 15:15:07.529: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 10.061166992s
Jun 11 15:15:09.540: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.072724074s
Jun 11 15:15:11.548: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 14.080356668s
Jun 11 15:15:13.558: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 16.090354845s
Jun 11 15:15:15.573: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 18.105241258s
Jun 11 15:15:17.581: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=true. Elapsed: 20.113042716s
Jun 11 15:15:19.590: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Running", Reason="", readiness=false. Elapsed: 22.122632139s
Jun 11 15:15:21.597: INFO: Pod "pod-subpath-test-projected-tr5x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.129894347s
STEP: Saw pod success
Jun 11 15:15:21.598: INFO: Pod "pod-subpath-test-projected-tr5x" satisfied condition "Succeeded or Failed"
Jun 11 15:15:21.603: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-subpath-test-projected-tr5x container test-container-subpath-projected-tr5x: <nil>
STEP: delete the pod
Jun 11 15:15:21.637: INFO: Waiting for pod pod-subpath-test-projected-tr5x to disappear
Jun 11 15:15:21.642: INFO: Pod pod-subpath-test-projected-tr5x no longer exists
STEP: Deleting pod pod-subpath-test-projected-tr5x
Jun 11 15:15:21.642: INFO: Deleting pod "pod-subpath-test-projected-tr5x" in namespace "subpath-1973"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jun 11 15:15:21.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1973" for this suite.

â€¢ [SLOW TEST:24.257 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":270,"skipped":4982,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:15:21.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jun 11 15:17:01.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6047" for this suite.

â€¢ [SLOW TEST:100.103 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":271,"skipped":5003,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:01.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jun 11 15:17:01.855: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:17:03.864: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 11 15:17:04.896: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jun 11 15:17:05.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-502" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":272,"skipped":5016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:05.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jun 11 15:17:05.986: INFO: Waiting up to 5m0s for pod "downward-api-53490590-61b4-4494-a9ff-cd1a41423342" in namespace "downward-api-6476" to be "Succeeded or Failed"
Jun 11 15:17:05.993: INFO: Pod "downward-api-53490590-61b4-4494-a9ff-cd1a41423342": Phase="Pending", Reason="", readiness=false. Elapsed: 7.592834ms
Jun 11 15:17:08.005: INFO: Pod "downward-api-53490590-61b4-4494-a9ff-cd1a41423342": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018798563s
Jun 11 15:17:10.014: INFO: Pod "downward-api-53490590-61b4-4494-a9ff-cd1a41423342": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028158831s
STEP: Saw pod success
Jun 11 15:17:10.014: INFO: Pod "downward-api-53490590-61b4-4494-a9ff-cd1a41423342" satisfied condition "Succeeded or Failed"
Jun 11 15:17:10.020: INFO: Trying to get logs from node ip-172-31-27-214 pod downward-api-53490590-61b4-4494-a9ff-cd1a41423342 container dapi-container: <nil>
STEP: delete the pod
Jun 11 15:17:10.067: INFO: Waiting for pod downward-api-53490590-61b4-4494-a9ff-cd1a41423342 to disappear
Jun 11 15:17:10.072: INFO: Pod downward-api-53490590-61b4-4494-a9ff-cd1a41423342 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jun 11 15:17:10.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6476" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":5066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jun 11 15:17:10.142: INFO: Waiting up to 5m0s for pod "test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090" in namespace "svcaccounts-9150" to be "Succeeded or Failed"
Jun 11 15:17:10.149: INFO: Pod "test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045826ms
Jun 11 15:17:12.155: INFO: Pod "test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618068s
Jun 11 15:17:14.163: INFO: Pod "test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020973957s
STEP: Saw pod success
Jun 11 15:17:14.163: INFO: Pod "test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090" satisfied condition "Succeeded or Failed"
Jun 11 15:17:14.168: INFO: Trying to get logs from node ip-172-31-41-158 pod test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:17:14.203: INFO: Waiting for pod test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090 to disappear
Jun 11 15:17:14.208: INFO: Pod test-pod-994fcc25-84fc-43f4-bc15-93c7d6e9b090 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 15:17:14.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9150" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":274,"skipped":5138,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:14.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 15:17:14.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913" in namespace "projected-1752" to be "Succeeded or Failed"
Jun 11 15:17:14.298: INFO: Pod "downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295151ms
Jun 11 15:17:16.309: INFO: Pod "downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018078119s
Jun 11 15:17:18.315: INFO: Pod "downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023853414s
STEP: Saw pod success
Jun 11 15:17:18.315: INFO: Pod "downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913" satisfied condition "Succeeded or Failed"
Jun 11 15:17:18.320: INFO: Trying to get logs from node ip-172-31-27-214 pod downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913 container client-container: <nil>
STEP: delete the pod
Jun 11 15:17:18.340: INFO: Waiting for pod downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913 to disappear
Jun 11 15:17:18.345: INFO: Pod downwardapi-volume-ba8d1a0b-b3fd-4b59-af5e-3f4a35b7e913 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 15:17:18.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1752" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":275,"skipped":5139,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:18.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-0de9652b-eb47-4663-a3a1-107e661a9439
STEP: Creating a pod to test consume configMaps
Jun 11 15:17:18.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757" in namespace "configmap-4835" to be "Succeeded or Failed"
Jun 11 15:17:18.441: INFO: Pod "pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757": Phase="Pending", Reason="", readiness=false. Elapsed: 11.655287ms
Jun 11 15:17:20.449: INFO: Pod "pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019649555s
Jun 11 15:17:22.455: INFO: Pod "pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025553422s
STEP: Saw pod success
Jun 11 15:17:22.455: INFO: Pod "pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757" satisfied condition "Succeeded or Failed"
Jun 11 15:17:22.460: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:17:22.486: INFO: Waiting for pod pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757 to disappear
Jun 11 15:17:22.489: INFO: Pod pod-configmaps-dada1b5c-4040-4711-a860-3c1ed65e7757 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:17:22.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4835" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":276,"skipped":5147,"failed":0}
SSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:22.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jun 11 15:17:22.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5718" for this suite.
â€¢{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":277,"skipped":5150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:22.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-a6a5a794-943c-4810-964f-df7283f4734c
STEP: Creating a pod to test consume configMaps
Jun 11 15:17:22.681: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e" in namespace "projected-7673" to be "Succeeded or Failed"
Jun 11 15:17:22.686: INFO: Pod "pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188659ms
Jun 11 15:17:24.692: INFO: Pod "pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010964612s
Jun 11 15:17:26.700: INFO: Pod "pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018818617s
STEP: Saw pod success
Jun 11 15:17:26.700: INFO: Pod "pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e" satisfied condition "Succeeded or Failed"
Jun 11 15:17:26.704: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:17:26.728: INFO: Waiting for pod pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e to disappear
Jun 11 15:17:26.732: INFO: Pod pod-projected-configmaps-59dd0a02-1374-471b-839e-0bda6cb9395e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 15:17:26.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7673" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5173,"failed":0}

------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:26.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jun 11 15:17:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1117" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 15:17:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8180" for this suite.
â€¢{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":280,"skipped":5208,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:32.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:58
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:17:33.043: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:17:35.051: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:37.049: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:39.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:41.048: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:43.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:45.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:47.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:49.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:51.049: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:53.050: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = false)
Jun 11 15:17:55.048: INFO: The status of Pod test-webserver-85299b1f-8be9-4a83-bbd1-2d44722e20af is Running (Ready = true)
Jun 11 15:17:55.064: INFO: Container started at 2022-06-11 15:17:33 +0000 UTC, pod became ready at 2022-06-11 15:17:53 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jun 11 15:17:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-634" for this suite.

â€¢ [SLOW TEST:22.106 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":281,"skipped":5211,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:17:55.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jun 11 15:17:55.147: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 11 15:18:55.221: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:18:55.229: INFO: Starting informer...
STEP: Starting pods...
Jun 11 15:18:55.461: INFO: Pod1 is running on ip-172-31-41-158. Tainting Node
Jun 11 15:18:57.692: INFO: Pod2 is running on ip-172-31-41-158. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 11 15:19:03.660: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 11 15:19:23.742: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jun 11 15:19:23.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2629" for this suite.

â€¢ [SLOW TEST:88.702 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":282,"skipped":5230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:23.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:19:23.836: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d" in namespace "security-context-test-1455" to be "Succeeded or Failed"
Jun 11 15:19:23.840: INFO: Pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59881ms
Jun 11 15:19:25.855: INFO: Pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019354399s
Jun 11 15:19:27.865: INFO: Pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029200946s
Jun 11 15:19:27.865: INFO: Pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d" satisfied condition "Succeeded or Failed"
Jun 11 15:19:27.882: INFO: Got logs for pod "busybox-privileged-false-1776485e-c47e-4ec6-b454-0efc04e3e34d": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 15:19:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1455" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":283,"skipped":5263,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:27.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jun 11 15:19:27.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-831 create -f -'
Jun 11 15:19:29.732: INFO: stderr: ""
Jun 11 15:19:29.732: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 11 15:19:30.738: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 15:19:30.738: INFO: Found 1 / 1
Jun 11 15:19:30.738: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 11 15:19:30.743: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 15:19:30.743: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 11 15:19:30.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-831 patch pod agnhost-primary-52frw -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 11 15:19:30.836: INFO: stderr: ""
Jun 11 15:19:30.836: INFO: stdout: "pod/agnhost-primary-52frw patched\n"
STEP: checking annotations
Jun 11 15:19:30.841: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 11 15:19:30.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:19:30.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-831" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":284,"skipped":5277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:30.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:19:30.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-937" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":285,"skipped":5348,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jun 11 15:19:30.976: INFO: Waiting up to 5m0s for pod "var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb" in namespace "var-expansion-5158" to be "Succeeded or Failed"
Jun 11 15:19:30.984: INFO: Pod "var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.299195ms
Jun 11 15:19:32.990: INFO: Pod "var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014451173s
Jun 11 15:19:34.997: INFO: Pod "var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021343911s
STEP: Saw pod success
Jun 11 15:19:34.997: INFO: Pod "var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb" satisfied condition "Succeeded or Failed"
Jun 11 15:19:35.003: INFO: Trying to get logs from node ip-172-31-41-158 pod var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb container dapi-container: <nil>
STEP: delete the pod
Jun 11 15:19:35.027: INFO: Waiting for pod var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb to disappear
Jun 11 15:19:35.031: INFO: Pod var-expansion-2941aa4e-0add-4125-9373-93891c1a00bb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 15:19:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5158" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":286,"skipped":5369,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:35.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1416
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1416
I0611 15:19:35.125012      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1416, replica count: 2
I0611 15:19:38.176402      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:19:38.176: INFO: Creating new exec pod
Jun 11 15:19:41.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-1416 exec execpodms2jx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 11 15:19:41.414: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 11 15:19:41.414: INFO: stdout: "externalname-service-mvqnt"
Jun 11 15:19:41.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-1416 exec execpodms2jx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.131.50 80'
Jun 11 15:19:41.634: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 10.100.131.50 80\n\nConnection to 10.100.131.50 80 port [tcp/http] succeeded!\n"
Jun 11 15:19:41.634: INFO: stdout: ""
Jun 11 15:19:42.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-1416 exec execpodms2jx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.131.50 80'
Jun 11 15:19:42.789: INFO: stderr: "+ nc -v -t -w 2 10.100.131.50 80\n+ echo hostName\nConnection to 10.100.131.50 80 port [tcp/http] succeeded!\n"
Jun 11 15:19:42.789: INFO: stdout: "externalname-service-mvqnt"
Jun 11 15:19:42.789: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:19:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1416" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:7.783 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":287,"skipped":5383,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:42.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:19:42.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2016" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":288,"skipped":5383,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:42.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-adea7952-03aa-41e0-8bfd-f8affdf2f3f6
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:19:42.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7829" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":289,"skipped":5394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:42.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jun 11 15:19:43.006: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-9180 proxy --unix-socket=/tmp/kubectl-proxy-unix2144846887/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:19:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9180" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":290,"skipped":5467,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:43.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jun 11 15:19:43.126: INFO: created test-event-1
Jun 11 15:19:43.131: INFO: created test-event-2
Jun 11 15:19:43.137: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 11 15:19:43.142: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 11 15:19:43.162: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jun 11 15:19:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9597" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":291,"skipped":5479,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:19:44.437: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:19:47.490: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:19:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9306" for this suite.
STEP: Destroying namespace "webhook-9306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":292,"skipped":5491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:47.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jun 11 15:19:57.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8794" for this suite.

â€¢ [SLOW TEST:10.093 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":293,"skipped":5533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:19:57.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jun 11 15:19:57.771: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jun 11 15:20:02.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4041" for this suite.

â€¢ [SLOW TEST:5.196 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":294,"skipped":5563,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:02.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:20:03.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1247" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":295,"skipped":5578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:03.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:20:03.118: INFO: The status of Pod busybox-readonly-fs7a70f9ea-a5f0-4902-a8bf-ac8f7909d834 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:20:05.124: INFO: The status of Pod busybox-readonly-fs7a70f9ea-a5f0-4902-a8bf-ac8f7909d834 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jun 11 15:20:05.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-664" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":296,"skipped":5601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:05.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-0f1a59fa-9918-411e-a7c0-af9d498be40c
STEP: Creating a pod to test consume configMaps
Jun 11 15:20:05.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf" in namespace "configmap-9021" to be "Succeeded or Failed"
Jun 11 15:20:05.250: INFO: Pod "pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.750987ms
Jun 11 15:20:07.256: INFO: Pod "pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013500838s
Jun 11 15:20:09.261: INFO: Pod "pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01875272s
STEP: Saw pod success
Jun 11 15:20:09.261: INFO: Pod "pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf" satisfied condition "Succeeded or Failed"
Jun 11 15:20:09.265: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf container configmap-volume-test: <nil>
STEP: delete the pod
Jun 11 15:20:09.300: INFO: Waiting for pod pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf to disappear
Jun 11 15:20:09.309: INFO: Pod pod-configmaps-ad3716ad-d77d-487b-a469-6260f3282daf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:20:09.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9021" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":297,"skipped":5627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:09.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-b39d0c4a-f565-44cb-891c-944789961a0c
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jun 11 15:20:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4206" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":298,"skipped":5667,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:09.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:20:09.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:20:15.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5820" for this suite.

â€¢ [SLOW TEST:6.489 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":299,"skipped":5673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:15.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:20:16.702: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:20:19.742: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:20:19.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1093-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:20:22.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9706" for this suite.
STEP: Destroying namespace "webhook-9706-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:7.147 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":300,"skipped":5707,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:23.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1180, will wait for the garbage collector to delete the pods
Jun 11 15:20:25.237: INFO: Deleting Job.batch foo took: 8.974455ms
Jun 11 15:20:25.338: INFO: Terminating Job.batch foo pods took: 101.151068ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jun 11 15:20:58.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1180" for this suite.

â€¢ [SLOW TEST:35.178 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":301,"skipped":5711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:20:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:20:58.302: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 11 15:21:00.355: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jun 11 15:21:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1465" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":302,"skipped":5735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:01.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-672
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-672 to expose endpoints map[]
Jun 11 15:21:01.475: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 11 15:21:02.501: INFO: successfully validated that service endpoint-test2 in namespace services-672 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-672
Jun 11 15:21:02.525: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:21:04.532: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-672 to expose endpoints map[pod1:[80]]
Jun 11 15:21:04.565: INFO: successfully validated that service endpoint-test2 in namespace services-672 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jun 11 15:21:04.565: INFO: Creating new exec pod
Jun 11 15:21:07.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 11 15:21:07.761: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:07.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:21:07.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.9.123 80'
Jun 11 15:21:07.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.9.123 80\nConnection to 10.99.9.123 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:07.978: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-672
Jun 11 15:21:07.993: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:21:10.004: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-672 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 11 15:21:10.026: INFO: successfully validated that service endpoint-test2 in namespace services-672 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jun 11 15:21:11.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 11 15:21:11.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:11.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:21:11.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.9.123 80'
Jun 11 15:21:11.515: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.99.9.123 80\nConnection to 10.99.9.123 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:11.515: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-672
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-672 to expose endpoints map[pod2:[80]]
Jun 11 15:21:12.574: INFO: successfully validated that service endpoint-test2 in namespace services-672 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jun 11 15:21:13.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 11 15:21:13.780: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:13.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:21:13.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-672 exec execpod45htt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.9.123 80'
Jun 11 15:21:13.974: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.9.123 80\nConnection to 10.99.9.123 80 port [tcp/http] succeeded!\n"
Jun 11 15:21:13.974: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-672
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-672 to expose endpoints map[]
Jun 11 15:21:15.019: INFO: successfully validated that service endpoint-test2 in namespace services-672 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:21:15.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-672" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:13.677 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":303,"skipped":5757,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:15.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-5304d932-ee27-4f26-89f0-085cf2c16244
STEP: Creating a pod to test consume configMaps
Jun 11 15:21:15.149: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731" in namespace "configmap-7432" to be "Succeeded or Failed"
Jun 11 15:21:15.153: INFO: Pod "pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081412ms
Jun 11 15:21:17.160: INFO: Pod "pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011375298s
Jun 11 15:21:19.168: INFO: Pod "pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019562782s
STEP: Saw pod success
Jun 11 15:21:19.168: INFO: Pod "pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731" satisfied condition "Succeeded or Failed"
Jun 11 15:21:19.173: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731 container agnhost-container: <nil>
STEP: delete the pod
Jun 11 15:21:19.197: INFO: Waiting for pod pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731 to disappear
Jun 11 15:21:19.201: INFO: Pod pod-configmaps-d5ead636-f55b-4fea-b800-1c3f24c85731 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jun 11 15:21:19.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7432" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":304,"skipped":5778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:19.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 15:21:35.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7721" for this suite.

â€¢ [SLOW TEST:16.232 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":305,"skipped":5841,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:35.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:21:35.509: INFO: The status of Pod busybox-host-aliasesa8ef3333-b24f-415b-9316-9ee46a6152e6 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:21:37.519: INFO: The status of Pod busybox-host-aliasesa8ef3333-b24f-415b-9316-9ee46a6152e6 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jun 11 15:21:37.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9569" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":306,"skipped":5844,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:37.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 11 15:21:37.614: INFO: Waiting up to 5m0s for pod "pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c" in namespace "emptydir-2105" to be "Succeeded or Failed"
Jun 11 15:21:37.619: INFO: Pod "pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453237ms
Jun 11 15:21:39.626: INFO: Pod "pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011698455s
Jun 11 15:21:41.640: INFO: Pod "pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025597359s
STEP: Saw pod success
Jun 11 15:21:41.640: INFO: Pod "pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c" satisfied condition "Succeeded or Failed"
Jun 11 15:21:41.649: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c container test-container: <nil>
STEP: delete the pod
Jun 11 15:21:41.678: INFO: Waiting for pod pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c to disappear
Jun 11 15:21:41.682: INFO: Pod pod-91d6e0b0-44a9-4af4-8f66-2ef805f98c8c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:21:41.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2105" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5847,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:41.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-74e1bcb3-2cec-4043-a855-7cbc31b66a85
STEP: Creating a pod to test consume secrets
Jun 11 15:21:41.768: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670" in namespace "projected-2415" to be "Succeeded or Failed"
Jun 11 15:21:41.772: INFO: Pod "pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670": Phase="Pending", Reason="", readiness=false. Elapsed: 4.216204ms
Jun 11 15:21:43.785: INFO: Pod "pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016382711s
Jun 11 15:21:45.789: INFO: Pod "pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020962968s
STEP: Saw pod success
Jun 11 15:21:45.790: INFO: Pod "pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670" satisfied condition "Succeeded or Failed"
Jun 11 15:21:45.794: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 11 15:21:45.821: INFO: Waiting for pod pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670 to disappear
Jun 11 15:21:45.826: INFO: Pod pod-projected-secrets-4bcf5978-18b1-4bd6-b815-f902a3245670 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 15:21:45.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2415" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":308,"skipped":5856,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:21:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:21:45.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jun 11 15:21:49.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 create -f -'
Jun 11 15:21:51.204: INFO: stderr: ""
Jun 11 15:21:51.204: INFO: stdout: "e2e-test-crd-publish-openapi-3658-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 11 15:21:51.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 delete e2e-test-crd-publish-openapi-3658-crds test-foo'
Jun 11 15:21:51.284: INFO: stderr: ""
Jun 11 15:21:51.284: INFO: stdout: "e2e-test-crd-publish-openapi-3658-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 11 15:21:51.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 apply -f -'
Jun 11 15:21:52.347: INFO: stderr: ""
Jun 11 15:21:52.347: INFO: stdout: "e2e-test-crd-publish-openapi-3658-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 11 15:21:52.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 delete e2e-test-crd-publish-openapi-3658-crds test-foo'
Jun 11 15:21:52.466: INFO: stderr: ""
Jun 11 15:21:52.466: INFO: stdout: "e2e-test-crd-publish-openapi-3658-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jun 11 15:21:52.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 create -f -'
Jun 11 15:21:52.703: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 11 15:21:52.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 create -f -'
Jun 11 15:21:52.900: INFO: rc: 1
Jun 11 15:21:52.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 apply -f -'
Jun 11 15:21:53.715: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jun 11 15:21:53.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 create -f -'
Jun 11 15:21:53.955: INFO: rc: 1
Jun 11 15:21:53.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 --namespace=crd-publish-openapi-8806 apply -f -'
Jun 11 15:21:54.197: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 11 15:21:54.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 explain e2e-test-crd-publish-openapi-3658-crds'
Jun 11 15:21:54.433: INFO: stderr: ""
Jun 11 15:21:54.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3658-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 11 15:21:54.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 explain e2e-test-crd-publish-openapi-3658-crds.metadata'
Jun 11 15:21:54.652: INFO: stderr: ""
Jun 11 15:21:54.652: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3658-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 11 15:21:54.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 explain e2e-test-crd-publish-openapi-3658-crds.spec'
Jun 11 15:21:54.920: INFO: stderr: ""
Jun 11 15:21:54.920: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3658-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 11 15:21:54.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 explain e2e-test-crd-publish-openapi-3658-crds.spec.bars'
Jun 11 15:21:55.139: INFO: stderr: ""
Jun 11 15:21:55.139: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3658-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 11 15:21:55.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-8806 explain e2e-test-crd-publish-openapi-3658-crds.spec.bars2'
Jun 11 15:21:55.366: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:22:00.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8806" for this suite.

â€¢ [SLOW TEST:14.838 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":309,"skipped":5859,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:00.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jun 11 15:22:06.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1680" for this suite.

â€¢ [SLOW TEST:6.172 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":310,"skipped":5873,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:06.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-e61421e6-725e-443e-a174-788c026ec97e
STEP: Creating a pod to test consume secrets
Jun 11 15:22:06.917: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e" in namespace "projected-8782" to be "Succeeded or Failed"
Jun 11 15:22:06.928: INFO: Pod "pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.212825ms
Jun 11 15:22:08.936: INFO: Pod "pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018056243s
Jun 11 15:22:10.948: INFO: Pod "pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030127208s
STEP: Saw pod success
Jun 11 15:22:10.948: INFO: Pod "pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e" satisfied condition "Succeeded or Failed"
Jun 11 15:22:10.954: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 15:22:10.988: INFO: Waiting for pod pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e to disappear
Jun 11 15:22:10.994: INFO: Pod pod-projected-secrets-51cfd4cf-4eba-4ae4-a298-c5344e24654e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 15:22:10.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8782" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5893,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 11 15:22:11.160: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 11 15:22:11.168: INFO: starting watch
STEP: patching
STEP: updating
Jun 11 15:22:11.194: INFO: waiting for watch events with expected annotations
Jun 11 15:22:11.194: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jun 11 15:22:11.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9964" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":312,"skipped":5898,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:11.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jun 11 15:22:11.366: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:22:13.378: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jun 11 15:22:13.398: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:22:15.408: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun 11 15:22:15.423: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 15:22:15.429: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 11 15:22:17.429: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 11 15:22:17.438: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jun 11 15:22:17.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4123" for this suite.

â€¢ [SLOW TEST:6.196 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":313,"skipped":5904,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:17.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-da6f2fc8-b2e3-4207-9e32-a3e65c00901c
STEP: Creating configMap with name cm-test-opt-upd-565bf289-6ebd-45e4-90b1-b46ff700c611
STEP: Creating the pod
Jun 11 15:22:17.584: INFO: The status of Pod pod-projected-configmaps-e2476a1f-00f9-43bb-914d-80a63509b3bf is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:22:19.594: INFO: The status of Pod pod-projected-configmaps-e2476a1f-00f9-43bb-914d-80a63509b3bf is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-da6f2fc8-b2e3-4207-9e32-a3e65c00901c
STEP: Updating configmap cm-test-opt-upd-565bf289-6ebd-45e4-90b1-b46ff700c611
STEP: Creating configMap with name cm-test-opt-create-34c2e9ee-ed6e-4c72-a344-f02ab50f3082
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jun 11 15:22:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5485" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5918,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:21.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jun 11 15:22:21.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4975" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:21.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-5550
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 11 15:22:21.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 11 15:22:21.905: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:22:23.916: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:25.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:27.911: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:29.917: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:31.916: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:33.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:35.915: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:37.917: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:39.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 11 15:22:41.923: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 11 15:22:41.940: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jun 11 15:22:43.990: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jun 11 15:22:43.991: INFO: Breadth first check of 192.168.149.111 on host 172.31.27.214...
Jun 11 15:22:43.995: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.203.28:9080/dial?request=hostname&protocol=http&host=192.168.149.111&port=8083&tries=1'] Namespace:pod-network-test-5550 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:22:43.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:22:43.996: INFO: ExecWithOptions: Clientset creation
Jun 11 15:22:43.996: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5550/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.203.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.149.111%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 11 15:22:44.092: INFO: Waiting for responses: map[]
Jun 11 15:22:44.092: INFO: reached 192.168.149.111 after 0/1 tries
Jun 11 15:22:44.092: INFO: Breadth first check of 192.168.203.26 on host 172.31.41.158...
Jun 11 15:22:44.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.203.28:9080/dial?request=hostname&protocol=http&host=192.168.203.26&port=8083&tries=1'] Namespace:pod-network-test-5550 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 11 15:22:44.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
Jun 11 15:22:44.100: INFO: ExecWithOptions: Clientset creation
Jun 11 15:22:44.100: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5550/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.203.28%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.203.26%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 11 15:22:44.187: INFO: Waiting for responses: map[]
Jun 11 15:22:44.187: INFO: reached 192.168.203.26 after 0/1 tries
Jun 11 15:22:44.187: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jun 11 15:22:44.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5550" for this suite.

â€¢ [SLOW TEST:22.385 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5957,"failed":0}
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:44.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lfsmm in namespace proxy-2575
I0611 15:22:44.292574      19 runners.go:193] Created replication controller with name: proxy-service-lfsmm, namespace: proxy-2575, replica count: 1
I0611 15:22:45.344616      19 runners.go:193] proxy-service-lfsmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0611 15:22:46.345581      19 runners.go:193] proxy-service-lfsmm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0611 15:22:47.345841      19 runners.go:193] proxy-service-lfsmm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:22:47.352: INFO: setup took 3.101819368s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 11 15:22:47.370: INFO: (0) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.803998ms)
Jun 11 15:22:47.370: INFO: (0) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 17.542594ms)
Jun 11 15:22:47.370: INFO: (0) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.522635ms)
Jun 11 15:22:47.371: INFO: (0) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 18.116033ms)
Jun 11 15:22:47.371: INFO: (0) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 18.175424ms)
Jun 11 15:22:47.371: INFO: (0) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 18.699272ms)
Jun 11 15:22:47.371: INFO: (0) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 19.349652ms)
Jun 11 15:22:47.377: INFO: (0) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 23.937872ms)
Jun 11 15:22:47.377: INFO: (0) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 24.661022ms)
Jun 11 15:22:47.377: INFO: (0) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 24.336587ms)
Jun 11 15:22:47.377: INFO: (0) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 24.977157ms)
Jun 11 15:22:47.377: INFO: (0) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 24.856695ms)
Jun 11 15:22:47.378: INFO: (0) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 25.423194ms)
Jun 11 15:22:47.378: INFO: (0) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 25.541246ms)
Jun 11 15:22:47.379: INFO: (0) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 26.192116ms)
Jun 11 15:22:47.381: INFO: (0) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 28.995908ms)
Jun 11 15:22:47.393: INFO: (1) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 12.147304ms)
Jun 11 15:22:47.394: INFO: (1) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 12.519899ms)
Jun 11 15:22:47.394: INFO: (1) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 12.654561ms)
Jun 11 15:22:47.394: INFO: (1) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 12.601261ms)
Jun 11 15:22:47.399: INFO: (1) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.049388ms)
Jun 11 15:22:47.399: INFO: (1) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.850234ms)
Jun 11 15:22:47.399: INFO: (1) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 17.322362ms)
Jun 11 15:22:47.400: INFO: (1) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 17.8364ms)
Jun 11 15:22:47.400: INFO: (1) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 17.89083ms)
Jun 11 15:22:47.400: INFO: (1) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 18.270596ms)
Jun 11 15:22:47.403: INFO: (1) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 20.793584ms)
Jun 11 15:22:47.409: INFO: (1) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 26.852786ms)
Jun 11 15:22:47.409: INFO: (1) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 27.189531ms)
Jun 11 15:22:47.409: INFO: (1) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 27.193781ms)
Jun 11 15:22:47.409: INFO: (1) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 27.606537ms)
Jun 11 15:22:47.409: INFO: (1) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 27.510275ms)
Jun 11 15:22:47.424: INFO: (2) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 14.53689ms)
Jun 11 15:22:47.425: INFO: (2) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 15.025757ms)
Jun 11 15:22:47.431: INFO: (2) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 20.854605ms)
Jun 11 15:22:47.431: INFO: (2) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 20.54013ms)
Jun 11 15:22:47.432: INFO: (2) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 22.199185ms)
Jun 11 15:22:47.432: INFO: (2) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 22.377488ms)
Jun 11 15:22:47.432: INFO: (2) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 22.285777ms)
Jun 11 15:22:47.432: INFO: (2) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 22.217185ms)
Jun 11 15:22:47.433: INFO: (2) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 23.351903ms)
Jun 11 15:22:47.435: INFO: (2) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 25.482285ms)
Jun 11 15:22:47.436: INFO: (2) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 25.940321ms)
Jun 11 15:22:47.436: INFO: (2) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 26.248937ms)
Jun 11 15:22:47.436: INFO: (2) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 26.753894ms)
Jun 11 15:22:47.436: INFO: (2) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 26.919556ms)
Jun 11 15:22:47.438: INFO: (2) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 27.944672ms)
Jun 11 15:22:47.438: INFO: (2) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 28.355398ms)
Jun 11 15:22:47.450: INFO: (3) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 12.172864ms)
Jun 11 15:22:47.456: INFO: (3) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 15.989582ms)
Jun 11 15:22:47.456: INFO: (3) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.698702ms)
Jun 11 15:22:47.456: INFO: (3) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 17.479574ms)
Jun 11 15:22:47.456: INFO: (3) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 17.014937ms)
Jun 11 15:22:47.457: INFO: (3) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.346122ms)
Jun 11 15:22:47.457: INFO: (3) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 17.26372ms)
Jun 11 15:22:47.457: INFO: (3) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 17.83066ms)
Jun 11 15:22:47.457: INFO: (3) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 17.936361ms)
Jun 11 15:22:47.457: INFO: (3) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 17.23494ms)
Jun 11 15:22:47.462: INFO: (3) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 22.843825ms)
Jun 11 15:22:47.462: INFO: (3) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 22.633232ms)
Jun 11 15:22:47.463: INFO: (3) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 24.059904ms)
Jun 11 15:22:47.463: INFO: (3) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 24.302058ms)
Jun 11 15:22:47.463: INFO: (3) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 23.942341ms)
Jun 11 15:22:47.463: INFO: (3) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 24.173415ms)
Jun 11 15:22:47.476: INFO: (4) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 12.260315ms)
Jun 11 15:22:47.476: INFO: (4) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 12.52552ms)
Jun 11 15:22:47.476: INFO: (4) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 13.255011ms)
Jun 11 15:22:47.476: INFO: (4) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 12.681562ms)
Jun 11 15:22:47.477: INFO: (4) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 13.321451ms)
Jun 11 15:22:47.477: INFO: (4) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 12.53878ms)
Jun 11 15:22:47.477: INFO: (4) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 13.033407ms)
Jun 11 15:22:47.477: INFO: (4) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 13.521105ms)
Jun 11 15:22:47.478: INFO: (4) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 13.774528ms)
Jun 11 15:22:47.479: INFO: (4) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 15.145959ms)
Jun 11 15:22:47.480: INFO: (4) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 16.154414ms)
Jun 11 15:22:47.483: INFO: (4) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 19.323482ms)
Jun 11 15:22:47.483: INFO: (4) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 18.926395ms)
Jun 11 15:22:47.484: INFO: (4) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 20.53857ms)
Jun 11 15:22:47.485: INFO: (4) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 20.641291ms)
Jun 11 15:22:47.485: INFO: (4) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 20.797784ms)
Jun 11 15:22:47.493: INFO: (5) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 8.112822ms)
Jun 11 15:22:47.499: INFO: (5) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 13.331502ms)
Jun 11 15:22:47.499: INFO: (5) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 13.149749ms)
Jun 11 15:22:47.499: INFO: (5) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 13.23679ms)
Jun 11 15:22:47.499: INFO: (5) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 13.452153ms)
Jun 11 15:22:47.499: INFO: (5) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 13.740197ms)
Jun 11 15:22:47.500: INFO: (5) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 14.470649ms)
Jun 11 15:22:47.506: INFO: (5) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 19.042058ms)
Jun 11 15:22:47.506: INFO: (5) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 20.056363ms)
Jun 11 15:22:47.506: INFO: (5) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 19.293831ms)
Jun 11 15:22:47.506: INFO: (5) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 19.459855ms)
Jun 11 15:22:47.506: INFO: (5) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 20.306007ms)
Jun 11 15:22:47.507: INFO: (5) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 21.638027ms)
Jun 11 15:22:47.507: INFO: (5) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 21.261341ms)
Jun 11 15:22:47.509: INFO: (5) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 22.008203ms)
Jun 11 15:22:47.509: INFO: (5) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 22.901786ms)
Jun 11 15:22:47.527: INFO: (6) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 17.719388ms)
Jun 11 15:22:47.533: INFO: (6) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 23.371583ms)
Jun 11 15:22:47.533: INFO: (6) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 24.188986ms)
Jun 11 15:22:47.534: INFO: (6) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 24.660123ms)
Jun 11 15:22:47.534: INFO: (6) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 24.52878ms)
Jun 11 15:22:47.534: INFO: (6) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 24.655893ms)
Jun 11 15:22:47.535: INFO: (6) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 24.717834ms)
Jun 11 15:22:47.536: INFO: (6) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 26.779614ms)
Jun 11 15:22:47.536: INFO: (6) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 26.991538ms)
Jun 11 15:22:47.537: INFO: (6) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 27.380834ms)
Jun 11 15:22:47.540: INFO: (6) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 30.47515ms)
Jun 11 15:22:47.542: INFO: (6) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 32.242477ms)
Jun 11 15:22:47.542: INFO: (6) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 32.290218ms)
Jun 11 15:22:47.543: INFO: (6) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 33.11792ms)
Jun 11 15:22:47.543: INFO: (6) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 33.865822ms)
Jun 11 15:22:47.543: INFO: (6) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 33.361674ms)
Jun 11 15:22:47.556: INFO: (7) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 12.671052ms)
Jun 11 15:22:47.558: INFO: (7) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 14.227655ms)
Jun 11 15:22:47.559: INFO: (7) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 15.031057ms)
Jun 11 15:22:47.559: INFO: (7) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 14.726172ms)
Jun 11 15:22:47.567: INFO: (7) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 22.186025ms)
Jun 11 15:22:47.567: INFO: (7) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 22.945847ms)
Jun 11 15:22:47.567: INFO: (7) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 23.332422ms)
Jun 11 15:22:47.567: INFO: (7) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 23.313592ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 23.270751ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 23.776649ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 23.416504ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 23.309023ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 24.172246ms)
Jun 11 15:22:47.568: INFO: (7) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 24.525481ms)
Jun 11 15:22:47.570: INFO: (7) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 24.974797ms)
Jun 11 15:22:47.570: INFO: (7) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 26.383729ms)
Jun 11 15:22:47.587: INFO: (8) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 16.168814ms)
Jun 11 15:22:47.587: INFO: (8) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 16.52779ms)
Jun 11 15:22:47.587: INFO: (8) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 16.945666ms)
Jun 11 15:22:47.589: INFO: (8) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 18.48088ms)
Jun 11 15:22:47.589: INFO: (8) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 18.360408ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 18.747073ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 18.956916ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 18.859904ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 19.473355ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 19.350033ms)
Jun 11 15:22:47.590: INFO: (8) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 19.318642ms)
Jun 11 15:22:47.594: INFO: (8) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 23.506595ms)
Jun 11 15:22:47.594: INFO: (8) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 23.682888ms)
Jun 11 15:22:47.594: INFO: (8) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 23.734238ms)
Jun 11 15:22:47.595: INFO: (8) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 24.102064ms)
Jun 11 15:22:47.595: INFO: (8) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 24.629342ms)
Jun 11 15:22:47.612: INFO: (9) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.468898ms)
Jun 11 15:22:47.612: INFO: (9) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 15.791218ms)
Jun 11 15:22:47.613: INFO: (9) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 16.981886ms)
Jun 11 15:22:47.612: INFO: (9) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 17.16263ms)
Jun 11 15:22:47.613: INFO: (9) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 17.21532ms)
Jun 11 15:22:47.613: INFO: (9) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 16.220786ms)
Jun 11 15:22:47.612: INFO: (9) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 16.298986ms)
Jun 11 15:22:47.612: INFO: (9) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.609131ms)
Jun 11 15:22:47.613: INFO: (9) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 16.068243ms)
Jun 11 15:22:47.615: INFO: (9) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 19.100729ms)
Jun 11 15:22:47.615: INFO: (9) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 18.50693ms)
Jun 11 15:22:47.616: INFO: (9) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 19.886801ms)
Jun 11 15:22:47.616: INFO: (9) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 20.096654ms)
Jun 11 15:22:47.616: INFO: (9) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 20.034823ms)
Jun 11 15:22:47.617: INFO: (9) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 20.257186ms)
Jun 11 15:22:47.618: INFO: (9) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 22.125925ms)
Jun 11 15:22:47.626: INFO: (10) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 8.273445ms)
Jun 11 15:22:47.634: INFO: (10) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 15.099758ms)
Jun 11 15:22:47.634: INFO: (10) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 15.105678ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.393222ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 16.187544ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 16.55227ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 16.370647ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 17.127638ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 16.313937ms)
Jun 11 15:22:47.635: INFO: (10) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 15.266151ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 22.349108ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 22.738583ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 21.415364ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 22.371078ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 22.692853ms)
Jun 11 15:22:47.641: INFO: (10) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 23.297012ms)
Jun 11 15:22:47.657: INFO: (11) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 15.004336ms)
Jun 11 15:22:47.657: INFO: (11) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 14.685962ms)
Jun 11 15:22:47.657: INFO: (11) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 14.738783ms)
Jun 11 15:22:47.659: INFO: (11) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 17.192569ms)
Jun 11 15:22:47.659: INFO: (11) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.263861ms)
Jun 11 15:22:47.660: INFO: (11) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.783988ms)
Jun 11 15:22:47.660: INFO: (11) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 17.916931ms)
Jun 11 15:22:47.661: INFO: (11) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 18.54287ms)
Jun 11 15:22:47.661: INFO: (11) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 18.758544ms)
Jun 11 15:22:47.661: INFO: (11) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 18.904756ms)
Jun 11 15:22:47.661: INFO: (11) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 19.004857ms)
Jun 11 15:22:47.665: INFO: (11) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 23.104969ms)
Jun 11 15:22:47.668: INFO: (11) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 25.908671ms)
Jun 11 15:22:47.668: INFO: (11) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 25.500235ms)
Jun 11 15:22:47.668: INFO: (11) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 25.684338ms)
Jun 11 15:22:47.668: INFO: (11) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 25.500456ms)
Jun 11 15:22:47.677: INFO: (12) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 8.877104ms)
Jun 11 15:22:47.683: INFO: (12) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 13.683717ms)
Jun 11 15:22:47.683: INFO: (12) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 14.766353ms)
Jun 11 15:22:47.683: INFO: (12) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 14.272696ms)
Jun 11 15:22:47.683: INFO: (12) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 14.375868ms)
Jun 11 15:22:47.685: INFO: (12) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 16.074743ms)
Jun 11 15:22:47.685: INFO: (12) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 16.53887ms)
Jun 11 15:22:47.685: INFO: (12) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 16.620102ms)
Jun 11 15:22:47.685: INFO: (12) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 16.388288ms)
Jun 11 15:22:47.685: INFO: (12) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.732193ms)
Jun 11 15:22:47.687: INFO: (12) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 18.762874ms)
Jun 11 15:22:47.687: INFO: (12) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 18.968257ms)
Jun 11 15:22:47.690: INFO: (12) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 20.965657ms)
Jun 11 15:22:47.690: INFO: (12) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 21.584376ms)
Jun 11 15:22:47.690: INFO: (12) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 21.793049ms)
Jun 11 15:22:47.690: INFO: (12) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 21.509885ms)
Jun 11 15:22:47.705: INFO: (13) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 13.839879ms)
Jun 11 15:22:47.711: INFO: (13) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 19.813729ms)
Jun 11 15:22:47.711: INFO: (13) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 19.85952ms)
Jun 11 15:22:47.713: INFO: (13) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 21.729369ms)
Jun 11 15:22:47.713: INFO: (13) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 21.707508ms)
Jun 11 15:22:47.713: INFO: (13) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 21.556476ms)
Jun 11 15:22:47.713: INFO: (13) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 21.710108ms)
Jun 11 15:22:47.714: INFO: (13) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 22.240836ms)
Jun 11 15:22:47.714: INFO: (13) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 22.217305ms)
Jun 11 15:22:47.714: INFO: (13) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 22.215966ms)
Jun 11 15:22:47.716: INFO: (13) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 24.651392ms)
Jun 11 15:22:47.717: INFO: (13) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 26.002473ms)
Jun 11 15:22:47.717: INFO: (13) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 25.930071ms)
Jun 11 15:22:47.719: INFO: (13) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 27.550886ms)
Jun 11 15:22:47.721: INFO: (13) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 29.477926ms)
Jun 11 15:22:47.721: INFO: (13) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 29.389554ms)
Jun 11 15:22:47.732: INFO: (14) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 10.006531ms)
Jun 11 15:22:47.732: INFO: (14) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 10.255614ms)
Jun 11 15:22:47.732: INFO: (14) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 10.816003ms)
Jun 11 15:22:47.741: INFO: (14) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 18.464729ms)
Jun 11 15:22:47.741: INFO: (14) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 18.935036ms)
Jun 11 15:22:47.741: INFO: (14) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 17.774988ms)
Jun 11 15:22:47.741: INFO: (14) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 17.645277ms)
Jun 11 15:22:47.741: INFO: (14) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 18.53036ms)
Jun 11 15:22:47.742: INFO: (14) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 19.101738ms)
Jun 11 15:22:47.742: INFO: (14) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 18.854545ms)
Jun 11 15:22:47.742: INFO: (14) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 20.076363ms)
Jun 11 15:22:47.742: INFO: (14) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 19.011177ms)
Jun 11 15:22:47.746: INFO: (14) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 23.093529ms)
Jun 11 15:22:47.747: INFO: (14) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 23.374764ms)
Jun 11 15:22:47.748: INFO: (14) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 24.894197ms)
Jun 11 15:22:47.749: INFO: (14) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 26.011313ms)
Jun 11 15:22:47.763: INFO: (15) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 13.450033ms)
Jun 11 15:22:47.763: INFO: (15) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 14.166154ms)
Jun 11 15:22:47.763: INFO: (15) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 14.046303ms)
Jun 11 15:22:47.764: INFO: (15) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 13.968061ms)
Jun 11 15:22:47.764: INFO: (15) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 13.561585ms)
Jun 11 15:22:47.766: INFO: (15) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 15.703917ms)
Jun 11 15:22:47.766: INFO: (15) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 16.131323ms)
Jun 11 15:22:47.766: INFO: (15) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 15.638126ms)
Jun 11 15:22:47.766: INFO: (15) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.773728ms)
Jun 11 15:22:47.766: INFO: (15) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.924611ms)
Jun 11 15:22:47.770: INFO: (15) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 19.945842ms)
Jun 11 15:22:47.772: INFO: (15) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 21.393634ms)
Jun 11 15:22:47.772: INFO: (15) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 21.490165ms)
Jun 11 15:22:47.772: INFO: (15) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 22.012863ms)
Jun 11 15:22:47.772: INFO: (15) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 22.407659ms)
Jun 11 15:22:47.772: INFO: (15) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 22.422069ms)
Jun 11 15:22:47.788: INFO: (16) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.473544ms)
Jun 11 15:22:47.788: INFO: (16) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.472504ms)
Jun 11 15:22:47.788: INFO: (16) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 15.450763ms)
Jun 11 15:22:47.791: INFO: (16) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 18.202415ms)
Jun 11 15:22:47.791: INFO: (16) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 17.940731ms)
Jun 11 15:22:47.791: INFO: (16) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 18.404628ms)
Jun 11 15:22:47.791: INFO: (16) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 18.377188ms)
Jun 11 15:22:47.791: INFO: (16) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 18.475739ms)
Jun 11 15:22:47.792: INFO: (16) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 19.610426ms)
Jun 11 15:22:47.792: INFO: (16) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 19.476364ms)
Jun 11 15:22:47.794: INFO: (16) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 21.322282ms)
Jun 11 15:22:47.803: INFO: (16) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 29.599077ms)
Jun 11 15:22:47.803: INFO: (16) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 29.852281ms)
Jun 11 15:22:47.803: INFO: (16) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 30.43482ms)
Jun 11 15:22:47.803: INFO: (16) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 29.760209ms)
Jun 11 15:22:47.803: INFO: (16) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 29.850341ms)
Jun 11 15:22:47.811: INFO: (17) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 8.304876ms)
Jun 11 15:22:47.812: INFO: (17) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 8.684691ms)
Jun 11 15:22:47.819: INFO: (17) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 15.035277ms)
Jun 11 15:22:47.819: INFO: (17) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 14.834554ms)
Jun 11 15:22:47.819: INFO: (17) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 14.831474ms)
Jun 11 15:22:47.819: INFO: (17) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 16.446019ms)
Jun 11 15:22:47.820: INFO: (17) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 15.970761ms)
Jun 11 15:22:47.820: INFO: (17) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 16.507989ms)
Jun 11 15:22:47.820: INFO: (17) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 16.643071ms)
Jun 11 15:22:47.820: INFO: (17) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 16.120614ms)
Jun 11 15:22:47.823: INFO: (17) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 18.645331ms)
Jun 11 15:22:47.823: INFO: (17) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 19.788748ms)
Jun 11 15:22:47.826: INFO: (17) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 21.415344ms)
Jun 11 15:22:47.826: INFO: (17) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 22.076874ms)
Jun 11 15:22:47.826: INFO: (17) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 21.812659ms)
Jun 11 15:22:47.828: INFO: (17) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 23.959672ms)
Jun 11 15:22:47.847: INFO: (18) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 17.22677ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 22.254136ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 21.962361ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 21.464284ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 21.228091ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 21.689797ms)
Jun 11 15:22:47.851: INFO: (18) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 21.600376ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 22.960386ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 23.20834ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 23.043008ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 23.025878ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 23.780469ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 23.945432ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 24.187765ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 23.923812ms)
Jun 11 15:22:47.853: INFO: (18) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 24.340087ms)
Jun 11 15:22:47.863: INFO: (19) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:462/proxy/: tls qux (200; 9.800629ms)
Jun 11 15:22:47.865: INFO: (19) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 11.455563ms)
Jun 11 15:22:47.865: INFO: (19) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">test<... (200; 11.564304ms)
Jun 11 15:22:47.866: INFO: (19) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:162/proxy/: bar (200; 12.041532ms)
Jun 11 15:22:47.866: INFO: (19) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:1080/proxy/rewriteme">... (200; 12.910145ms)
Jun 11 15:22:47.866: INFO: (19) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:460/proxy/: tls baz (200; 12.413557ms)
Jun 11 15:22:47.867: INFO: (19) /api/v1/namespaces/proxy-2575/pods/http:proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 12.60278ms)
Jun 11 15:22:47.875: INFO: (19) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb/proxy/rewriteme">test</a> (200; 20.592991ms)
Jun 11 15:22:47.876: INFO: (19) /api/v1/namespaces/proxy-2575/pods/proxy-service-lfsmm-sl9lb:160/proxy/: foo (200; 21.163ms)
Jun 11 15:22:47.877: INFO: (19) /api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/: <a href="/api/v1/namespaces/proxy-2575/pods/https:proxy-service-lfsmm-sl9lb:443/proxy/tlsrewritem... (200; 22.369588ms)
Jun 11 15:22:47.877: INFO: (19) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname2/proxy/: tls qux (200; 23.319152ms)
Jun 11 15:22:47.877: INFO: (19) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname2/proxy/: bar (200; 23.755049ms)
Jun 11 15:22:47.877: INFO: (19) /api/v1/namespaces/proxy-2575/services/proxy-service-lfsmm:portname1/proxy/: foo (200; 23.002757ms)
Jun 11 15:22:47.878: INFO: (19) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname1/proxy/: foo (200; 23.205781ms)
Jun 11 15:22:47.879: INFO: (19) /api/v1/namespaces/proxy-2575/services/http:proxy-service-lfsmm:portname2/proxy/: bar (200; 24.388789ms)
Jun 11 15:22:47.879: INFO: (19) /api/v1/namespaces/proxy-2575/services/https:proxy-service-lfsmm:tlsportname1/proxy/: tls baz (200; 25.83449ms)
STEP: deleting ReplicationController proxy-service-lfsmm in namespace proxy-2575, will wait for the garbage collector to delete the pods
Jun 11 15:22:47.949: INFO: Deleting ReplicationController proxy-service-lfsmm took: 12.983446ms
Jun 11 15:22:48.052: INFO: Terminating ReplicationController proxy-service-lfsmm pods took: 102.208644ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jun 11 15:22:49.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2575" for this suite.

â€¢ [SLOW TEST:5.275 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":317,"skipped":5957,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jun 11 15:22:49.556: INFO: created test-podtemplate-1
Jun 11 15:22:49.570: INFO: created test-podtemplate-2
Jun 11 15:22:49.579: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 11 15:22:49.592: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 11 15:22:49.638: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jun 11 15:22:49.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3571" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":318,"skipped":5962,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:22:49.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 15:23:17.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1573" for this suite.

â€¢ [SLOW TEST:28.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":319,"skipped":5965,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:23:17.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:756
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6164
STEP: creating service affinity-clusterip-transition in namespace services-6164
STEP: creating replication controller affinity-clusterip-transition in namespace services-6164
I0611 15:23:17.929357      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6164, replica count: 3
I0611 15:23:20.980572      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 11 15:23:20.998: INFO: Creating new exec pod
Jun 11 15:23:24.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-6164 exec execpod-affinitygv96l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun 11 15:23:24.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 11 15:23:24.259: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:23:24.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-6164 exec execpod-affinitygv96l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.165.60 80'
Jun 11 15:23:24.452: INFO: stderr: "+ nc -v -t -w 2 10.102.165.60 80\nConnection to 10.102.165.60 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jun 11 15:23:24.452: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 11 15:23:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-6164 exec execpod-affinitygv96l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.165.60:80/ ; done'
Jun 11 15:23:24.946: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n"
Jun 11 15:23:24.946: INFO: stdout: "\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-kw2gl\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-kw2gl\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-kw2gl\naffinity-clusterip-transition-kw2gl\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-hv7n6\naffinity-clusterip-transition-hv7n6"
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-kw2gl
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-kw2gl
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-kw2gl
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-kw2gl
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.946: INFO: Received response from host: affinity-clusterip-transition-hv7n6
Jun 11 15:23:24.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=services-6164 exec execpod-affinitygv96l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.165.60:80/ ; done'
Jun 11 15:23:25.312: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.165.60:80/\n"
Jun 11 15:23:25.312: INFO: stdout: "\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x\naffinity-clusterip-transition-q4n5x"
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Received response from host: affinity-clusterip-transition-q4n5x
Jun 11 15:23:25.312: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6164, will wait for the garbage collector to delete the pods
Jun 11 15:23:25.402: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.981771ms
Jun 11 15:23:25.503: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.611246ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jun 11 15:23:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6164" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:760

â€¢ [SLOW TEST:9.835 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":320,"skipped":5976,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:23:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 11 15:23:27.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7357  702ce75e-310a-4e9c-b793-40faa6feb1dc 43007 0 2022-06-11 15:23:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-11 15:23:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 11 15:23:27.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7357  702ce75e-310a-4e9c-b793-40faa6feb1dc 43008 0 2022-06-11 15:23:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-11 15:23:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jun 11 15:23:27.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7357" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":321,"skipped":5990,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:23:27.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jun 11 15:23:33.862: INFO: 80 pods remaining
Jun 11 15:23:33.862: INFO: 80 pods has nil DeletionTimestamp
Jun 11 15:23:33.862: INFO: 
Jun 11 15:23:34.895: INFO: 70 pods remaining
Jun 11 15:23:34.895: INFO: 69 pods has nil DeletionTimestamp
Jun 11 15:23:34.895: INFO: 
Jun 11 15:23:35.849: INFO: 60 pods remaining
Jun 11 15:23:35.849: INFO: 60 pods has nil DeletionTimestamp
Jun 11 15:23:35.849: INFO: 
Jun 11 15:23:36.854: INFO: 40 pods remaining
Jun 11 15:23:36.854: INFO: 40 pods has nil DeletionTimestamp
Jun 11 15:23:36.854: INFO: 
Jun 11 15:23:37.869: INFO: 31 pods remaining
Jun 11 15:23:37.869: INFO: 31 pods has nil DeletionTimestamp
Jun 11 15:23:37.869: INFO: 
Jun 11 15:23:38.850: INFO: 20 pods remaining
Jun 11 15:23:38.850: INFO: 20 pods has nil DeletionTimestamp
Jun 11 15:23:38.850: INFO: 
STEP: Gathering metrics
Jun 11 15:23:39.888: INFO: The status of Pod kube-controller-manager-ip-172-31-30-216 is Running (Ready = true)
Jun 11 15:23:39.999: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jun 11 15:23:40.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8980" for this suite.

â€¢ [SLOW TEST:12.285 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":322,"skipped":5992,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:23:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6772
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6772
STEP: Waiting until pod test-pod will start running in namespace statefulset-6772
STEP: Creating statefulset with conflicting port in namespace statefulset-6772
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6772
Jun 11 15:23:54.161: INFO: Observed stateful pod in namespace: statefulset-6772, name: ss-0, uid: 7a3c48e2-f8fe-472b-84e1-c49a20acbe3c, status phase: Pending. Waiting for statefulset controller to delete.
Jun 11 15:23:54.189: INFO: Observed stateful pod in namespace: statefulset-6772, name: ss-0, uid: 7a3c48e2-f8fe-472b-84e1-c49a20acbe3c, status phase: Failed. Waiting for statefulset controller to delete.
Jun 11 15:23:54.234: INFO: Observed stateful pod in namespace: statefulset-6772, name: ss-0, uid: 7a3c48e2-f8fe-472b-84e1-c49a20acbe3c, status phase: Failed. Waiting for statefulset controller to delete.
Jun 11 15:23:54.234: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6772
STEP: Removing pod with conflicting port in namespace statefulset-6772
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6772 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 15:23:56.322: INFO: Deleting all statefulset in ns statefulset-6772
Jun 11 15:23:56.328: INFO: Scaling statefulset ss to 0
Jun 11 15:24:06.383: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:24:06.387: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 15:24:06.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6772" for this suite.

â€¢ [SLOW TEST:26.401 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":323,"skipped":6002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:06.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:24:06.486: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 11 15:24:11.507: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 11 15:24:11.507: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 15:24:13.610: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5441  5a47ed27-56a7-4ae8-b09b-e47de38a9369 44659 1 2022-06-11 15:24:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-11 15:24:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 15:24:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002658f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-11 15:24:11 +0000 UTC,LastTransitionTime:2022-06-11 15:24:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-54c6999cf6" has successfully progressed.,LastUpdateTime:2022-06-11 15:24:12 +0000 UTC,LastTransitionTime:2022-06-11 15:24:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 11 15:24:13.615: INFO: New ReplicaSet "test-cleanup-deployment-54c6999cf6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-54c6999cf6  deployment-5441  433104d1-75b3-4101-9500-ac3eeb0649cb 44649 1 2022-06-11 15:24:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:54c6999cf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5a47ed27-56a7-4ae8-b09b-e47de38a9369 0xc0026592e7 0xc0026592e8}] []  [{kube-controller-manager Update apps/v1 2022-06-11 15:24:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a47ed27-56a7-4ae8-b09b-e47de38a9369\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 15:24:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 54c6999cf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:54c6999cf6] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002659398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 11 15:24:13.627: INFO: Pod "test-cleanup-deployment-54c6999cf6-jj5v6" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-54c6999cf6-jj5v6 test-cleanup-deployment-54c6999cf6- deployment-5441  e777c66b-501b-4449-a6ca-208c288c09c4 44648 0 2022-06-11 15:24:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:54c6999cf6] map[cni.projectcalico.org/containerID:1d9825521182df2661381250abf157a68c493f8954707385d14572a2a606da1a cni.projectcalico.org/podIP:192.168.203.41/32 cni.projectcalico.org/podIPs:192.168.203.41/32] [{apps/v1 ReplicaSet test-cleanup-deployment-54c6999cf6 433104d1-75b3-4101-9500-ac3eeb0649cb 0xc002659727 0xc002659728}] []  [{kube-controller-manager Update v1 2022-06-11 15:24:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"433104d1-75b3-4101-9500-ac3eeb0649cb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-06-11 15:24:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-11 15:24:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8q8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8q8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:24:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:24:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:24:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:192.168.203.41,StartTime:2022-06-11 15:24:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-11 15:24:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.36,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:f5241226198f5a54d22540acf2b3933ea0f49458f90c51fc75833d0c428687b8,ContainerID:containerd://59b6ca144f8e938d42e188e44a905974167064574dc71f99080bcb2a73abb5a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 15:24:13.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5441" for this suite.

â€¢ [SLOW TEST:7.200 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":324,"skipped":6031,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:13.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-2470/secret-test-74cfaa11-414e-440e-bc20-6e582d940d2b
STEP: Creating a pod to test consume secrets
Jun 11 15:24:13.692: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490" in namespace "secrets-2470" to be "Succeeded or Failed"
Jun 11 15:24:13.700: INFO: Pod "pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490": Phase="Pending", Reason="", readiness=false. Elapsed: 7.28784ms
Jun 11 15:24:15.707: INFO: Pod "pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014999517s
Jun 11 15:24:17.718: INFO: Pod "pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025480172s
STEP: Saw pod success
Jun 11 15:24:17.718: INFO: Pod "pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490" satisfied condition "Succeeded or Failed"
Jun 11 15:24:17.722: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490 container env-test: <nil>
STEP: delete the pod
Jun 11 15:24:17.761: INFO: Waiting for pod pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490 to disappear
Jun 11 15:24:17.769: INFO: Pod pod-configmaps-8e346764-9cf1-4a1c-a782-02f2c4160490 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jun 11 15:24:17.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2470" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":6036,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:17.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jun 11 15:24:17.853: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jun 11 15:24:17.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1824" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":326,"skipped":6039,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:17.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 11 15:24:17.934: INFO: The status of Pod pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:24:19.943: INFO: The status of Pod pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 11 15:24:20.474: INFO: Successfully updated pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc"
Jun 11 15:24:20.474: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc" in namespace "pods-4807" to be "terminated due to deadline exceeded"
Jun 11 15:24:20.478: INFO: Pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.575979ms
Jun 11 15:24:22.490: INFO: Pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016225496s
Jun 11 15:24:24.502: INFO: Pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.028289176s
Jun 11 15:24:26.516: INFO: Pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.042767703s
Jun 11 15:24:26.516: INFO: Pod "pod-update-activedeadlineseconds-150735a6-72e6-4c0b-b4b7-1493fbb113bc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jun 11 15:24:26.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4807" for this suite.

â€¢ [SLOW TEST:8.663 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":327,"skipped":6056,"failed":0}
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:26.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jun 11 15:24:26.584: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jun 11 15:24:26.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-8165" for this suite.
â€¢{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":328,"skipped":6063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:26.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-f28r
STEP: Creating a pod to test atomic-volume-subpath
Jun 11 15:24:26.701: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-f28r" in namespace "subpath-1644" to be "Succeeded or Failed"
Jun 11 15:24:26.712: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651341ms
Jun 11 15:24:28.723: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 2.021362141s
Jun 11 15:24:30.732: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 4.03067232s
Jun 11 15:24:32.742: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 6.0412689s
Jun 11 15:24:34.754: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 8.052908958s
Jun 11 15:24:36.766: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 10.064477817s
Jun 11 15:24:38.777: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 12.075952188s
Jun 11 15:24:40.786: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 14.084737415s
Jun 11 15:24:42.798: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 16.096635741s
Jun 11 15:24:44.815: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 18.11423627s
Jun 11 15:24:46.830: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=true. Elapsed: 20.12843256s
Jun 11 15:24:48.843: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Running", Reason="", readiness=false. Elapsed: 22.141581216s
Jun 11 15:24:50.859: INFO: Pod "pod-subpath-test-downwardapi-f28r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.157822325s
STEP: Saw pod success
Jun 11 15:24:50.859: INFO: Pod "pod-subpath-test-downwardapi-f28r" satisfied condition "Succeeded or Failed"
Jun 11 15:24:50.864: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-subpath-test-downwardapi-f28r container test-container-subpath-downwardapi-f28r: <nil>
STEP: delete the pod
Jun 11 15:24:50.891: INFO: Waiting for pod pod-subpath-test-downwardapi-f28r to disappear
Jun 11 15:24:50.898: INFO: Pod pod-subpath-test-downwardapi-f28r no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-f28r
Jun 11 15:24:50.898: INFO: Deleting pod "pod-subpath-test-downwardapi-f28r" in namespace "subpath-1644"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jun 11 15:24:50.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1644" for this suite.

â€¢ [SLOW TEST:24.297 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":329,"skipped":6088,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jun 11 15:24:50.983: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:24:52.993: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jun 11 15:24:54.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6323" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":330,"skipped":6090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:24:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8198 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8198;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8198 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8198;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8198.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8198.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8198.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8198.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8198.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8198.svc;check="$$(dig +notcp +noall +answer +search 233.76.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.76.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.76.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.76.233_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8198 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8198;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8198 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8198;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8198.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8198.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8198.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8198.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8198.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8198.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8198.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8198.svc;check="$$(dig +notcp +noall +answer +search 233.76.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.76.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.76.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.76.233_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 11 15:24:56.227: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.234: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.240: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.247: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.269: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.276: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.331: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.337: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.355: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.368: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.386: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.400: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.413: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:24:56.443: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:01.451: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.460: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.469: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.476: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.502: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.510: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.519: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.529: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.579: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.588: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.599: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.608: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.617: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.633: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.643: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:01.678: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:06.453: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.462: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.490: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.496: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.502: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.532: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.538: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.551: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.566: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.577: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.584: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.592: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:06.615: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:11.451: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.462: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.488: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.497: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.506: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.515: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.526: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.570: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.578: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.585: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.597: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.607: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.614: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.631: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:11.654: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:16.454: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.464: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.485: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.491: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.498: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.505: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.552: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.560: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.568: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.574: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.586: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.612: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.620: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:16.646: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:21.452: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.472: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.511: INFO: Unable to read wheezy_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.522: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.532: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.603: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.610: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.618: INFO: Unable to read jessie_udp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198 from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.632: INFO: Unable to read jessie_udp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.644: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc from pod dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde: the server could not find the requested resource (get pods dns-test-e6a783a0-e74d-491e-87c2-781727186fde)
Jun 11 15:25:21.682: INFO: Lookups using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8198 wheezy_tcp@dns-test-service.dns-8198 wheezy_udp@dns-test-service.dns-8198.svc wheezy_tcp@dns-test-service.dns-8198.svc wheezy_udp@_http._tcp.dns-test-service.dns-8198.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8198.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8198 jessie_tcp@dns-test-service.dns-8198 jessie_udp@dns-test-service.dns-8198.svc jessie_tcp@dns-test-service.dns-8198.svc jessie_udp@_http._tcp.dns-test-service.dns-8198.svc jessie_tcp@_http._tcp.dns-test-service.dns-8198.svc]

Jun 11 15:25:26.628: INFO: DNS probes using dns-8198/dns-test-e6a783a0-e74d-491e-87c2-781727186fde succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jun 11 15:25:26.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8198" for this suite.

â€¢ [SLOW TEST:32.902 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":331,"skipped":6121,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:26.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jun 11 15:25:27.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-217 create -f -'
Jun 11 15:25:28.028: INFO: stderr: ""
Jun 11 15:25:28.028: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 11 15:25:28.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-217 diff -f -'
Jun 11 15:25:28.348: INFO: rc: 1
Jun 11 15:25:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-217 delete -f -'
Jun 11 15:25:28.453: INFO: stderr: ""
Jun 11 15:25:28.453: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:25:28.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-217" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":332,"skipped":6132,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 15:25:28.554: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:28.554: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:28.554: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:28.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:25:28.564: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:29.571: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:29.571: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:29.571: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:29.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:25:29.576: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:30.572: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.572: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.572: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 15:25:30.577: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 11 15:25:30.621: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.621: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.621: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:30.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 15:25:30.631: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:31.641: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:31.642: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:31.642: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:31.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 11 15:25:31.648: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:32.641: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:32.641: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:32.641: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:32.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 15:25:32.648: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8236, will wait for the garbage collector to delete the pods
Jun 11 15:25:32.731: INFO: Deleting DaemonSet.extensions daemon-set took: 8.818233ms
Jun 11 15:25:32.831: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.168196ms
Jun 11 15:25:36.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:25:36.140: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 11 15:25:36.144: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45266"},"items":null}

Jun 11 15:25:36.148: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45266"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 15:25:36.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8236" for this suite.

â€¢ [SLOW TEST:7.696 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":333,"skipped":6139,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:36.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jun 11 15:25:47.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9713" for this suite.

â€¢ [SLOW TEST:11.475 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":334,"skipped":6154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 15:25:47.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327" in namespace "downward-api-7533" to be "Succeeded or Failed"
Jun 11 15:25:47.715: INFO: Pod "downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327": Phase="Pending", Reason="", readiness=false. Elapsed: 8.55514ms
Jun 11 15:25:49.727: INFO: Pod "downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020058213s
Jun 11 15:25:51.740: INFO: Pod "downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032984288s
STEP: Saw pod success
Jun 11 15:25:51.740: INFO: Pod "downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327" satisfied condition "Succeeded or Failed"
Jun 11 15:25:51.745: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327 container client-container: <nil>
STEP: delete the pod
Jun 11 15:25:51.777: INFO: Waiting for pod downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327 to disappear
Jun 11 15:25:51.782: INFO: Pod downwardapi-volume-b801bc4c-e4d0-4382-b5b8-a32daafbc327 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jun 11 15:25:51.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7533" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:51.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 11 15:25:51.901: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:51.901: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:51.901: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:51.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:25:51.907: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:52.920: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:52.920: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:52.920: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:52.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 11 15:25:52.925: INFO: Node ip-172-31-27-214 is running 0 daemon pod, expected 1
Jun 11 15:25:53.917: INFO: DaemonSet pods can't tolerate node ip-172-31-0-57 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:53.917: INFO: DaemonSet pods can't tolerate node ip-172-31-15-213 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:53.917: INFO: DaemonSet pods can't tolerate node ip-172-31-30-216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 11 15:25:53.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 11 15:25:53.922: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jun 11 15:25:53.970: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45426"},"items":null}

Jun 11 15:25:53.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45426"},"items":[{"metadata":{"name":"daemon-set-d46mh","generateName":"daemon-set-","namespace":"daemonsets-3137","uid":"af5e9d21-202b-4115-9751-9139a1d153b3","resourceVersion":"45420","creationTimestamp":"2022-06-11T15:25:51Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ef8350ea1c605176db9f99c6288a3242743c8ce79f115d6c79b444d227b6fa87","cni.projectcalico.org/podIP":"192.168.203.48/32","cni.projectcalico.org/podIPs":"192.168.203.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a6c53b29-6ec4-410f-9269-694df7fb6d00","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6c53b29-6ec4-410f-9269-694df7fb6d00\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.203.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cwgx9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cwgx9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-41-158","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-41-158"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:51Z"}],"hostIP":"172.31.41.158","podIP":"192.168.203.48","podIPs":[{"ip":"192.168.203.48"}],"startTime":"2022-06-11T15:25:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-11T15:25:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a1217b8d5626d7e3af11d9eab987068b59ebfb3e37c5e0e9713e3b39668d6b92","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ssvks","generateName":"daemon-set-","namespace":"daemonsets-3137","uid":"e82e23a4-42ae-4c8c-89ec-74e37b9b5388","resourceVersion":"45422","creationTimestamp":"2022-06-11T15:25:51Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0b9a4de91bd880de2950a9eb3079d73a4d268a7229e8d29c546c7f5df1a9840c","cni.projectcalico.org/podIP":"192.168.149.69/32","cni.projectcalico.org/podIPs":"192.168.149.69/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a6c53b29-6ec4-410f-9269-694df7fb6d00","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6c53b29-6ec4-410f-9269-694df7fb6d00\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-11T15:25:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.149.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ln9ms","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ln9ms","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-27-214","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-27-214"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-11T15:25:51Z"}],"hostIP":"172.31.27.214","podIP":"192.168.149.69","podIPs":[{"ip":"192.168.149.69"}],"startTime":"2022-06-11T15:25:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-11T15:25:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c0768294b7c1e47164ca31bb3bdf36661f0047b3406fb28089476f1ed4f85bcc","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jun 11 15:25:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3137" for this suite.
â€¢{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":336,"skipped":6219,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:54.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 11 15:25:54.069: INFO: Waiting up to 5m0s for pod "pod-8332e335-7fa4-4757-9d60-3303160712df" in namespace "emptydir-9086" to be "Succeeded or Failed"
Jun 11 15:25:54.079: INFO: Pod "pod-8332e335-7fa4-4757-9d60-3303160712df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039582ms
Jun 11 15:25:56.084: INFO: Pod "pod-8332e335-7fa4-4757-9d60-3303160712df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015139464s
Jun 11 15:25:58.093: INFO: Pod "pod-8332e335-7fa4-4757-9d60-3303160712df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023693377s
STEP: Saw pod success
Jun 11 15:25:58.093: INFO: Pod "pod-8332e335-7fa4-4757-9d60-3303160712df" satisfied condition "Succeeded or Failed"
Jun 11 15:25:58.097: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-8332e335-7fa4-4757-9d60-3303160712df container test-container: <nil>
STEP: delete the pod
Jun 11 15:25:58.123: INFO: Waiting for pod pod-8332e335-7fa4-4757-9d60-3303160712df to disappear
Jun 11 15:25:58.129: INFO: Pod pod-8332e335-7fa4-4757-9d60-3303160712df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jun 11 15:25:58.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9086" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6237,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:25:58.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:25:59.080: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:26:02.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:26:02.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6206" for this suite.
STEP: Destroying namespace "webhook-6206-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":338,"skipped":6240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-5be646f2-a7ed-4a4b-b65a-701ad92890c0
STEP: Creating a pod to test consume secrets
Jun 11 15:26:02.335: INFO: Waiting up to 5m0s for pod "pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0" in namespace "secrets-8250" to be "Succeeded or Failed"
Jun 11 15:26:02.343: INFO: Pod "pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.899019ms
Jun 11 15:26:04.351: INFO: Pod "pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015822059s
Jun 11 15:26:06.362: INFO: Pod "pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026832556s
STEP: Saw pod success
Jun 11 15:26:06.362: INFO: Pod "pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0" satisfied condition "Succeeded or Failed"
Jun 11 15:26:06.366: INFO: Trying to get logs from node ip-172-31-41-158 pod pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0 container secret-volume-test: <nil>
STEP: delete the pod
Jun 11 15:26:06.403: INFO: Waiting for pod pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0 to disappear
Jun 11 15:26:06.410: INFO: Pod pod-secrets-7881d3af-d909-4570-b379-6a4ddc0341c0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jun 11 15:26:06.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8250" for this suite.
STEP: Destroying namespace "secret-namespace-9896" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":339,"skipped":6267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:06.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 11 15:26:10.573: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jun 11 15:26:10.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2533" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":340,"skipped":6309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:10.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3644
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-3644
Jun 11 15:26:10.682: INFO: Found 0 stateful pods, waiting for 1
Jun 11 15:26:20.690: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 11 15:26:20.743: INFO: Deleting all statefulset in ns statefulset-3644
Jun 11 15:26:20.754: INFO: Scaling statefulset ss to 0
Jun 11 15:26:30.815: INFO: Waiting for statefulset status.replicas updated to 0
Jun 11 15:26:30.820: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jun 11 15:26:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3644" for this suite.

â€¢ [SLOW TEST:20.254 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":341,"skipped":6369,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:30.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:26:32.972: INFO: Deleting pod "var-expansion-193664ec-96c4-4655-8153-c4e381bec587" in namespace "var-expansion-4781"
Jun 11 15:26:32.984: INFO: Wait up to 5m0s for pod "var-expansion-193664ec-96c4-4655-8153-c4e381bec587" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jun 11 15:26:36.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4781" for this suite.

â€¢ [SLOW TEST:6.136 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":342,"skipped":6373,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:37.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jun 11 15:26:37.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b" in namespace "projected-5774" to be "Succeeded or Failed"
Jun 11 15:26:37.076: INFO: Pod "downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.093762ms
Jun 11 15:26:39.091: INFO: Pod "downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025392074s
Jun 11 15:26:41.101: INFO: Pod "downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03502952s
STEP: Saw pod success
Jun 11 15:26:41.101: INFO: Pod "downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b" satisfied condition "Succeeded or Failed"
Jun 11 15:26:41.106: INFO: Trying to get logs from node ip-172-31-41-158 pod downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b container client-container: <nil>
STEP: delete the pod
Jun 11 15:26:41.134: INFO: Waiting for pod downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b to disappear
Jun 11 15:26:41.139: INFO: Pod downwardapi-volume-5f9a04ae-dcda-4ff4-a453-e7d337506a5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jun 11 15:26:41.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5774" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":343,"skipped":6381,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:41.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jun 11 15:26:41.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 create -f -'
Jun 11 15:26:42.685: INFO: stderr: ""
Jun 11 15:26:42.685: INFO: stdout: "pod/pause created\n"
Jun 11 15:26:42.685: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 11 15:26:42.685: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2575" to be "running and ready"
Jun 11 15:26:42.703: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.683057ms
Jun 11 15:26:44.730: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.044682806s
Jun 11 15:26:44.730: INFO: Pod "pause" satisfied condition "running and ready"
Jun 11 15:26:44.730: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 11 15:26:44.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 label pods pause testing-label=testing-label-value'
Jun 11 15:26:44.836: INFO: stderr: ""
Jun 11 15:26:44.836: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 11 15:26:44.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 get pod pause -L testing-label'
Jun 11 15:26:44.931: INFO: stderr: ""
Jun 11 15:26:44.931: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 11 15:26:44.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 label pods pause testing-label-'
Jun 11 15:26:45.042: INFO: stderr: ""
Jun 11 15:26:45.042: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 11 15:26:45.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 get pod pause -L testing-label'
Jun 11 15:26:45.141: INFO: stderr: ""
Jun 11 15:26:45.141: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jun 11 15:26:45.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 delete --grace-period=0 --force -f -'
Jun 11 15:26:45.234: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 11 15:26:45.234: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 11 15:26:45.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 get rc,svc -l name=pause --no-headers'
Jun 11 15:26:45.362: INFO: stderr: "No resources found in kubectl-2575 namespace.\n"
Jun 11 15:26:45.362: INFO: stdout: ""
Jun 11 15:26:45.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-2575 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 11 15:26:45.436: INFO: stderr: ""
Jun 11 15:26:45.436: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:26:45.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2575" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":344,"skipped":6399,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:45.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:26:45.855: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 11 15:26:47.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 11, 15, 26, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 15, 26, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 11, 15, 26, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 11, 15, 26, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f978cd6d5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:26:50.908: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:26:51.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4246" for this suite.
STEP: Destroying namespace "webhook-4246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:5.856 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":345,"skipped":6409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:26:51.370: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b" in namespace "security-context-test-6300" to be "Succeeded or Failed"
Jun 11 15:26:51.382: INFO: Pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.452233ms
Jun 11 15:26:53.397: INFO: Pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026963966s
Jun 11 15:26:55.418: INFO: Pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04767754s
Jun 11 15:26:57.429: INFO: Pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05842505s
Jun 11 15:26:57.429: INFO: Pod "alpine-nnp-false-2227590e-e360-4ee7-a9fa-a547962f381b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jun 11 15:26:57.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6300" for this suite.

â€¢ [SLOW TEST:6.143 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6441,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:57.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:26:57.528: INFO: The status of Pod pod-secrets-26480535-d450-49ad-8c66-2e01c566c2a3 is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:26:59.534: INFO: The status of Pod pod-secrets-26480535-d450-49ad-8c66-2e01c566c2a3 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jun 11 15:26:59.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6019" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":347,"skipped":6448,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:26:59.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:26:59.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jun 11 15:27:03.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 create -f -'
Jun 11 15:27:05.027: INFO: stderr: ""
Jun 11 15:27:05.027: INFO: stdout: "e2e-test-crd-publish-openapi-7307-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 11 15:27:05.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 delete e2e-test-crd-publish-openapi-7307-crds test-cr'
Jun 11 15:27:05.165: INFO: stderr: ""
Jun 11 15:27:05.165: INFO: stdout: "e2e-test-crd-publish-openapi-7307-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 11 15:27:05.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 apply -f -'
Jun 11 15:27:06.456: INFO: stderr: ""
Jun 11 15:27:06.456: INFO: stdout: "e2e-test-crd-publish-openapi-7307-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 11 15:27:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 delete e2e-test-crd-publish-openapi-7307-crds test-cr'
Jun 11 15:27:06.557: INFO: stderr: ""
Jun 11 15:27:06.557: INFO: stdout: "e2e-test-crd-publish-openapi-7307-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 11 15:27:06.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=crd-publish-openapi-5314 explain e2e-test-crd-publish-openapi-7307-crds'
Jun 11 15:27:06.775: INFO: stderr: ""
Jun 11 15:27:06.775: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7307-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:27:12.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5314" for this suite.

â€¢ [SLOW TEST:13.074 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":348,"skipped":6468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:27:12.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:27:13.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-532" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":349,"skipped":6516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jun 11 15:27:15.877: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5780 pod-service-account-89ccbf5e-42d5-414e-99c8-b3622fe0f736 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 11 15:27:16.039: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5780 pod-service-account-89ccbf5e-42d5-414e-99c8-b3622fe0f736 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 11 15:27:16.221: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5780 pod-service-account-89ccbf5e-42d5-414e-99c8-b3622fe0f736 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun 11 15:27:16.404: INFO: Got root ca configmap in namespace "svcaccounts-5780"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jun 11 15:27:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5780" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":350,"skipped":6543,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:16.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:27:17.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:27:20.204: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:27:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-53" for this suite.
STEP: Destroying namespace "webhook-53-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":351,"skipped":6553,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jun 11 15:27:32.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-118" for this suite.

â€¢ [SLOW TEST:12.112 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":352,"skipped":6554,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:32.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 11 15:27:33.282: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 11 15:27:36.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jun 11 15:27:48.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8800" for this suite.
STEP: Destroying namespace "webhook-8800-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

â€¢ [SLOW TEST:16.112 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":353,"skipped":6558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:48.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-ca66573e-33a2-4212-82e2-56187decae14
STEP: Creating secret with name s-test-opt-upd-af534762-6ff4-4909-b338-f86d0995a919
STEP: Creating the pod
Jun 11 15:27:48.797: INFO: The status of Pod pod-projected-secrets-61852883-2323-4b52-90e4-c77f4f63380c is Pending, waiting for it to be Running (with Ready = true)
Jun 11 15:27:50.809: INFO: The status of Pod pod-projected-secrets-61852883-2323-4b52-90e4-c77f4f63380c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-ca66573e-33a2-4212-82e2-56187decae14
STEP: Updating secret s-test-opt-upd-af534762-6ff4-4909-b338-f86d0995a919
STEP: Creating secret with name s-test-opt-create-d33e4852-d844-4ec3-aaaf-cd9b5c0ab5d3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jun 11 15:27:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5878" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":354,"skipped":6595,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jun 11 15:27:52.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.36 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 11 15:27:53.098: INFO: stderr: ""
Jun 11 15:27:53.098: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jun 11 15:27:53.098: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 11 15:27:53.098: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3529" to be "running and ready, or succeeded"
Jun 11 15:27:53.105: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.245469ms
Jun 11 15:27:55.119: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.021504815s
Jun 11 15:27:55.119: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 11 15:27:55.119: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 11 15:27:55.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator'
Jun 11 15:27:55.236: INFO: stderr: ""
Jun 11 15:27:55.236: INFO: stdout: "I0611 15:27:53.981956       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/qlbd 438\nI0611 15:27:54.182066       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gm2 345\nI0611 15:27:54.382041       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/stc 475\nI0611 15:27:54.582544       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/t8g 540\nI0611 15:27:54.782894       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/v28d 573\nI0611 15:27:54.982526       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/ct76 592\nI0611 15:27:55.182940       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4rb 413\n"
STEP: limiting log lines
Jun 11 15:27:55.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator --tail=1'
Jun 11 15:27:55.345: INFO: stderr: ""
Jun 11 15:27:55.345: INFO: stdout: "I0611 15:27:55.182940       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4rb 413\n"
Jun 11 15:27:55.345: INFO: got output "I0611 15:27:55.182940       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4rb 413\n"
STEP: limiting log bytes
Jun 11 15:27:55.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator --limit-bytes=1'
Jun 11 15:27:55.453: INFO: stderr: ""
Jun 11 15:27:55.453: INFO: stdout: "I"
Jun 11 15:27:55.453: INFO: got output "I"
STEP: exposing timestamps
Jun 11 15:27:55.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 11 15:27:55.571: INFO: stderr: ""
Jun 11 15:27:55.571: INFO: stdout: "2022-06-11T15:27:55.382442713Z I0611 15:27:55.382287       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/d27l 436\n"
Jun 11 15:27:55.572: INFO: got output "2022-06-11T15:27:55.382442713Z I0611 15:27:55.382287       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/d27l 436\n"
STEP: restricting to a time range
Jun 11 15:27:58.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator --since=1s'
Jun 11 15:27:58.189: INFO: stderr: ""
Jun 11 15:27:58.189: INFO: stdout: "I0611 15:27:57.382477       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/rptw 392\nI0611 15:27:57.583163       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/bmx 260\nI0611 15:27:57.782832       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/hzks 254\nI0611 15:27:57.981990       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/4kqc 486\nI0611 15:27:58.182496       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/trv7 376\n"
Jun 11 15:27:58.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 logs logs-generator logs-generator --since=24h'
Jun 11 15:27:58.296: INFO: stderr: ""
Jun 11 15:27:58.296: INFO: stdout: "I0611 15:27:53.981956       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/qlbd 438\nI0611 15:27:54.182066       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gm2 345\nI0611 15:27:54.382041       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/stc 475\nI0611 15:27:54.582544       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/t8g 540\nI0611 15:27:54.782894       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/v28d 573\nI0611 15:27:54.982526       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/ct76 592\nI0611 15:27:55.182940       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/4rb 413\nI0611 15:27:55.382287       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/d27l 436\nI0611 15:27:55.582689       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/5h7z 444\nI0611 15:27:55.782942       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/9kq 309\nI0611 15:27:55.982455       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/xbkp 427\nI0611 15:27:56.182948       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/twf 398\nI0611 15:27:56.382423       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/n76t 221\nI0611 15:27:56.582834       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/fpbg 211\nI0611 15:27:56.782376       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/gjcq 361\nI0611 15:27:56.982776       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/8kp7 546\nI0611 15:27:57.182068       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/ccf 540\nI0611 15:27:57.382477       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/rptw 392\nI0611 15:27:57.583163       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/bmx 260\nI0611 15:27:57.782832       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/hzks 254\nI0611 15:27:57.981990       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/4kqc 486\nI0611 15:27:58.182496       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/trv7 376\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jun 11 15:27:58.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3807392589 --namespace=kubectl-3529 delete pod logs-generator'
Jun 11 15:27:59.601: INFO: stderr: ""
Jun 11 15:27:59.601: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jun 11 15:27:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3529" for this suite.

â€¢ [SLOW TEST:6.676 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":355,"skipped":6601,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jun 11 15:27:59.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3807392589
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jun 11 15:27:59.660: INFO: Creating deployment "test-recreate-deployment"
Jun 11 15:27:59.669: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 11 15:27:59.682: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 11 15:28:01.695: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 11 15:28:01.700: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 11 15:28:01.719: INFO: Updating deployment test-recreate-deployment
Jun 11 15:28:01.719: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 11 15:28:01.863: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9407  f068e15e-bc3a-4b2c-8fdd-60550b0c9b5f 46835 2 2022-06-11 15:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f9ff88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-11 15:28:01 +0000 UTC,LastTransitionTime:2022-06-11 15:28:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-06-11 15:28:01 +0000 UTC,LastTransitionTime:2022-06-11 15:27:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 11 15:28:01.868: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-9407  d5450efc-aec6-4a20-9a56-5073029854cf 46834 1 2022-06-11 15:28:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f068e15e-bc3a-4b2c-8fdd-60550b0c9b5f 0xc004fda630 0xc004fda631}] []  [{kube-controller-manager Update apps/v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f068e15e-bc3a-4b2c-8fdd-60550b0c9b5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fda6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 15:28:01.868: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 11 15:28:01.868: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-848969dbcd  deployment-9407  4bc26539-4dd1-4891-a6b4-2c5dd77037b3 46824 2 2022-06-11 15:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f068e15e-bc3a-4b2c-8fdd-60550b0c9b5f 0xc004fda517 0xc004fda518}] []  [{kube-controller-manager Update apps/v1 2022-06-11 15:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f068e15e-bc3a-4b2c-8fdd-60550b0c9b5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 848969dbcd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:848969dbcd] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.36 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fda5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 11 15:28:01.873: INFO: Pod "test-recreate-deployment-cd8586fc7-nl5pd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-nl5pd test-recreate-deployment-cd8586fc7- deployment-9407  d428c4bf-0d24-4295-bbcb-61376d24f8bb 46836 0 2022-06-11 15:28:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 d5450efc-aec6-4a20-9a56-5073029854cf 0xc004fdae00 0xc004fdae01}] []  [{kube-controller-manager Update v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5450efc-aec6-4a20-9a56-5073029854cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-11 15:28:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfs4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfs4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-41-158,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:28:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-11 15:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.41.158,PodIP:,StartTime:2022-06-11 15:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jun 11 15:28:01.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9407" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":356,"skipped":6603,"failed":0}
SSSSSSJun 11 15:28:01.889: INFO: Running AfterSuite actions on all nodes
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun 11 15:28:01.889: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jun 11 15:28:01.889: INFO: Running AfterSuite actions on node 1
Jun 11 15:28:01.889: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6609,"failed":0}

Ran 356 of 6965 Specs in 5663.102 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6609 Skipped
PASS

Ginkgo ran 1 suite in 1h34m25.509460312s
Test Suite Passed
